{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kxQOE10niEvK",
        "outputId": "a10cb6bf-afe5-46c2-f12c-f9b004aa1dca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/86.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m123.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m127.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m122.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.4/127.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m126.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m112.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mFiles removed: 153\n",
            "\u001b[33mWARNING: No matching packages\u001b[0m\u001b[33m\n",
            "\u001b[0mFiles removed: 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'cached_download' from 'huggingface_hub' (/usr/local/lib/python3.11/dist-packages/huggingface_hub/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f1c066aace6a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCrossEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2.2.2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0m__MODEL_HUB_ORGANIZATION__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sentence-transformers'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentencesDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParallelSentencesDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mLoggingHandler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoggingHandler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mSentenceTransformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mDenoisingAutoEncoderDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDenoisingAutoEncoderDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mNoDuplicatesDataLoader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNoDuplicatesDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mParallelSentencesDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParallelSentencesDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mSentencesDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentencesDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mSentenceLabelDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceLabelDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/datasets/ParallelSentencesDataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInputExample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHfApi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHfFolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRepository\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf_hub_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached_download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'cached_download' from 'huggingface_hub' (/usr/local/lib/python3.11/dist-packages/huggingface_hub/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# # STEP 1: Install required packages (if not already installed)\n",
        "# !pip install -q sentence-transformers==2.2.2 transformers==4.31.0 torch==2.0.1 huggingface_hub==0.16.4 groq nltk\n",
        "\n",
        "# !pip cache purge\n",
        "# !rm -rf ~/.cache/huggingface\n",
        "\n",
        "# STEP 2: Set up\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from groq import Groq\n",
        "import nltke\n",
        "\n",
        "# Login to Hugging Face (uncomment if needed)\n",
        "# !huggingface-cli login–\n",
        "# Enter your Hugging Face token (get it from https://huggingface.co/settings/tokens)\n",
        "\n",
        "import numpy as np\n",
        "# from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "import nltk\n",
        "import time\n",
        "import logging\n",
        "import torch\n",
        "import os\n",
        "from groq import Groq\n",
        "import json\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Download NLTK data\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('punkt_tab')  # Added for sent_tokenize\n",
        "    logging.info(\"NLTK data downloaded successfully\")\n",
        "except Exception as e:\n",
        "    logging.error(f\"Failed to download NLTK data: {e}\")\n",
        "    raise\n",
        "\n",
        "# Verify Groq API key\n",
        "if not os.environ.get(\"GROQ_API_KEY\"):\n",
        "    raise EnvironmentError(\"GROQ_API_KEY environment variable not set\")\n",
        "\n",
        "# Validate input JSON files\n",
        "reference_json_path = \"/kaggle/input/llm-dataset/question_answers.json\"\n",
        "pdf_text_json_path = \"/kaggle/input/llm-dataset/pdf_text_data.json\"\n",
        "for path in [reference_json_path, pdf_text_json_path]:\n",
        "    if not os.path.exists(path):\n",
        "        logging.error(f\"Input file not found: {path}\")\n",
        "        raise FileNotFoundError(f\"Input file not found: {path}\")\n",
        "\n",
        "# Check disk usage\n",
        "!df -h"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logging.info(f\"Using device: {device}\")\n",
        "\n",
        "# Set cache directory\n",
        "cache_dir = \"/content/hf_cache\"\n",
        "os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "# Clear GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Load embedding model\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2', cache_folder=cache_dir).to(device)\n",
        "\n",
        "# Load cross-encoder for re-ranking\n",
        "cross_encoder = CrossEncoder('sentence-transformers/paraphrase-MiniLM-L6-v2', device=device)\n",
        "\n",
        "# Load tokenizer for token counting\n",
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2', cache_dir=cache_dir)\n",
        "\n",
        "# Initialize Groq client\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"  # Replace with your actual key\n",
        "client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
        "\n",
        "# Enhanced Groq generation function\n",
        "def generate_text(messages, max_tokens=1000, retries=3):\n",
        "    prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in messages])\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            completion = client.chat.completions.create(\n",
        "                model=\"llama-3.3-70b-versatile\",\n",
        "                messages=messages,\n",
        "                max_tokens=max_tokens,\n",
        "                temperature=0.2,  # Slightly higher for natural responses\n",
        "                top_p=0.9,\n",
        "                stream=False\n",
        "            )\n",
        "            return completion.choices[0].message.content.strip()\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Groq API error (attempt {attempt+1}/{retries}): {e}\")\n",
        "            time.sleep(1)  # Brief delay before retry\n",
        "    return \"Error: Failed to generate response after multiple attempts\"\n",
        "\n",
        "# Check disk and GPU memory usage\n",
        "!df -h\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "kkgvDb3Hirzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_chunking(corpus: str, max_sentences_per_chunk: int = 10, min_sentences_per_chunk: int = 3, max_tokens_per_chunk: int = 500) -> list[dict]:\n",
        "    logging.info(\"Starting semantic chunking...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Split into sentences\n",
        "    sentences = nltk.sent_tokenize(corpus)\n",
        "    if not sentences:\n",
        "        logging.warning(\"Empty corpus provided\")\n",
        "        return []\n",
        "\n",
        "    # Compute sentence embeddings\n",
        "    sentence_embeddings = embedder.encode(sentences, show_progress_bar=False)\n",
        "\n",
        "    # Cluster sentences using KMeans\n",
        "    num_clusters = max(1, len(sentences) // min_sentences_per_chunk)\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(sentence_embeddings)\n",
        "\n",
        "    # Group sentences by cluster\n",
        "    clusters = {}\n",
        "    for sent, label in zip(sentences, kmeans.labels_):\n",
        "        if label not in clusters:\n",
        "            clusters[label] = []\n",
        "        clusters[label].append(sent)\n",
        "\n",
        "    # Form chunks from clusters\n",
        "    chunks = []\n",
        "    for cluster_sentences in clusters.values():\n",
        "        for i in range(0, len(cluster_sentences), max_sentences_per_chunk):\n",
        "            chunk_sentences = cluster_sentences[i:i + max_sentences_per_chunk]\n",
        "            if len(chunk_sentences) >= min_sentences_per_chunk:\n",
        "                chunk_text = \" \".join(chunk_sentences)\n",
        "                token_count = len(tokenizer.encode(chunk_text))\n",
        "                if token_count <= max_tokens_per_chunk:\n",
        "                    chunks.append({\n",
        "                        \"text\": chunk_text,\n",
        "                        \"embedding\": embedder.encode(chunk_text),\n",
        "                        \"token_count\": token_count\n",
        "                    })\n",
        "\n",
        "    logging.info(f\"Semantic chunking completed in {time.time() - start_time:.2f} seconds. {len(chunks)} chunks created (avg tokens: {np.mean([c['token_count'] for c in chunks]):.1f})\")\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "DWg9b2jXjAuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def enhanced_rag_retrieval(query: str, chunks: list[dict], max_tokens: int = 3000, top_k_initial: int = 20, top_k_final: int = 5) -> list[dict]:\n",
        "    logging.info(f\"Retrieving chunks for query: {query}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    if not chunks:\n",
        "        logging.warning(\"No chunks provided for retrieval\")\n",
        "        return []\n",
        "\n",
        "    # Initial retrieval with cosine similarity\n",
        "    query_embedding = embedder.encode(query)\n",
        "    similarities = cosine_similarity([query_embedding], [c[\"embedding\"] for c in chunks])[0]\n",
        "    top_k_initial = min(top_k_initial, len(chunks))  # Adjust for small chunk lists\n",
        "    initial_top_indices = np.argsort(similarities)[::-1][:top_k_initial]\n",
        "\n",
        "    # Prepare pairs for cross-encoder\n",
        "    cross_encoder_pairs = [(query, chunks[idx][\"text\"]) for idx in initial_top_indices]\n",
        "    if not cross_encoder_pairs:\n",
        "        return []\n",
        "\n",
        "    # Re-rank with cross-encoder\n",
        "    cross_encoder_scores = cross_encoder.predict(cross_encoder_pairs)\n",
        "    reranked_indices = np.argsort(cross_encoder_scores)[::-1][:min(top_k_final, len(cross_encoder_scores))]\n",
        "    final_indices = [initial_top_indices[i] for i in reranked_indices]\n",
        "\n",
        "    # Select chunks within token limit\n",
        "    selected_chunks = []\n",
        "    total_tokens = 0\n",
        "    for idx in final_indices:\n",
        "        chunk = chunks[idx]\n",
        "        if total_tokens + chunk[\"token_count\"] <= max_tokens:\n",
        "            selected_chunks.append(chunk)\n",
        "            total_tokens += chunk[\"token_count\"]\n",
        "\n",
        "    logging.info(f\"Retrieval completed in {time.time() - start_time:.2f} seconds. Retrieved {len(selected_chunks)} chunks (total tokens: {total_tokens})\")\n",
        "    return selected_chunks"
      ],
      "metadata": {
        "id": "PkVrf96YjC2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normal_rag_answer(query: str, chunks: list[dict], max_tokens: int = 1000) -> str:\n",
        "    logging.info(f\"Generating Normal RAG answer for query: {query}\")\n",
        "    if not chunks:\n",
        "        logging.warning(\"No chunks provided for answer generation\")\n",
        "        return \"No relevant information found to answer the query.\"\n",
        "\n",
        "    # Combine chunk texts\n",
        "    context = \"\\n\\n\".join([c[\"text\"] for c in chunks])\n",
        "    token_count = len(tokenizer.encode(context))\n",
        "    logging.info(f\"Context token count: {token_count}\")\n",
        "\n",
        "    # High-quality prompt with LLM-as-a-judge style\n",
        "    prompt = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": (\n",
        "                \"You are an expert assistant tasked with answering questions based on provided context. \"\n",
        "                \"Your goal is to generate a concise, accurate, and complete answer using only the relevant information from the context. \"\n",
        "                \"If the context is insufficient or irrelevant, state that clearly. \"\n",
        "                \"Structure your answer clearly, using bullet points or paragraphs as appropriate, and avoid unnecessary elaboration.\"\n",
        "            )\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": (\n",
        "                f\"Query: {query}\\n\\n\"\n",
        "                f\"Context:\\n{context}\\n\\n\"\n",
        "                \"Instructions:\\n\"\n",
        "                \"1. Analyze the context to identify information directly relevant to the query.\\n\"\n",
        "                \"2. Provide a complete answer based on the relevant information.\\n\"\n",
        "                \"3. If the context lacks sufficient details, state: 'The provided context does not contain enough information to answer the query.'\\n\"\n",
        "                \"4. Keep the answer concise and focused, within {max_tokens} tokens.\"\n",
        "            )\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    answer = generate_text(prompt, max_tokens=max_tokens)\n",
        "    if not answer:\n",
        "        answer = \"Error: Failed to generate answer.\"\n",
        "    logging.info(f\"Answer generated (length: {len(answer)} characters)\")\n",
        "    return answer"
      ],
      "metadata": {
        "id": "Sr2Bfl_njFMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from typing import List, Dict\n",
        "\n",
        "def build_corpus_and_chunks(pdf_text: str, max_sentences: int = 10, min_sentences: int = 3, max_tokens: int = 500) -> List[Dict]:\n",
        "    \"\"\"Create semantic chunks from PDF text.\"\"\"\n",
        "    return semantic_chunking(pdf_text, max_sentences, min_sentences, max_tokens)\n",
        "\n",
        "def get_rag_answer_from_chunks(question: str, chunks: List[Dict], max_tokens: int = 1000) -> str:\n",
        "    \"\"\"Generate RAG answer using retrieved chunks.\"\"\"\n",
        "    retrieved = enhanced_rag_retrieval(question, chunks, max_tokens=3000, top_k_initial=20, top_k_final=5)\n",
        "    return normal_rag_answer(question, retrieved, max_tokens=max_tokens)\n",
        "\n",
        "def extract_pdf_text(pdf_name: str, pdf_text_json_path: str) -> str:\n",
        "    \"\"\"Extract text for a given PDF from JSON.\"\"\"\n",
        "    try:\n",
        "        with open(pdf_text_json_path, 'r') as f:\n",
        "            pdf_data = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        logging.error(f\"PDF text JSON file not found: {pdf_text_json_path}\")\n",
        "        return \"\"\n",
        "    except json.JSONDecodeError:\n",
        "        logging.error(f\"Invalid JSON in {pdf_text_json_path}\")\n",
        "        return \"\"\n",
        "\n",
        "    if pdf_name not in pdf_data:\n",
        "        logging.error(f\"PDF {pdf_name} not found in {pdf_text_json_path}\")\n",
        "        return \"\"\n",
        "\n",
        "    text_data = pdf_data.get(pdf_name, {})\n",
        "    text = text_data.get(\"text\", \"\")\n",
        "    if not text:\n",
        "        logging.warning(f\"No text found for PDF {pdf_name}\")\n",
        "    return text\n",
        "\n",
        "def generate_rag_answers(reference_json_path: str, pdf_text_json_path: str, output_json_path: str) -> None:\n",
        "    \"\"\"Generate RAG answers for all questions and save to JSON.\"\"\"\n",
        "    try:\n",
        "        with open(reference_json_path, 'r') as f:\n",
        "            reference_data = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        logging.error(f\"Reference JSON file not found: {reference_json_path}\")\n",
        "        return\n",
        "    except json.JSONDecodeError:\n",
        "        logging.error(f\"Invalid JSON in {reference_json_path}\")\n",
        "        return\n",
        "\n",
        "    rag_answers = {}\n",
        "\n",
        "    for pdf_name, entries in reference_data.items():\n",
        "        logging.info(f\"Processing PDF: {pdf_name}\")\n",
        "        pdf_text = extract_pdf_text(pdf_name, pdf_text_json_path)\n",
        "        if not pdf_text:\n",
        "            logging.warning(f\"Skipping PDF {pdf_name} due to missing text\")\n",
        "            continue\n",
        "\n",
        "        chunks = build_corpus_and_chunks(pdf_text)\n",
        "        if not chunks:\n",
        "            logging.warning(f\"No chunks created for PDF {pdf_name}\")\n",
        "            continue\n",
        "\n",
        "        rag_entries = []\n",
        "        for entry in entries:\n",
        "            question = entry.get(\"question\", \"\")\n",
        "            if not question:\n",
        "                logging.warning(f\"Skipping empty question in PDF {pdf_name}\")\n",
        "                continue\n",
        "\n",
        "            answer = get_rag_answer_from_chunks(question, chunks)\n",
        "            if len(answer) < 50:  # Arbitrary threshold for incomplete answers\n",
        "                logging.warning(f\"Short answer generated for question: {question[:50]}...\")\n",
        "            rag_entries.append({\"question\": question, \"answer\": answer})\n",
        "\n",
        "        rag_answers[pdf_name] = rag_entries\n",
        "\n",
        "    try:\n",
        "        with open(output_json_path, 'w') as f:\n",
        "            json.dump(rag_answers, f, indent=2)\n",
        "        logging.info(f\"RAG answers saved to {output_json_path}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error saving RAG answers: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define file paths\n",
        "    reference_json_path = \"/content/question_answers.json\"  # e.g., {\"vision_pro\": [{\"question\": \"...\"}]}\n",
        "    pdf_text_json_path = \"/content/pdf_text_data.json\"     # e.g., {\"vision_pro\": {\"text\": \"...\"}}\n",
        "    output_json_path = \"/content/generated_rag_answers.json\"  # Output path\n",
        "\n",
        "    # Run the pipeline\n",
        "    generate_rag_answers(reference_json_path, pdf_text_json_path, output_json_path)"
      ],
      "metadata": {
        "id": "2gsH59G5Lz2_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}