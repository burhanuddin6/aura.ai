{
    "https://developer.apple.com/documentation/visionos/bot-anist": "visionOS\nBOT-anist\nBOT-anist\nBOT-anist\nBOT-anist\nOverview\nBOT-anist is a game-like experience where you build a custom robot botanist by selecting from a variety of color and shape options, and then guide your robot around a futuristic greenhouse to plant alien flowers. This app demonstrates how to build an app for iOS, iPadOS, macOS, and visionOS using a single shared Xcode target and a shared Reality Composer Pro project.\nThis sample shows off a number of RealityKit and visionOS features, including volume ornaments, dynamic lights and shadows, animation library components, and vertex animation using blend shapes. It also demonstrates how to set a volume’s default size and enable user resizing of volumes.\nCustomize the robot and explore\nWhen the app launches, you see a window that contains your robot and a number of different user interface elements you can use to customize it. You can change the shape of your robot’s head, backpack, and body, as well as the material and color scheme for each part. You can also change the color of the lights for each piece.\nTo get a better look at your robot while customizing it, spin it using a drag gesture on iOS and visionOS, or by dragging it with your mouse in macOS.\nWhen you’re happy with the look of your robot botanist, tap or click the Start Planting button to send the robot to explore its futuristic greenhouse. In visionOS, BOT-anist displays the greenhouse in a resizable 3D volume. In iOS and macOS, it appears in the same window as the customization tools. There are three illuminated planters in different colors on the floor of the greenhouse. Move your robot around using drag gestures or keyboard controls to plant flowers in each one. When using the keyboard to control the robot, you have the option to use the traditionalWASDkey combination on QWERTY keyboards, as well as the right-handed equivalent (IJKL). You can also use the arrow keys and, if using an extended keyboard, the numeric keypad (8456). You can find the key bindings inRealityView+KeyboardControls.swiftif you want to change them to an alternative scheme.\n\nMake the project multiplatform\nYou can now use RealityKit to create multiplatform apps that run in iOS, iPadOS, macOS, and visionOS usingRealityView. As long as your Xcode project uses only SwiftUI for its user interface, you can convert it to a multiplatform app by navigating to your app target in Xcode and adding the platforms you want to support. You don’t need to add a new target or scheme. When you’re developing your app, Xcode compiles the right code for the selected destination. When you build your app for distribution, it builds it for all the platforms you select.\n\nThere are, however, platform differences you need to take into account in some apps. For example, visionOS uses a different unit scale for RealityKit scenes than other platforms do. Also, different devices have different screen sizes and aspect ratios. To account for these differences, you may want to set the scale and position of the root entity in theRealityViewusing different values.\nIn BOT-anist on visionOS, you use aGeometryReader3Dto position and resize the robot view to fill 80% of the available space\nSet up the window groups\nBOT-anist sets up two window groups because it uses both a window and a volume in visionOS, but runs the entire app in a single window on its other supported platforms. The first window group uses the default platform window style, which creates a standard window for the platform it’s running on. In visionOS only, the app configures this window group to dismiss the other window group, which holds the 3D volume, when this window appears. That ensures the window and volume are never visible at the same time.\nThe app class contains a second window group with avolumetricstyle to hold the greenhouse in visionOS. This second window group uses platform conditionals to ensure that it only compiles for visionOS. The default window style behavior in visionOS for 2D windows isdynamic, which means the window changes its size as it changes its distance to the player to ensure the window is always a good size for them to interact with.\nVolumes, on the other hand, default to a fixed window style, which means the farther away from the player the volume is, the smaller it appears. This is often the desired behavior for volumes because you want the virtual contents to blend in with real-world perspective. In this case, however, the player needs to interact with the greenhouse features much like they do with the UI elements in a 2D window. BOT-anist changes the default scaling of the volume todynamicso it stays usable even if the player moves away from it.\nShow the volume’s baseplate\nvisionOS volumes are, by default, user resizable. People can resize them by looking at one of the four bottom corners of the volume, and then pinching and dragging the control that appears.\nBOT-anist uses the default behavior for the volume that displays the greenhouse. To make it more obvious to the player that they can resize the volume, and to give them better visual feedback when doing it, BOT-anist makes the volume’s baseplate visible. Thebaseplateis a white, rounded rectangle on the bottom plane of the volume that the app enables by callingvolumeBaseplateVisibility(_:)on the volume’s root view.\nBOT-anist also sets the default size of the volume to make sure it starts large enough for the player to interact with.\nNote\nTo create a volume with fixed style, don’t specify a default size. Instead, useframe(minDepth:idealDepth:maxDepth:alignment:)on the volume’s root view and pass the same value forminDepth,idealDepth, andmaxDepth.\nBy default, when a volume changes size, the size of its contents don’t scale with it. BOT-anist’s contents do resize with the volume, which it accomplishes using thescaleEffect(_:anchor:)modifier.\nWhen the contents resize relative to the real-world surroundings, it affects the robot’s speed of movement, causing it to move too fast when you make the volume smaller and move too slow when you make it larger. To make sure the robot moves at a consistent speed no matter the size of the volume, the window group uses anonChange(of:initial:_:)modifier to update the robot’s speed based on the volume’s size.\nSpecify the volume’s ornament view\nStarting with visionOS 2, you can place ornaments in different locations in the 3D space of a volume. BOT-anist uses an ornament view to display the score, along with buttons for re-starting and for going back to the robot customization screen.\n\nInstead of the default placement, BOT-anist displays the ornament view at the top back. To specify its ornament view, it usesornament(visibility:attachmentAnchor:contentAlignment:ornament:), and a value oftopBack, which centers it at the top of the far side of the volume.\nCreate dynamic lights with shadows using Reality Composer Pro\nTo create dynamic lighting effects with shadows, add lights to your Reality Composer Pro project. To see the lights that BOT-anist uses, openBOTanistAssets.swiftin Reality Composer Pro and open the scene calledvolume.usda.\n\nAfter you add lights to your scene, build and run your app to see it with the new lights, including dynamic shadows. If you watch the robot botanist as you move it around the greenhouse, you see that it casts a shadow.\nYou can use up to eight lights in a RealityKit scene, but lights have a nontrivial performance impact, so use them strategically. Even when using dynamic lights, your entities still receive light from any image-based or environmental lighting your app uses.\n\nDetect viewpoint changes in volumes\nIn visionOS 2 and later, apps can useonVolumeViewpointChange(updateStrategy:initial:_:)to receive updates when the player moves to a different side. When BOT-anist receives an update, it rotates the robot toward the viewer and waves to them at their new location.\nInExplorationView.swift, which is the top-level view in the volume’s window group, the app usesonVolumeViewpointChange(updateStrategy:initial:_:)to receive updates about which side of the volume is facing the person, and stores the new facing in a property.\nThe value the app receives is of typeViewpoint3D, and it identifies which side of the volume is currently facing the viewer relative to which side was the front face when the app first launched. The code that handles movement input monitors this property. When it detects a viewpoint change and the robot isn’t moving, it starts the animations that cause the robot to rotate toward the new front face and wave cheerily.\nAnimate the robot\nBOT-anist contains multiple different body types that players can choose when building their robot, including one that walks, one that rolls, and one that floats. Each of these bodies has a different set of animations, and the app uses a state machine defined inAnimationStateMachine.swiftto keep track of which animation is currently playing, and when and how it transitions to a different animation.\nRealityKit can load multiple animations from different USDZ files and store them in anAnimationLibraryComponent. As long as two rigged entities have the same joint hierarchy, they can use each other’s animations. BOT-anist uses oneAnimationLibraryComponentper body entity to store the animations for that body type.\nAt launch, BOT-anist loads each of the different modular parts that players use to build their robot. When it loads the bodies, it creates anAnimationLibraryComponenton each loaded entity, then loads and stores one animation per animation state.\nWhen the app transitions to a new animation state, it can find and play the correct animation by retrieving the animation for the current state from the animation library on the body entity that’s in the scene.\nAnimate the plants using blend shapes\nWhile skeletal animations are an incredibly powerful and useful tool, certain types of animations need to move each vertex in the model individually. RealityKit stores vertex-level changes to a model usingblend shapes, which contain offset data for the model entity’s vertices. You can set each blend shape to a value between0.0and1.0. Any value other than0.0or1.0represents a partial state in-between the model’s default shape, and the shape contained in that blend shape.\nFor example, if you have a model of a plant as a sprout, and a shape key representing its fully grown shape, you can set the plant to grow only partially by setting that blend shape to a fractional value. Animating that value over time creates a blend-shape animation, which is how BOT-anist grows the flowers when you plant them.\nTo access blend shapes, useBlendShapeWeightsComponent. You can create blend shapes and set their values procedurally but, more often, you create blend shapes and blend shape animations using a 3D modeling tool, then store them in the model’s USDZ file. RealityKit automatically creates aBlendShapeWeightsComponentfor any model entity it loads from a USDZ file that contains blend shapes. It also adds any blend shape animations in the USDZ file to the entity’sAnimationLibraryComponent.\nNote\nSome software uses different terms when referring to per-vertex offset data. In addition to blend shape, you may also find the same functionality referred to asmorph targetsorshape keys. All of these export to USDZ files as blend shapes and work identically.\nTo animate the plants growing, BOT-anist uses blend shape animations created in a 3D modeling program and stored in the model’s USDZ file. It uses the same approach to animate the celebratory dancing the flowers do once the robot has planted them all. Each type of plant has its own blend shapes and blend shape animations to show the plant growing and celebrating.\nTo play the blend shape animations, the app iterates through entities in the scene that have aBlendShapeWeightsComponentand plays the corresponding blend shape weight animation. For example, here’s how it generates the grow animation:\nWhen BOT-anist transitions to the greenhouse, it has to make sure that all the growing plants are reset to their initial value. To do that, it manually sets all of the blend shapes to0.0except for the one that represents the initial hidden state.\nAnimate the head and backpack\nTo make the robot customizable, the app combines three separate entities to build it. Each of the three bodies (walking, rolling, floating) is a skeletal mesh with its own unique set of animations. When the app enters the greenhouse mode, it combines the selected head and backpack, which are static meshes, with the animated entity for the selected body.\nFor the head and backpack to animate correctly, BOT-anist needs to update their position and rotation every frame so they line up with the appropriate joint in the body’s skeleton. BOT-anist does this using a system and custom component calledJointPinSystemandJointPinComponent, which together to keep the robot parts animating in sync. When the player taps or clicks the Start Planting button, the app adds the component to the parent entity that the three body parts share. It identifies that its entity has children thatJointPinSystemneeds to reposition.JointPinSystemthen uses the dataJointPinComponentprovides to reposition the backpack and head to match the body’s animated position on each frame.\n\nWhen the player selects the Start Planting button, the app combines the three selected entities using theRobotCharacterclass. That class’s initializer retrieves the transforms for the head and backpack joints using thepinsproperty onEntity. This property provides access to the entity’sGeometricPinsComponent, which stores a collection of transforms, each of which identifies a different location, orientation, and scale relative to the entity, but without the overhead of a separate child entity for each one. People can create pins, but RealityKit also automatically creates a collection of pins to represent the joints in a rigged model.\nAfter the player taps or clicks the button, the app retrieves the two geometric pins that represent the head and backpack joints in the body’s skeleton. Because skeleton joints are arranged in a hierarchy, with each joint inheriting its parent’s transform, the app retrieves the entire joint chain from the root joint to the backpack or head joint using a private function calledgetJointHierarchy(_:for:).\nNext, it calculates an offset for the two pins. The back and head model entities are places so they align with the correct spot on the body model. Because they’re not at the origin, in order to rotate them on the origin, theJointPinSystemneeds to move them before applying the transform, otherwise they have the wrong pivot point. To calculate the offset, it gets the position of each pin and inverts it by multiplying the position by-1.\nFinally, it creates theJointPinComponentwith all the information the system needs to update the head and backpack entities.\nTo move the head and body each frame,JointPinSystemuses an entity query to find the parent entity the head, body, and backpack share. It then retrieves the entity representing the body’s skeleton and also retrieves all of the skeleton’s joint transforms.\nBecause BOT-anist has to apply the same logic to two different meshes, only using a different joint and offset, it uses a private function calledpinEntity(indices:skeleton:transforms:offset:staticEntity:shouldRotate)to apply that logic, which it then calls twice — once for the head and once for the backpack — after retrieving the data it needs from the component.\nTo calculate the correct transform for each joint, the system uses the joint chain it put in the component, and multiplies each joint’s transform matrix together starting with the root joint. It usesreduce(_:_:)to iterate through the joint chain, multiplying each transform with the next one. It then takes that calculated transform and offsets it to move it back to its original location.\nSee Also",
    "https://developer.apple.com/documentation/visionos/placing-content-on-detected-planes": "visionOS\nPlacing content on detected planes\nPlacing content on detected planes\nPlacing content on detected planes\nPlacing content on detected planes\nOverview\nFlat surfaces are an ideal place to position content in an app that uses a Full Space in visionOS. They provide a place for virtual 3D content to live alongside a person’s surroundings. Use plane detection in ARKit to detect these kinds of surfaces and filter the available planes based on criteria your app might need, such as the size of the plane, its proximity to someone, or a required plane orientation.\nUse RealityKit anchor entities for basic plane anchoring\nIf you don’t need a specific plane in your app and you’re rendering your app’s 3D content in RealityKit, you can use anAnchorEntityinstead. This approach lets you attach 3D content to a plane without prompting the person for world-sensing permission and without any particular knowledge of where that plane is relative to the person.\nThe following shows an anchor that you can use to attach entities to a table:\nAnchor entities don’t let you choose a specific plane in a person’s surroundings, but rather let you ask for a plane with certain characteristics. When you need more specific plane selection or real-time information about the plane’s position and orientation in the world, useARKitSessionandPlaneDetectionProvider.\nConfigure an ARKit session for plane detection\nPlane-detection information comes from anARKitSessionthat’s configured to use aPlaneDetectionProvider. You can choose to detect horizontal planes, vertical planes, or both. Each plane that ARKit detects comes with a classification, likePlaneAnchor.Classification.tableorPlaneAnchor.Classification.floor. You can use these classifications to further refine which kinds of planes your app uses to present content. Plane detection requiresARKitSession.AuthorizationType.worldSensingauthorization.\nThe following starts a session that detects both horizontal and vertical planes, but filters out planes classified as windows:\nCreate and update entities associated with each plane\nIf you’re displaying content that needs to appear attached to a particular plane, update your content whenever you receive new information from ARKit. When a plane is no longer available in the person’s surroundings, ARKit sends a removal event. Respond to these events by removing content associated with the plane.\nThe following shows plane updates that place a text entity on each plane in a person’s surroundings; the text entity displays the kind of plane ARKit detected:\nSee Also\nARKit",
    "https://developer.apple.com/documentation/visionos/understanding-the-visionos-render-pipeline": "visionOS\nUnderstanding the visionOS render pipeline\nUnderstanding the visionOS render pipeline\nUnderstanding the visionOS render pipeline\nUnderstanding the visionOS render pipeline\nOverview\nLike other Apple platforms, visionOS renders changes to the UI in response to updates your app makes, input events, callbacks from actions you initiate, timers, and notifications. Unique on Apple Vision Pro, the system renders updates to the images the device displays in order to reposition the UI relative to changes in head position. When the system detects eye and hand movement, deliberate or inadvertent, it requires additional processing to determine what a person is looking at, or interacting with, to calculate a response. The system does a lot more to render an up-to-date UI and account for input from spatial algorithms.\nThe following diagram illustrates how input propagates through the system and updates content in visionOS:\n\nA person initiates an interaction through a look, hand gesture, a keyboard, or pointing device.\nSensors, or other hardware, recognize the input and forward it to the operating system (OS).\nThe OS relies on collision detection to determine which entity or entities to direct an interaction to, and the process responsible for handling the user input event — the app that owns the scene where the event occurs.\nThe system puts the event into an app-specific queue, and your app’s main thread picks up those events from the queue and processes them. Your app updates its audio and visual output in response to these events. Your app updates the shared render server with any changes to the UI view hierarchy and 3D RealityKit based content.\nTherender serveris a continuously running process (backboardd) that receives updates from all the running apps and other processes with Core Animation and RealityKit content to display. The render server processes the updates and composites them into a single drawable image. It sends this combined image to the compositor.\nThecompositor, another continuously running process, receives the image frames from the render server, processes data about your surroundings from Apple Vision Pro sensors and cameras, then combines them into a set of images to display. To maintain visual smoothness, the compositor strives to maintain a consistent display refresh rate.\nOn visionOS, the display driver wakes up at a regular interval to update the display with the new image from the compositor. This differs from other platforms where the display driver checks at regular intervals with the render server to determine whether the screen needs updating.\nIn the Shared Space, each app updates its own UI and 3D content and syncs the updates to its view hierarchy and content to the shared render server. This server renders the updates from multiple apps running side-by-side. Then, the render server works with the compositor to generate final images to display of the UI and people’s surroundings. Metal apps in a Full Space use theCompositor Servicesframework to render drawable frames directly to the Compositor, bypassing the render server.\n\nIf your app running in the Shared Space has content or updates that take too long to render, the render server can miss deadlines. Visual updates you expect in one compositor frame don’t show up until a later frame.\n\nUse the RealityKit Trace template in Instruments to profile your app and identify workflows with dropped frames and other rendering and responsiveness bottlenecks. For more information, seeAnalyzing the performance of your visionOS app. To learn about optimizations you can make in your SwiftUI and UIKit interfaces, seeReducing the rendering cost of your UI on visionOS. To learn about optimizing your RealityKit content, seeReducing the rendering cost of RealityKit content on visionOS.\nWhen usingMetaland theCompositor Servicesframework to bypass the render server, use the Metal System Trace template to profile your app’s performance. For more info, seeAnalyzing the performance of your Metal app.\nPace your metal frame submissions so that the compositor receives a new frame from your app on each of its updates.\nPace your metal frame submissions so that the compositor receives a new frame from your app on each of its updates.\nMaintain a consistent metal rendering frame rate that is equal to the Apple Vision Pro display refresh rate. Use the visualizations in the Display instrument timeline to compare your rendered frame times to the display refresh rate — usually 90Hz on the Apple Vision Pro, but it can be higher under certain environmental conditions.\nMaintain a consistent metal rendering frame rate that is equal to the Apple Vision Pro display refresh rate. Use the visualizations in the Display instrument timeline to compare your rendered frame times to the display refresh rate — usually 90Hz on the Apple Vision Pro, but it can be higher under certain environmental conditions.\nQuery a new foveation map and pose prediction for each frame immediately before you use it to encode GPU work.\nQuery a new foveation map and pose prediction for each frame immediately before you use it to encode GPU work.\nAvoid long-running fragment and vertex shader executions from your Metal app, or from custom materials with Reality Composer Pro.\nAvoid long-running fragment and vertex shader executions from your Metal app, or from custom materials with Reality Composer Pro.\nAvoid any long frame stalls. If your app takes too long to submit a new frame to the compositor, the system terminates it.\nAvoid any long frame stalls. If your app takes too long to submit a new frame to the compositor, the system terminates it.\nTo learn more about implementing fully immersive Metal apps, seeDrawing fully immersive content using Metaland watch the videoDiscover Metal for immersive apps.\nSee Also\nPerformance",
    "https://developer.apple.com/documentation/visionos/incorporating-real-world-surroundings-in-an-immersive-experience": "visionOS\nIncorporating real-world surroundings in an immersive experience\nIncorporating real-world surroundings in an immersive experience\nIncorporating real-world surroundings in an immersive experience\nIncorporating real-world surroundings in an immersive experience\nOverview\nScene reconstruction helps bridge the gap between the rendered 3D content in your app and the person’s surroundings. Use scene reconstruction in ARKit to give your app an idea of the shape of the person’s surroundings and to bring your app experience into their world. Immersive experiences  — those that use themixedspace style — are best positioned to incorporate this kind of contextual information: scene reconstruction is only available in spaces and isn’t as relevant for thefullspace style.\nIn addition to providing a 3D mesh of the shape of different nearby objects, ARKit gives a classification to each mesh face it detects. For example, it might classify a face of a mesh as being part of an appliance, a piece of furniture, or structural information about the room like the position of walls and floors. The following video shows virtual cubes colliding with the scene reconstruction mesh, which makes the cubes appear to land on a table:\nConfigure a scene reconstruction session\nScene reconstruction requires theARKitSession.AuthorizationType.worldSensingauthorization type and corresponding usage description that you supply in your app’sInfo.plistfile. The following starts a session and processes updates as ARKit refines its reconstruction of the person’s surroundings:\nAdd real-world interactivity using collision components\nYou can make rendered 3D content more lifelike by having it appear to interact physically with objects in the person’s surroundings, like furniture and floors. Use RealityKit’s collision components and physics support to provide these interactions in your app. ThegenerateStaticMesh(from:)method bridges between scene reconstruction and RealityKit’s physics simulation.\nWarning\nBe mindful of how much content you include in immersive scenes that use themixedstyle. Content that fills a significant portion of the screen, even if that content is partially transparent, can prevent the person from seeing potential hazards in their surroundings. If you want to immerse the person in your content, configure your space with thefullstyle. For more information, seeCreating fully immersive experiences in your app.\nUse low-priority tasks to generate meshes, because generating them is a computationally expensive operation. The following creates a mesh entity with collision shapes using scene reconstruction:\nNote\nScene reconstruction meshes only support thePhysicsBodyMode.staticphysics body component mode.\nEach object in the scene reconstruction mesh updates itsoriginFromAnchorTransforminformation independently and requires a separate static mesh because ARKit subdivides its representation of the world into multiple, distinct sections.\nDisplay scene reconstruction meshes during debugging\nPeople using an app that leverages scene reconstruction typically don’t need to see a visual rendering of the scene reconstruction mesh. The system already shows passthrough video in an immersive experience. However, temporarily displaying the scene reconstruction mesh can help while you’re developing and debugging your app. In Xcode’s debugging toolbar, click the Enable Visualizations button and select Collision Shapes. Because each element of the scene reconstruction mesh has a collision component, the details of the mesh appear in the debug visualization. For more information, seeDiagnosing issues in the appearance of a running app.\nSee Also\nARKit",
    "https://developer.apple.com/documentation/visionos": "visionOS\nOverview\nvisionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.\n\nDeveloping for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app’s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS.\nExpand your app into immersive spaces\nStart with a familiar window-based experience to introduce people to your content. From there, addSwiftUIscene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences.\nBuild your app’s 3D content withRealityKitand Reality Composer Pro, and display it with aRealityView. In an immersive experience, useARKitto integrate your content with the person’s surroundings.\n\nExplore new kinds of interaction\nPeople can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures.SwiftUIprovides built-in support for these standard gestures, so rely on them for most of your app’s input. When you want to go beyond the standard gestures, useARKitto create custom gestures.\nTap to select\nPinch to rotate\nManipulate objects\nCreate custom gestures\nDive into featured sample apps\nExplore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash.\nTopics\nApp construction\nDesign\nSwiftUI\nRealityKit and Reality Composer Pro\nARKit\nVideo playback\nXcode and Simulator\nPerformance\niOS migration and compatibility\nEnterprise APIs for visionOS\nvisionOS\nOverview\nExpand your app into immersive spaces\nExplore new kinds of interaction\nDive into featured sample apps\nTopics",
    "https://developer.apple.com/documentation/visionos/introductory-visionos-samples": "visionOS\nIntroductory visionOS samples\nIntroductory visionOS samples\nIntroductory visionOS samples\nIntroductory visionOS samples\nOverview\nThe samples on this page are a starting point for developers new to visionOS. Each focuses on a specific feature, providing a solid foundation to build apps for the Apple Vision Pro.\nTopics\nBuilding shapes\nWorking with windows\nDrawing text\nImplementing an immersive space\nIntegrating ARKit\nBuilding materials\nApplying spatial audio\nCreating portals\nSee Also\nApp construction",
    "https://developer.apple.com/documentation/visionos/tracking-points-in-world-space": "visionOS\nTracking specific points in world space\nTracking specific points in world space\nTracking specific points in world space\nTracking specific points in world space\nOverview\nUse world anchors along with an ARKit session’sWorldTrackingProviderto track points of interest in the world over time, as a person moves while wearing the device, and across device usage sessions. For example, someone might place a 3D object in a specific position on their desk and expect it to come back the next time they use the device.\nARKit keeps track of a unique identifier for each world anchor your app creates and automatically places those anchors back in the space when the person returns to your app in the same location. A world tracking provider also provides the position of the device the person is wearing.\nStart an ARKit session with world tracking\nUse anARKitSessionconfigured for world tracking to start receiving updates on the world anchors your app places. The following shows updates to world anchors your app previously registered using theaddAnchor(_:)method:\nImportant\nIf a person repositions the current space — for example, by holding down the Digital Crown — world anchor updates begin updating their position relative to the new world origin. For example, a world anchor placed on a table still reports information about the table’s position, but those positions are relative to the updated world origin.\nCreate and add world anchors\nYou can create world anchors for any point of interest in your app’s world coordinate system once you’ve started a world tracking ARKit session. For example, you might track that a person placed an item at a particular offset from a desk in their space:\nOnce you add a world anchor to your app’s tracking provider using theaddAnchor(_:)method, theanchorUpdatessequence in the current session and future runs of your app provides updates to the current position of that new world anchor.\nPersist world anchors across sessions\nThe only information ARKit persists about the world anchors in your app is theirUUID— aWorldAnchorinstance’sidproperty — and pose in a particular space. It’s your app’s responsibility to persist additional information, such as the meaning of each anchor. For example, you might save local data about a custom 3D lamp model that a person placed on their desk.\nAs a person moves from town-to-town or room-to-room, your app won’t receive all of the world anchor updates from each place someone used your app. Instead, theanchorUpdatessequence only provides world anchors for nearby objects.\nTrack the device position in the world\nUse the Compositor Services framework and theWorldTrackingProviderclass’squeryDeviceAnchor(atTimestamp:)method to get low-latency information about the current and future-predicted pose of the person’s device in world space. For more information, seeDrawing fully immersive content using Metal.\nSee Also\nARKit",
    "https://developer.apple.com/documentation/visionos/adding-3d-content-to-your-app": "visionOS\nAdding 3D content to your app\nAdding 3D content to your app\nAdding 3D content to your app\nAdding 3D content to your app\nOverview\nA device with a stereoscopic display lets people experience 3D content in a way that feels more real. Content appears to have real depth, and people can view it from different angles, making it seem like it’s there in front of them.\nWhen building an app for visionOS, think about ways you might add depth to your app’s interface. The system provides several ways to display 3D content, including in your existing windows, in a volume, and in an immersive space. Choose the options that work best for your app and the content you offer.\nWindow\nVolume\nImmersive space\nAdd depth to traditional 2D windows\nWindows are an important part of your app’s interface. With visionOS, apps automatically get materials with the visionOS look and feel, fully resizable windows with spacing tuned for eyes and hands input, and access to highlighting adjustments for your custom controls.\nIncorporate depth effects into your custom views as needed, and use 3D layout options to arrange views in your windows.\nApply ashadow(color:radius:x:y:)orvisualEffect(_:)modifier to the view.\nApply ashadow(color:radius:x:y:)orvisualEffect(_:)modifier to the view.\nLift or highlight the view when someone looks at it using ahoverEffect(_:in:isEnabled:)modifier.\nLift or highlight the view when someone looks at it using ahoverEffect(_:in:isEnabled:)modifier.\nLay out views using aZStack.\nLay out views using aZStack.\nAnimate view-related changes withtransform3DEffect(_:).\nAnimate view-related changes withtransform3DEffect(_:).\nRotate the view using arotation3DEffect(_:axis:anchor:anchorZ:perspective:)modifier.\nRotate the view using arotation3DEffect(_:axis:anchor:anchorZ:perspective:)modifier.\nIn addition to giving 2D views more depth, you can also add static 3D models to your 2D windows. TheModel3Dview loads a USDZ file or other asset type and displays it at its intrinsic size in your window. Use this in places where you already have the model data in your app, or can download it from the network. For example, a shopping app might use this type of view to display a 3D version of a product.\nDisplay dynamic 3D scenes using RealityKit\nRealityKit is Apple’s technology for building 3D models and scenes that you update dynamically onscreen. In visionOS, use RealityKit and SwiftUI together to seamlessly couple your app’s 2D and 3D content. Load existing USDZ assets or create scenes in Reality Composer Pro that incorporate animation, physics, lighting, sounds, and custom behaviors for your content. To use a Reality Composer Pro project in your app, add the Swift package to your Xcode project and import its module in your Swift file. For more information, seeManaging files and folders in your Xcode project.\n\nWhen you’re ready to display 3D content in your interface, use aRealityView. This SwiftUI view serves as a container for your RealityKit content, and lets you update that content using familiar SwiftUI techniques.\nThe following example shows a view that uses aRealityViewto display a 3D sphere. The code in the view’s closure creates a RealityKit entity for the sphere, applies a texture to the surface of the sphere, and adds the sphere to the view’s content.\nWhen SwiftUI displays yourRealityView, it executes your code once to create the entities and other content. Because creating entities is relatively expensive, the view runs your creation code only once. When you want to update the state of your entities, change the state of your view and use an update closure to apply those changes to your content. The following example uses an update closure to change the size of the sphere when the value in thescaleproperty changes:\nFor information about how to create content using RealityKit, seeRealityKit.\nRespond to interactions with RealityKit content\nTo handle interactions with the entities of your RealityKit scenes:\nAttach a gesture recognizer to yourRealityViewand add thetargetedToAnyEntity()modifier to it.\nAttach a gesture recognizer to yourRealityViewand add thetargetedToAnyEntity()modifier to it.\nAttach anInputTargetComponentto the entity or one of its parent entities.\nAttach anInputTargetComponentto the entity or one of its parent entities.\nAdd collision shapes to the RealityKit entities that support interactions.\nAdd collision shapes to the RealityKit entities that support interactions.\nThetargetedToAnyEntity()modifier provides a bridge between the gesture recognizer and your RealityKit content. For example, to recognize when someone drags an entity, specify aDragGestureand add the modifier to it. When the specified gesture occurs on an entity, SwiftUI executes the provided closure.\nThe following example adds a tap gesture recognizer to the sphere view from the previous example. The code also addsInputTargetComponentandCollisionComponentcomponents to the shape to allow the interactions to occur. If you omit these components, the view doesn’t detect the interactions with your entity.\nDisplay 3D content in a volume\nA volume is a type of window that grows in three dimensions to match the size of the content it contains. Windows and volumes both accommodate 2D and 3D content, and are alike in many ways. However, windows clip 3D content that extends too far from the window’s surface, so volumes are the better choice for content that is primarily 3D.\nTo create a volume, add aWindowGroupscene to your app and set its style tovolumetric. This style tells SwiftUI to create a window for 3D content. Include any 2D or 3D views you want in your volume. You can also add aRealityViewto build your content using RealityKit. The following example creates a volume with a static 3D model of some balloons stored in the app’s bundle:\nWindows and volumes are a convenient way to display bounded 2D and 3D content, but your app doesn’t control the placement of that content in the person’s surroundings. The system sets the initial position of each window and volume at display time. The system also adds a window bar to allow someone to reposition the window or resize it.\n\nFor more information about when to use volumes, seeHuman Interface Guidelines > Windows.\nDisplay 3D content in a person’s surroundings\nWhen you need more control over the placement of your app’s content, add that content to anImmersiveSpace. An immersive space offers an unbounded area for your content, and you control the size and placement of content within the space. After receiving permission from the user, you can also use ARKit with an immersive space to integrate content into their surroundings. For example, you can use ARKit scene reconstruction to obtain a mesh of furniture and nearby objects and have your content interact with that mesh.\nAnImmersiveSpaceis a scene type that you create alongside your app’s other scenes. The following example shows an app that contains an immersive space and a window:\nIf you don’t add a style modifier to yourImmersiveSpacedeclaration, the system creates that space using themixedstyle. This style displays your content together with the passthrough content that shows the person’s surroundings. Other styles let you hide passthrough to varying degrees. Use theimmersionStyle(selection:in:)modifier to specify which styles your space supports. If you specify more than one style, you can toggle between the styles using theselectionparameter of the modifier.\nWarning\nBe mindful of how much content you include in immersive scenes that use themixedstyle. Content that fills a significant portion of the screen, even if that content is partially transparent, can prevent the person from seeing potential hazards in their surroundings. If you want to immerse the person in your content, configure your space with thefullstyle. For more information, see,Creating fully immersive experiences in your app.\nRemember to set the position of items you place in anImmersiveSpace. Position SwiftUI views using modifiers, and position a RealityKit entity using its transform component. SwiftUI places the origin of a space at a person’s feet initially, but can change this origin in response to other events. For example, the system might shift the origin to accommodate a SharePlay activity that displays your content with Spatial Personas. If you need to position SwiftUI views and RealityKit entities relative to one another, perform any needed coordinate conversions using the methods in thecontentparameter ofRealityView.\nTo display yourImmersiveSpacescene, open it using theopenImmersiveSpaceaction, which you obtain from the SwiftUI environment. This action runs asynchronously and uses the provided information to find and initialize your scene. The following example shows a button that opens the space with thesolarSystemidentifier:\nWhen an app presents anImmersiveSpace, the system hides the content of other apps to prevent visual conflicts. The other apps remain hidden while your space is visible but return when you dismiss it. If your app defines multiple spaces, you must dismiss the currently visible space before displaying a different space. If you don’t dismiss the visible space, the system issues a runtime warning when you try to open the other space.\nSee Also\nApp construction",
    "https://developer.apple.com/documentation/visionos/accessing-the-main-camera": "visionOS\nAccessing the main camera\nAccessing the main camera\nAccessing the main camera\nAccessing the main camera\nOverview\nThis sample code project demonstrates how to use ARKit to access and display the left main camera frame in your visionOS app. You can use this functionality to implement computer vision-powered experiences or live streaming in your enterprise app. For instance, support technicians can live stream their surroundings to remote experts for improved guidance.\nConfigure the sample code project\nReplaceEnterprise.licensewith your license file. The sample app requires a valid license file to display the main camera.\nRequest the entitlement\nMain camera access is a part of enterprise APIs for visionOS, a collection of APIs that unlock capabilities for enterprise customers. To use main camera access, you need to apply for theMain camera accessentitlement. For more information, including how to apply for this entitlement, seeBuilding spatial experiences for business apps with enterprise APIs for visionOS.\nAdd usage descriptions for ARKit data access\nTo help protect people’s privacy, visionOS limits app access to cameras and other sensors in Apple Vision Pro. You need to add anNSEnterpriseMCAMUsageDescriptionto your app’s information property list to provide a usage description that explains how your app uses the data these sensors provide. People see this description when your app prompts for access to camera data.\nNote\nIn visionOS, ARKit is only available in an immersive space. SeeSetting up access to ARKit datato learn more about opening an immersive space and requesting authorization for ARKit data access. To learn more about best practices for privacy, seeAdopting best practices for privacy and user preferences.\nAccess and display main camera frames\nThe following code example accesses and displays the left main camera at the highest available resolution. To access the camera, start anARKitSessionwith aCameraFrameProvider, and then requestCameraFrameProvider.CameraFrameUpdatesin a given format. ARKit delivers a stream ofCameraFrameinstances; each frame includes aCameraFrame.Samplecontaining apixelBufferandCameraFrame.Sample.Parametersdescribing the frame’s characteristics.\nTo display the frame’s content, convert itspixelBufferto anImageusing the following extension:\nSee Also\nEnterprise APIs for visionOS",
    "https://developer.apple.com/documentation/visionos/object-tracking-with-reality-composer-pro-experiences": "visionOS\nObject tracking with Reality Composer Pro experiences\nObject tracking with Reality Composer Pro experiences\nObject tracking with Reality Composer Pro experiences\nObject tracking with Reality Composer Pro experiences\nOverview\nNote\nThis sample code project is associated with WWDC24 session100101: Explore object tracking for visionOS.\nConfigure the sample code project\nSimulator doesn’t support ARKit, so you can only run this sample on a physical device. This sample can run on Apple Vision Pro with visionOS 2 or later.\nSee Also\nARKit",
    "https://developer.apple.com/documentation/visionos/bringing-your-app-to-visionos": "visionOS\nBringing your existing apps to visionOS\nBringing your existing apps to visionOS\nBringing your existing apps to visionOS\nBringing your existing apps to visionOS\nOverview\nIf you have an existing app that runs in iPadOS or iOS, you can build that app against the visionOS SDK to run it on the platform. Apps built specifically for visionOS adopt the standard system appearance, and they look more natural on the platform. Updating your app is also an opportunity to add elements that work well on the platform, such as 3D content and immersive experiences.\nIn most cases, all you need to do to support visionOS is update your Xcode project’s settings and recompile your code. Depending on your app, you might need to make additional changes to account for features that are only found in the iOS SDK. While most of the same technologies are available on both platforms, some technologies don’t make sense or require hardware that isn’t present on visionOS devices. For example, people don’t typically use a headset to make contactless payments, so apps that use the ProximityReader framework must disable those features when running in visionOS.\nNote\nIf you use ARKit in your iOS app to create an augmented reality experience, you need to make additional changes to support ARKit in visionOS. For information on how to update this type of app, seeBringing your ARKit app to visionOS.\nCreate a visionOS-specific version of your app\nTo update your app to build specifically for the visionOS SDK:\nIn your project’s settings, select your app target.\nIn your project’s settings, select your app target.\nNavigate to the General tab.\nNavigate to the General tab.\nIn Supported Destinations, click the Add (+) button to add a new destination and select the Apple Vision option.\nIn Supported Destinations, click the Add (+) button to add a new destination and select the Apple Vision option.\n\nWhen you add Apple Vision as a destination, Xcode makes some one-time changes to your project’s build settings. After you add the destination, you can modify your project’s build settings and build phases to customize the build behavior specifically for visionOS. For example, you might remove dependencies for the visionOS version of your app, or change the set of source files you want to compile.\nFor more information about how to update a target’s configuration, seeCustomizing the build phases of a target.\nUpdate your code for features that are unavailable in visionOS\nThe first step to updating your existing app to run in visionOS is to stop using deprecated frameworks and to check for frameworks with behavioral changes in visionOS.\nIf your app currently uses deprecated APIs or frameworks, update your code to use appropriate replacements. visionOS removes many deprecated symbols entirely, turning these deprecation warnings into missing-symbol errors on the platform. Fix any deprecation warnings in the iOS version of your code before you build for visionOS to see the original deprecation warning and replacement details. ReadDetermining whether to bring your app to visionOSfor a list of deprecated frameworks.\nTo help you avoid using APIs for unavailable features, many frameworks offer APIs to check the availability of those features. Continue to use those APIs and take appropriate actions when the features aren’t available. In other cases, prepare for the framework code to do nothing or to generate errors when you use it. ReadDetermining whether to bring your app to visionOSfor a list of APIs with availability checks and deprecated frameworks.\nThe iOS SDK includes many frameworks that don’t apply to visionOS, either because they use hardware that isn’t available or their features don’t apply to the platform. Move code that uses these frameworks to separate source files whenever possible, and include those files only in the iOS version of your app. The following frameworks are available in the iOS SDK but not in the visionOS SDK.\n\n\n\nActivityKit\nAdSupport\nApp Clips\nAutomatedDeviceEnrollment\nBusinessChat\nCarKey\nCarPlay\nCinematic\nClockKit\nCoreLocationUI\nCoreMediaIO\nCoreNFC\nCoreTelephony\nDeviceActivity\nDockKit\nExposureNotification\nFamilyControls\nFinanceKit\nFinanceKitUI\nManagedSettings\nManagedSettingsUI\nMessages\nMLCompute\nNearbyInteraction\nOpenAL\nOpenGLES\nProximityReader\nSafetyKit\nScreenTime\nSensorKit\nServiceManagement\nSocial\nTwitter\nWidgetKit\nWorkoutKit\n\nWhen you can’t isolate the code to separate source files, use conditional statements such as the ones below to offer a different code path for visionOS and iOS. The following example shows how to do this:\nFor additional information about how to isolate code to the iOS version of your app, seeRunning code on a specific platform or OS version.\nUpdate your interface to take advantage of visionOS features\nAfter your existing code runs correctly in visionOS, look for ways to improve the experience you offer on the platform. In visionOS, you can display content using more than just windows. Think about ways to incorporate the following elements into your interface:\nDepth.Many SwiftUI views use visual effects to add depth. Look for similar ways to incorporate depth into your own custom views. For guidance on how best to incorporate depth and 3D elements in your interface, seeHuman Interface Guidelines.\nDepth.Many SwiftUI views use visual effects to add depth. Look for similar ways to incorporate depth into your own custom views. For guidance on how best to incorporate depth and 3D elements in your interface, seeHuman Interface Guidelines.\n3D content.Think about where you might incorporate 3D models and shapes into your content. Use RealityKit to implement your content, and aRealityViewto present that content from your app. SeeAdding 3D content to your app.\n3D content.Think about where you might incorporate 3D models and shapes into your content. Use RealityKit to implement your content, and aRealityViewto present that content from your app. SeeAdding 3D content to your app.\nImmersive experiences.Present a space to immerse someone in your app’s content. Spaces let you place content anywhere in a person’s surroundings. You can also create fully immersive experiences that display only your app’s content. SeeCreating fully immersive experiences in your app.\nImmersive experiences.Present a space to immerse someone in your app’s content. Spaces let you place content anywhere in a person’s surroundings. You can also create fully immersive experiences that display only your app’s content. SeeCreating fully immersive experiences in your app.\nInteractions with someone’s surroundings.Use ARKit to facilitate interactions between your content and the surroundings. For example, detect planar surfaces to use as anchor points for your content. SeeARKitfor more details.\nInteractions with someone’s surroundings.Use ARKit to facilitate interactions between your content and the surroundings. For example, detect planar surfaces to use as anchor points for your content. SeeARKitfor more details.\nColor.Standard visionOS windows typically use the system-defined glass material, which lets light and objects from people’s physical surroundings show through. Use color sparingly to draw attention to specific elements. Refer to the visionOS platform specific guidelines in the Human Interface GuidelinesColordocumentation for best practices.\nColor.Standard visionOS windows typically use the system-defined glass material, which lets light and objects from people’s physical surroundings show through. Use color sparingly to draw attention to specific elements. Refer to the visionOS platform specific guidelines in the Human Interface GuidelinesColordocumentation for best practices.\nSystem-provided UI components.Take advantage of the components built for visionOS in SwiftUI. Components likeTabViewappear on the bottom of iOS or iPadOS apps and on the left side of visionOS apps. Other UI elements, like ornaments, are introduced specifically for visionOS. Using these elements keeps your interface adaptable between platforms and provides hover effects for free.\nSystem-provided UI components.Take advantage of the components built for visionOS in SwiftUI. Components likeTabViewappear on the bottom of iOS or iPadOS apps and on the left side of visionOS apps. Other UI elements, like ornaments, are introduced specifically for visionOS. Using these elements keeps your interface adaptable between platforms and provides hover effects for free.\nAdditional information on how to design your layout for visionOS is available on the Human Interface GuidelinesLayoutpage.\nNote\nIf you use ARKit in your iOS app to create an augmented reality experience, you need to make additional changes to support ARKit in visionOS. For information on how to update this type of app, seeBringing your ARKit app to visionOS.\nConsider your implementation\nIf your existing app is built with UIKit, consider your implementation plan for visionOS. Although you can still use UIKit and load iOS storyboards into your app, you can’t include visionOS-specific or 3D content without using SwiftUI.\nConsider migrating your app to SwiftUI. The declarative syntax gives you less code to maintain and makes it easier to validate that your interface does what you want. SwiftUI is also unified across all Apple platforms and adapts to device-specific behavior without extra work. To learn more about migrating to the SwiftUI lifecycle, seeMigrating to the SwiftUI life cycle. You can also continue using UIKit views in a SwiftUI app withUIViewRepresentable.\nIf you plan to continue using UIKit, readCreate adaptive layouts in UIKitto ensure your layout looks good on visionOS. The WWDC23 videoMeet UIKit for Spatial Computingwalks through bringing your UIKit app to visionOS and shows how to take advantage of native features for visionOS by including SwiftUI views usingUIHostingController.\nFor a tutorial on mixing SwiftUI and UIKit content, seeInterfacing with UIKit.\nCreate adaptive layouts in UIKit\nAlthough visionOS is built for SwiftUI adoption, you can continue to use and build out your UIKit codebase as well. If your UIKit app uses hardcoded values or relies onUIScreenfor layout, the first step to migrating your app to visionOS is to start using an adaptable layout. When you make decisions using device details, your app might produce inconsistent or erroneous results on an unknown device type, or it might fail altogether. Find solutions that rely on environmental information, rather than the device type. For example, SwiftUI and UIKit start layout using the app’s window size, which isn’t necessarily the same size as the device’s display.\nNote\nDevice-specific information is available when you absolutely need it, but validate the information you receive and provide reasonable default behavior for unexpected values.\nThink about ways to create adaptive layouts using the following techniques:\nUse stack views.UIStackViewobjects adjust the position of their contained views automatically when interface dimensions change. Alternatively,Auto Layoutconstraints let you specify the rules that determine the size and position of the views in your interface.\nUse stack views.UIStackViewobjects adjust the position of their contained views automatically when interface dimensions change. Alternatively,Auto Layoutconstraints let you specify the rules that determine the size and position of the views in your interface.\nStay within layout margins.ReadPositioning content within layout marginsto set up constraints that respect layout margins and don’t crowd other content.\nStay within layout margins.ReadPositioning content within layout marginsto set up constraints that respect layout margins and don’t crowd other content.\nRespect the safe area.Place views so they’re not obstructed by other content. Each view has alayout guidethat helps you create constraints to position your views within the safe area. ReadPositioning content relative to the safe areafor guidance.\nRespect the safe area.Place views so they’re not obstructed by other content. Each view has alayout guidethat helps you create constraints to position your views within the safe area. ReadPositioning content relative to the safe areafor guidance.\nAdapt based on changes in UITraitCollection.Write code to adjust your app’s layout according to changes in the iOS interface elements, such as size class, display scale, and layout direction. ReadUITraitCollectionfor more information.\nAdapt based on changes in UITraitCollection.Write code to adjust your app’s layout according to changes in the iOS interface elements, such as size class, display scale, and layout direction. ReadUITraitCollectionfor more information.\nUpdate your app’s assets\nAdd vector-based or high-resolution images to your project specifically to support visionOS. In visionOS, people can view your app’s content at different angles and different distances, so image pixels rarely line up with screen pixels. Vector-based images work best because they maintain their detail and crispness at any size. For bitmap-based images, use high-resolution images (@2xor better) to ensure they retain detail at different sizes.\nFor more information about designing images for your app, seeImagesin Human Interface Guidelines.\nSee Also\niOS migration and compatibility",
    "https://developer.apple.com/documentation/visionos/locating-and-decoding-barcodes-in-3d-space": "visionOS\nLocating and decoding barcodes in 3D space\nLocating and decoding barcodes in 3D space\nLocating and decoding barcodes in 3D space\nLocating and decoding barcodes in 3D space\nOverview\nThis sample code project demonstrates how to use barcode detection in ARKit to detect, decode, and place content near barcodes in your visionOS app. ARKit enables your visionOS app to detect barcodes (including QR codes), locate their positions in a person’s surroundings, and decode their contents. For example, a warehouse worker can use your app to retrieve an item by looking at a package’s barcode to confirm its contents.\nConfigure your sample code project\nReplaceEnterprise.licensewith your license file. The sample app requires a valid license file to detect barcodes.\nRequest the entitlement\nBarcode detection is a part of enterprise APIs for visionOS, a collection of APIs that unlock capabilities for enterprise customers. To use barcode detection, you need to apply for theSpatial barcode and QR code scanningentitlement. For more information, including how to apply for this entitlement, seeBuilding spatial experiences for business apps with enterprise APIs for visionOS.\nAdd usage descriptions for ARKit data access\nTo help protect people’s privacy, visionOS limits app access to cameras and other sensors in Apple Vision Pro. You need to add anNSWorldSensingUsageDescriptionto your app’s information property list to provide a usage description that explains how your app uses the data those sensors provide. People see this description when your app prompts for access to world-sensing data.\nNote\nIn visionOS, ARKit is only available in an immersive space. SeeSetting up access to ARKit datato learn more about opening an immersive space and requesting authorization for ARKit data access. To learn more about best practices for privacy, seeAdopting best practices for privacy and user preferences.\nDetect, decode, and highlight barcodes\nYour visionOS app can detect barcodes in a person’s surroundings and highlight the barcode the person is looking for. The following code example detects and highlights every Code 128 or QR Code symbology in a person’s surroundings. The code example includes three steps: detecting the barcode, printing its decoded content, and creating the highlight animation.\nTo start detecting barcodes, create aBarcodeDetectionProviderto get the positions of barcodes. Next, specify theBarcodeAnchor.Symbologyto indicate the types of barcodes you want ARKit to detect in the person’s surroundings. Then, start anARKitSessionwith theBarcodeDetectionProvider.\nARKit delivers an asynchronous stream of updates as it detects changes in the scene. Each update includes aBarcodeAnchorcontaining the barcode’s payload, extent, and transform. To implement the highlight animation, create a plane that you size and position to match that of the barcode, then fade it in and out.\nNote\nYou define a barcode’s extents in the x-z plane. They have a width (x-axis), depth (z-axis), and zero height (y-axis).\nNote\nBecauseBarcodeDetectionProviderhas a low refresh rate, use its transform to initialize the position of content relative to a stationary barcode.\nDetermine the ideal barcode width\nThe sample code project can’t read barcodes that are too small to appear clearly in a person’s field of view. Larger barcodes generally improve readability, providing that they remain within the field of view. The minimum barcode size depends on itsBarcodeAnchor.Symbology. Refer to the table below to determine the minimum width required for a barcode to be readable under nominal lighting conditions (100 lux) at an arm’s length distance (~40 cm).\nSymbology\nMinimum width (in cm)\nAztec\tCode\n2.0\nCodabar\n5.5\nCode 39\n6.5\nCode 39 Checksum\n6.5\nCode 39 Full ASCII\n3.0\nCode 39 Full ASCII Checksum\n4.5\nCode 93\n5.0\nCode 93i\n5.0\nCode 128\n2.5\nData Matrix\n1.0\nEAN-8\n3.0\nEAN-13\n4.0\nGS1 DataBar\n3.0\nGS1 DataBar Expanded\n6.5\nGS1 DataBar Limited\n3.0\nITF\n3.5\nITF-14\n5.0\nITF Checksum\n3.5\nMicroPDF417\n6.5\nMicro QR Code\n2.0\nMSI Plessey\n4.5\nPDF417\n6.0\nQR Code\n1.5\nUPC-E\n2.5\nSee Also\nEnterprise APIs for visionOS",
    "https://developer.apple.com/documentation/visionos/creating-fully-immersive-experiences": "visionOS\nCreating fully immersive experiences in your app\nCreating fully immersive experiences in your app\nCreating fully immersive experiences in your app\nCreating fully immersive experiences in your app\nOverview\nA fully immersive experience replaces everything the person sees with custom content you create. You might use this type of experience to:\nOffer a temporary transitional experience\nOffer a temporary transitional experience\nCreate a distraction-free space for your content\nCreate a distraction-free space for your content\nImplement a virtual reality (VR) game\nImplement a virtual reality (VR) game\nPresent a virtual world to explore\nPresent a virtual world to explore\nWith a fully immersive experience, you’re responsible for everything that appears onscreen. The system hides passthrough video and displays the content you provide, showing the person’s hands only when they come into view. To achieve the best performance, use RealityKit or Metal to create and animate your content.\nTypically, you combine a fully immersive experience with other types of experiences and provide transitions between them. When you display a window first and then offer controls to enter your immersive experience, you give people time to prepare for the transition. It also gives them the option to skip the experience if they prefer to use your app’s windows instead.\nPrepare someone for your app’s transitions\nGive people control over when they enter or exit fully immersive experiences, and provide clear transitions to and from those experiences. Clear visual transitions make it easier to adjust to such a large change. Sudden transitions might be disorienting, unpleasant, or make the person think something went wrong.\nAt launch time, display windows or other content that allows the person to see their surroundings. Add controls to that content to initiate the transition to the fully immersive experience, and provide a clear indication of what the controls do. Inside your experience, provide clear controls and instructions on how to exit the experience.\nWarning\nWhen you start a fully immersive experience, visionOS defines a system boundary that extends approximately 1.5 meters from the initial position of the person’s head. If their head moves outside of that zone, the system automatically stops the immersive experience and turns on the external video again. This feature is an assistant to help prevent someone from colliding with objects.\nFor guidelines on how to design fully immersive experiences, seeHuman Interface Guidelines.\nOpen an immersive space\nTo create a fully immersive experience, open anImmersiveSpaceand set its style tofull. An immersive space is a type of SwiftUI scene that lets you place content anywhere in the person’s surroundings. Applying thefullstyle to the scene tells the system to hide passthrough video and display only your app’s content.\nDeclare spaces in thebodyproperty of your app object, or anywhere you manage SwiftUI scenes. The following example shows an app with a main window and a fully immersive space. At launch time, the app displays the window.\nTo display anImmersiveSpace, open it using theopenImmersiveSpaceaction, which you obtain from the SwiftUI environment. This action runs asynchronously and uses the provided information to find and initialize your scene. The following example shows a button that opens the space with thesolarSystemidentifier:\nAn app can display only one space at a time, and it’s an error for you to try to open a space while another space is visible. To dismiss an open space, use thedismissImmersiveSpaceaction.\nFor more information about displaying spaces, see theImmersiveSpacetype.\nDraw your content using RealityKit\nRealityKit works well when your content consists of primitive shapes or existing content in USD files. Organize the contents of your scene using RealityKit entities, and animate that content using components and systems. Use Reality Composer Pro to assemble your content visually, and to attach dynamic shaders, animations, audio, and other behaviors to your content. Display the contents of your RealityKit scene in aRealityViewin your scene.\nTo load a Reality Composer Pro scene at runtime, fetch the URL of your Reality Composer Pro package file, and load the root entity of your scene. The following example shows how to create the entity for a package located in the app’s bundle:\nFor more information about how to display content in aRealityViewand manage interactions with your content, seeAdding 3D content to your app.\nDraw your content using Metal\nAnother option for creating fully immersive scenes is to draw everything yourself using Metal. When using Metal to draw your content, use the Compositor Services framework to place that content onscreen. Compositor Services provides the code you need to set up your Metal rendering engine and start drawing.\nFor details on how to render content using Metal and Compositor Services, and manage interactions with your content, seeDrawing fully immersive content using Metal.\nSee Also\nApp construction",
    "https://developer.apple.com/documentation/visionos#Enterprise-APIs-for-visionOS": "visionOS\nOverview\nvisionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.\n\nDeveloping for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app’s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS.\nExpand your app into immersive spaces\nStart with a familiar window-based experience to introduce people to your content. From there, addSwiftUIscene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences.\nBuild your app’s 3D content withRealityKitand Reality Composer Pro, and display it with aRealityView. In an immersive experience, useARKitto integrate your content with the person’s surroundings.\n\nExplore new kinds of interaction\nPeople can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures.SwiftUIprovides built-in support for these standard gestures, so rely on them for most of your app’s input. When you want to go beyond the standard gestures, useARKitto create custom gestures.\nTap to select\nPinch to rotate\nManipulate objects\nCreate custom gestures\nDive into featured sample apps\nExplore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash.\nTopics\nApp construction\nDesign\nSwiftUI\nRealityKit and Reality Composer Pro\nARKit\nVideo playback\nXcode and Simulator\nPerformance\niOS migration and compatibility\nEnterprise APIs for visionOS\nvisionOS\nOverview\nExpand your app into immersive spaces\nExplore new kinds of interaction\nDive into featured sample apps\nTopics",
    "https://developer.apple.com/documentation/visionos#Video-playback": "visionOS\nOverview\nvisionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.\n\nDeveloping for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app’s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS.\nExpand your app into immersive spaces\nStart with a familiar window-based experience to introduce people to your content. From there, addSwiftUIscene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences.\nBuild your app’s 3D content withRealityKitand Reality Composer Pro, and display it with aRealityView. In an immersive experience, useARKitto integrate your content with the person’s surroundings.\n\nExplore new kinds of interaction\nPeople can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures.SwiftUIprovides built-in support for these standard gestures, so rely on them for most of your app’s input. When you want to go beyond the standard gestures, useARKitto create custom gestures.\nTap to select\nPinch to rotate\nManipulate objects\nCreate custom gestures\nDive into featured sample apps\nExplore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash.\nTopics\nApp construction\nDesign\nSwiftUI\nRealityKit and Reality Composer Pro\nARKit\nVideo playback\nXcode and Simulator\nPerformance\niOS migration and compatibility\nEnterprise APIs for visionOS\nvisionOS\nOverview\nExpand your app into immersive spaces\nExplore new kinds of interaction\nDive into featured sample apps\nTopics",
    "https://developer.apple.com/documentation/visionos#SwiftUI": "visionOS\nOverview\nvisionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.\n\nDeveloping for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app’s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS.\nExpand your app into immersive spaces\nStart with a familiar window-based experience to introduce people to your content. From there, addSwiftUIscene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences.\nBuild your app’s 3D content withRealityKitand Reality Composer Pro, and display it with aRealityView. In an immersive experience, useARKitto integrate your content with the person’s surroundings.\n\nExplore new kinds of interaction\nPeople can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures.SwiftUIprovides built-in support for these standard gestures, so rely on them for most of your app’s input. When you want to go beyond the standard gestures, useARKitto create custom gestures.\nTap to select\nPinch to rotate\nManipulate objects\nCreate custom gestures\nDive into featured sample apps\nExplore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash.\nTopics\nApp construction\nDesign\nSwiftUI\nRealityKit and Reality Composer Pro\nARKit\nVideo playback\nXcode and Simulator\nPerformance\niOS migration and compatibility\nEnterprise APIs for visionOS\nvisionOS\nOverview\nExpand your app into immersive spaces\nExplore new kinds of interaction\nDive into featured sample apps\nTopics",
    "https://developer.apple.com/documentation/visionos/setting-up-access-to-arkit-data": "visionOS\nSetting up access to ARKit data\nSetting up access to ARKit data\nSetting up access to ARKit data\nSetting up access to ARKit data\nOverview\nIn visionOS, ARKit can enable new kinds of experiences that leverage data such as hand tracking and world sensing. The system gates access to this kind of sensitive information. Because people can decline your app’s request to use ARKit data or revoke access later, you need to provide alternative ways to use your app and to handle cases where your app loses access to data.\n\nAdd usage descriptions for ARKit data access\nPeople need to know why your app wants to access data from ARKit. Add the following keys to your app’s information property list to provide a user-facing usage description that explains how your app uses the data:\nUse this key if your app uses hand tracking.\nUse this key if your app uses image tracking, plane detection, or scene reconstruction.\nNote\nWorldtracking— unlike worldsensing— doesn’t require authorization. For more information, seeTracking specific points in world space.\nChoose between up-front or as-needed authorization\nYou can choose when someone sees an authorization request to use ARKit data. If you need precise control over when the request appears, call therequestAuthorization(for:)method onARKitSessionto explicitly authorize access at the time you call it. Otherwise, people see an authorization request when you call therun(_:)method. This is an implicit authorization because the timing of the request depends entirely on when you start the session.\nOpen a space and run a session\nTo help protect people’s privacy, ARKit data is available only when your app presents a Full Space and other apps are hidden. Present one of these space styles before calling therun(_:)method.\nThe following shows an app structure that’s set up to use a space with ARKit:\nCallopenImmersiveSpacefrom your app’s user interface to create a space, start running an ARKit session, and kick off an immersive experience. The following shows a simple view with a button that opens the space:\nProvide alternatives for declined and revoked authorizations\nSomeone might not want to give your app access to data from ARKit, or they might choose to revoke that access later in Settings. Handle these situations gracefully, and remove or transition content that depends on ARKit data. For example, you might fade out content that you need to remove, or recenter content to an appropriate starting position. If your app uses ARKit data to place content in a person’s surroundings, consider letting people place content using the system-provided interface.\nProviding alternatives is especially important if you’re using ARKit for user input. People using accessibility features, trackpads, keyboards, or other forms of input might need a way to use your app without ARKit.\nSee Also\nARKit",
    "https://developer.apple.com/documentation/visionos/reducing-the-rendering-cost-of-realitykit-content-on-visionos": "visionOS\nReducing the rendering cost of RealityKit content on visionOS\nReducing the rendering cost of RealityKit content on visionOS\nReducing the rendering cost of RealityKit content on visionOS\nReducing the rendering cost of RealityKit content on visionOS\nOverview\nThe complexity of the assets and features you use in aRealityViewhave a big impact on the work your app and the render server do to render each frame. On visionOS, the system continuously renders 3D content in response to changes in head position and also incorporates any changes you make through system updates. Performance bottlenecks that prevent timely rendering and responsive feedback interfere with the spatial experience.\nTo identify any performance bottlenecks, use the RealityKit Trace template to profile your app. The RealityKit Metrics instrument that it includes collects data on 3D mesh rendering, spatial systems, entity commits, animations, physics, and particles effects work. If you encounter performance bottlenecks in yourRealityKitscenes, attempt to reduce the overhead in the bottleneck area to improve the rendering and responsiveness of your app. With careful balancing, your app can maintain utility and realism without exhausting the resources available on the system. Minimize overhead in less noticeable areas while prioritizing aspects of your app that provide critical functionality and an engaging experience.\nFor information on how input propagates through the system and updates content on visionOS, seeUnderstanding the visionOS render pipeline. For more information on using the RealityKit Trace template in Instruments to profile your app, seeAnalyzing the performance of your visionOS app.\nRelated sessions from WWDC23\n\nSession 10099:Meet RealityKit Trace\nSession 10100:Optimize app power and performance for spatial computing\nSession 10274:Create 3D models for Quick Look spatial experiences\nSession 10160:Demystify SwiftUI performance\nSession 10080:Build spatial experiences with RealityKit\nReduce the complexity of meshes and materials\nTo reduce the render server’s CPU overhead, lower draw call counts. One way to do this is to combine parts of your mesh that share a material. The render server performs a draw call for each individual part and uses the CPU to setup each call it sends to the GPU. While combining parts can improve performance, avoid combining parts that are far apart in your scene or that become too large to stay in the field of view when combined. The system renders the entire mesh for a part that is only partially in the field of view. It doesn’t render a part that completely falls outside the field of view.\nTo reduce both the CPU time the render server spends to create data for the draw calls and rendering work on the GPU, lower vertex and triangle counts, and optimize your meshes for overdraw.Overdrawis drawing pixels multiple times to produce the final result.\nTo identify areas of complex mesh rendering, check the RealityKit Metrics instrument for 3D Render Encoding (CPU), 3D Render GPU (GPU), and GPU Work Stall (CPU) bottlenecks. The instrument collects metrics on the number of 3D Mesh Draw calls, 3D Mesh Triangles, and 3D Mesh Vertices. Expand the view of the RealityKit Metrics instrument in the timeline pane to reveal graphs of this data under the 3D Render section. Select the section to view these metrics under Summary: Reality Module Metrics in the detail pane.\n\nIn general, use less than:\n250 draw calls in the Shared Space (or 500 in a Full Space)\n250 draw calls in the Shared Space (or 500 in a Full Space)\n250k vertices and 250k triangles in the Shared Space (or 500k in a Full Space)\n250k vertices and 250k triangles in the Shared Space (or 500k in a Full Space)\nFind a balance between the other work your app needs to complete and the features of your 3D meshes. Complex meshes and materials in addition to frequent mesh draw calls can create CPU performance bottlenecks. The GPU overhead of visual effects the system applies to 3D content can make the effects of these complexities worse. Under certain conditions, the system applies visual effects automatically, for example, when an app’s scenes overlap a scene from another app or a person’s surroundings. In these cases, you can turn off other effects, such as grounding shadows. To turn off grounding shadows, toggle theGroundingShadowComponentsetting in Reality Composer Pro. You can efficiently use Physically Based materials in Reality Composer Pro for smaller, fully opaque content. For larger content or content you choose to make transparent, the environmental lighting these materials use is expensive. Consider using a custom material with an unlit surface and add lighting textures or other less expensive effects to produce a similar effect.\nLoad assets efficiently\nComplex assets take a long time to load, contribute to longer app launches, and can trigger expensive render updates. To reduce asset loading times and their impact:\nExport files from Reality Composer Pro to use in your visionOS project. Reality Composer Pro optimizes the content in these files for fast loading and lower memory costs.\nExport files from Reality Composer Pro to use in your visionOS project. Reality Composer Pro optimizes the content in these files for fast loading and lower memory costs.\nMinimize the number and size of expensive file types in your assets, such as textures, meshes, audio, and video files. Reality Composer Pro performs texture compression automatically when you use it to export your assets.\nMinimize the number and size of expensive file types in your assets, such as textures, meshes, audio, and video files. Reality Composer Pro performs texture compression automatically when you use it to export your assets.\nMinimize the number of entities, textures, and primitives that USD models contain and the number of distinct and custom materials they use.\nMinimize the number of entities, textures, and primitives that USD models contain and the number of distinct and custom materials they use.\nPreload assets that contain custom materials compiling shaders at runtime.\nPreload assets that contain custom materials compiling shaders at runtime.\nUse asynchronous loading APIs to avoid blocking the main thread. This can be especially useful when you need to load multiple assets together.\nUse asynchronous loading APIs to avoid blocking the main thread. This can be especially useful when you need to load multiple assets together.\nReuse assets as much as possible for entities that can share the same resources. Entities that use the same assets and models can load the file once and share instances.\nReuse assets as much as possible for entities that can share the same resources. Entities that use the same assets and models can load the file once and share instances.\nThe complexity of an asset is primarily depends on its size and the amount of sub-assets it contains. When a RealityView loads an asset, your app’s process performs the work to load the asset but the system still registers the asset entities with the render server. The RealityKit Metrics instrument might detect a bottleneck, or stall, in the render server while loading an expensive asset.\nMinimize updates\nWhen your custom systems modify entities and audio components, your app uses the CPU to encode the update. Then the render server performs additional work on the CPU to decode it and apply the changes to your content. To reduce the number of updates that your 3D RealityKit scenes make per frame:\nMinimize the amount of entity creation and destruction that your app does per frame, especially for attachments. Create entities in advance, and hide or show them as needed by adding or removing them from the hierarchy or toggling the value ofisEnabled.\nMinimize the amount of entity creation and destruction that your app does per frame, especially for attachments. Create entities in advance, and hide or show them as needed by adding or removing them from the hierarchy or toggling the value ofisEnabled.\nPerform work in custom systems only when you require it, rather than on every single frame.\nPerform work in custom systems only when you require it, rather than on every single frame.\nChange properties deliberately and avoid altering them unnecessarily.\nChange properties deliberately and avoid altering them unnecessarily.\nReduce the number of entities your app updates in response to events and actions. One way to accomplish this is to flatten your mesh entity hierarchies.\nReduce the number of entities your app updates in response to events and actions. One way to accomplish this is to flatten your mesh entity hierarchies.\nLower the update rates of code based animations and reduce the number of entities that animations update.\nLower the update rates of code based animations and reduce the number of entities that animations update.\nAvoid triggering excessiveSwiftUIredraw when updating RealityKit entities.\nAvoid triggering excessiveSwiftUIredraw when updating RealityKit entities.\nNote\nCertain actions initiate updates indirectly. For example, a physics simulation might cause transform updates.\nTo identify areas of frequent and complex updates, check the RealityKit Metrics instrument for Entity Commits (CPU) and  Custom RealityKit Systems (CPU) bottlenecks. The instrument collects metrics on the number of app updates the render server receives and the number of entities that it creates and destroys. Expand the view of the RealityKit Metrics instrument in the timeline pane to reveal graphs of this data under the Entity Commits section. Select the section to view a summary of these metrics in the detail pane.\n\nThe number of updates the render server gets from all apps over a particular interval.\nThe number of entities the render server creates and destroys across frames.\nNote\nDepending on the type of content, you might see additional bottlenecks that result from these updates. Transform updates, animations, material updates, asset loading, and view hierarchy updates also cause the render server to redraw 3D content.\nOptimize animations\nMinimize the CPU overhead of your animations and the total number of entities your animation affects.\nFor transform and material animations, minimize the number of entities you impact to lower the cost your app incurs. For example, you might use a custom RealityKit system to trigger and run these animations and callmove(to:relativeTo:)or make changes to materials, rotation, scale and transform properties in your app’s update loop. Each entity you impact adds to the work your custom systems do and results in additional entity commits between your app and the render server.\nFor code based animations, synchronize updates with the device’s display refresh rate. Inconsistent animation update intervals create a poor visual experience. Avoid creating material animations by generating a sequence of new materials at runtime. Instead, use a single material and animate a set of parameters usingParameterSetandMaterialParameters. Retain a copy of the material structs in your app to reuse across frames. This approach avoids redundant allocations and unnecessary overhead.\nFor Skeletal animations, reduce the total number of vertices in mesh geometries, weights per vertex, and separate the animations in content where possible. The render server does most of the work to implement these animations running shader deformers on the GPU. Limiting the cost of these operations is important; their overhead is greater when operating on more complex mesh geometries.\nTo identify areas of frequent and complex updates, check the RealityKit Metrics instrument for RealityKit Animations (GPU) and RealityKit Animations (CPU) bottlenecks. The RealityKit Metrics instrument collects metrics on number of skeletal animations running. Expand the view of the RealityKit Metrics instrument in the timeline pane to reveal graphs of this data under the RealityKit Animation section. Select the section to view a summary of these metrics in the detail pane.\n\nReduce collisions\nCollisions between objects in the Shared Space result in expensive physics calculations. To reduce the number of collisions and the work the system performs to compute collisions:\nRemove colliders and dynamic rigid bodies from a scene that aren’t necessary to produce the experience you want to create.\nRemove colliders and dynamic rigid bodies from a scene that aren’t necessary to produce the experience you want to create.\nSpace objects apart or organize objects into groups to avoid clustering. For example, 200 rigid bodies bouncing about in a single box results in more collisions than 50 rigid bodies bouncing around in 4 separate boxes.\nSpace objects apart or organize objects into groups to avoid clustering. For example, 200 rigid bodies bouncing about in a single box results in more collisions than 50 rigid bodies bouncing around in 4 separate boxes.\nTry different shapes. Choose collision shapes that minimize contact. Simpler shapes require less compute time. Avoid usinggenerateCollisionShapes(recursive:static:)when you can pick a simple shape. This function might create a more complex shape to fit a mesh.\nTry different shapes. Choose collision shapes that minimize contact. Simpler shapes require less compute time. Avoid usinggenerateCollisionShapes(recursive:static:)when you can pick a simple shape. This function might create a more complex shape to fit a mesh.\nMinimize the number of colliders. Using collider components for tap interactions is expensive when you create a lot of colliders.\nMinimize the number of colliders. Using collider components for tap interactions is expensive when you create a lot of colliders.\nReduce the number of physics objects stacked on top of one another. Stacked and overlapping physics shapes require more memory to track the overlapping contact pairs.\nReduce the number of physics objects stacked on top of one another. Stacked and overlapping physics shapes require more memory to track the overlapping contact pairs.\nTip\nTake into account a mesh’s structure when you usegenerateCollisionShapes(recursive:static:)to generate physics shapes. This function recursively generates shapes for the meshes, but the shapes might be inefficient for collisions. SetisStaticto true to generate static colliders that are more efficient.\nTo identify areas with high CPU overhead resulting from collisions and physics calculations, check the RealityKit Metrics instrument for RealityKit Physics (CPU) bottlenecks. Expand the RealityKit Metrics instrument in the timeline pane to reveal graphs of this data under the RealityKit Physics section. Select the section to view a summary of this data in the detail pane.\n\nLower the impact of particle effects\nTo reduce the overhead of particle systems and the number of draw calls they generate, minimize the the total number of particles you have in your app. Often, you can create a similar effects with fewer particles. Without increasing the total number of particles, try to produce more particles per particle emitter rather than running several particle emitters synchronously with a small number of particles each. Loading particle effects is also an expensive operation, so preload them whenever possible and avoid loading several at the same time.\nExperiment with different particle effects, shapes, and shaders when attempting to reduce the overhead of your particles on the GPU. The total number of vertices and triangles for all particles, and their material and transparency, affect the amount of GPU work and overdraw the particles require and the number of pixels your effects take up.\nTo identify areas with high GPU overhead resulting from particle effects simulation, check the RealityKit Metrics instrument for 3D Render GPU bottlenecks. To identify areas with high CPU overhead, check for Particles Update (CPU) bottlenecks. The instrument collects metrics on the number of Particle Draw Calls, Particle Triangles, and Particle Vertices. Expand the RealityKit Metrics instrument in the timeline pane to reveal graphs of this data under the 3D Render section. Select the section to view a summary of this data in the detail pane.\nView scene statistics in Reality Composer Pro\nTake advantage of the statistics Reality Composer Pro provides when you use it to author RealityKit scenes. To view this data:\nOpen the project’s Reality Composer Pro (.realitycomposerpro) file in Reality Composer Pro.\nOpen the project’s Reality Composer Pro (.realitycomposerpro) file in Reality Composer Pro.\nTo select a scene to inspect, click on its tab at the top of the project window.\nTo select a scene to inspect, click on its tab at the top of the project window.\nChoose View > Statistics to display the statistics at the bottom of the window.\nChoose View > Statistics to display the statistics at the bottom of the window.\n\nThe tool provides information about entity counts, physics, animations, particle emitters, materials, lighting effects, and mesh geometry that effect rendering overhead. For more information about Reality Composer Pro, seeDesigning RealityKit content with Reality Composer Pro.\nSee Also\nPerformance",
    "https://developer.apple.com/documentation/visionos/exploring_object_tracking_with_arkit": "visionOS\nExploring object tracking with ARKit\nExploring object tracking with ARKit\nExploring object tracking with ARKit\nExploring object tracking with ARKit\nOverview\nThe sample app demonstrates how to use a reference object to discover and track a specific object in a person’s surroundings in visionOS. This capability allows you to create engaging experiences based on objects in a person’s surroundings and lets you  attach digital content to these objects. For example, you can build an app that uses reference objects to describe the specific assembly of a machine a person is testing or repairing. Using this reference model, when ARKit recognizes that object, you can attach digital content to it, such as a diagram of the device, more information about its function, and so on.\nThis sample includes a reference object that ARKit uses to recognize a Magic Keyboard in someone’s surroundings.\nNote\nThis sample code project is associated with WWDC24Session 10101 — Explore object tracking for visionOS.\nConfigure the sample code project\nNote\nThis app requires Xcode 16 and visionOS 2 or later, and an Apple Vision Pro. Object tracking isn’t supported in the visionOS simulator.\nIn the project’s settings, select Signing and Capabilities.\nIn the project’s settings, select Signing and Capabilities.\nSelect your team name from the drop-down menu.\nSelect your team name from the drop-down menu.\nPair Xcode with your device wirelessly or using the developer strap.\nPair Xcode with your device wirelessly or using the developer strap.\nClick Run or press Command-R to launch the app.\nClick Run or press Command-R to launch the app.\nImport the reference object to track a specific object\nObject tracking demonstrates two methods for importing a reference object into the app. The first loads reference objects directly from the app’s bundle, as shown here:\nIn the second method, a person can provide a URL for a reference object file through a file importer dialog:\nRun object tracking on a session\nTo start receiving events, create anObjectTrackingProviderthat you initialize with a reference object, and then start anARKitSessionwith theObjectTrackingProvideryou created, as shown below:\nHandle adding, updating, removing, and visualizing objects\nARKit delivers an asynchronous stream of updates as it detects changes in the scene. Your app needs to process these as they arrive and update the scene in response. The example below demonstrates handling these events inside the app’sRealityView:\nWhen the app adds objects to the scene, it attaches virtual content to the reference object using theObjectAnchorVisualizationentity to render a wireframe that shows the reference object’s outline.\nCreate your own reference objects\nCreating your own reference objects requires an iPhone, iPad, or other device that you can use to create high-fidelity scans of the physical object you want to model, and a Mac with an M2 chip or later to process the images and create a reference object using Create ML.\nSee Also\nARKit",
    "https://developer.apple.com/documentation/visionos#RealityKit-and-Reality-Composer-Pro": "visionOS\nOverview\nvisionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.\n\nDeveloping for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app’s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS.\nExpand your app into immersive spaces\nStart with a familiar window-based experience to introduce people to your content. From there, addSwiftUIscene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences.\nBuild your app’s 3D content withRealityKitand Reality Composer Pro, and display it with aRealityView. In an immersive experience, useARKitto integrate your content with the person’s surroundings.\n\nExplore new kinds of interaction\nPeople can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures.SwiftUIprovides built-in support for these standard gestures, so rely on them for most of your app’s input. When you want to go beyond the standard gestures, useARKitto create custom gestures.\nTap to select\nPinch to rotate\nManipulate objects\nCreate custom gestures\nDive into featured sample apps\nExplore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash.\nTopics\nApp construction\nDesign\nSwiftUI\nRealityKit and Reality Composer Pro\nARKit\nVideo playback\nXcode and Simulator\nPerformance\niOS migration and compatibility\nEnterprise APIs for visionOS\nvisionOS\nOverview\nExpand your app into immersive spaces\nExplore new kinds of interaction\nDive into featured sample apps\nTopics",
    "https://developer.apple.com/documentation/visionos#app-top": "visionOS\nOverview\nvisionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.\n\nDeveloping for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app’s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS.\nExpand your app into immersive spaces\nStart with a familiar window-based experience to introduce people to your content. From there, addSwiftUIscene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences.\nBuild your app’s 3D content withRealityKitand Reality Composer Pro, and display it with aRealityView. In an immersive experience, useARKitto integrate your content with the person’s surroundings.\n\nExplore new kinds of interaction\nPeople can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures.SwiftUIprovides built-in support for these standard gestures, so rely on them for most of your app’s input. When you want to go beyond the standard gestures, useARKitto create custom gestures.\nTap to select\nPinch to rotate\nManipulate objects\nCreate custom gestures\nDive into featured sample apps\nExplore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash.\nTopics\nApp construction\nDesign\nSwiftUI\nRealityKit and Reality Composer Pro\nARKit\nVideo playback\nXcode and Simulator\nPerformance\niOS migration and compatibility\nEnterprise APIs for visionOS\nvisionOS\nOverview\nExpand your app into immersive spaces\nExplore new kinds of interaction\nDive into featured sample apps\nTopics",
    "https://developer.apple.com/documentation/visionos/placing-entities-using-head-and-device-transform": "visionOS\nPlacing entities using head and device transform\nPlacing entities using head and device transform\nPlacing entities using head and device transform\nPlacing entities using head and device transform\nOverview\nThis sample code project demonstrates how to create and display content that appears at a person’s head location, and follows a person’s view as they move their head in immersive spaces. It usesAnchorEntityandqueryDeviceAnchor(atTimestamp:)to get the transform of the person’s head and Apple Vision Pro to place content relative to them.\nThis sample creates the following two views and allows you to toggle between them:\nA hummingbird and a feeder directly in front of the person wearing the device.\nA hummingbird and a feeder directly in front of the person wearing the device.\nA hummingbird that flies to stay in the view of the person wearing the device.\nA hummingbird that flies to stay in the view of the person wearing the device.\nThe sample code project uses RealityKit and ARKit, respectively, to position the entities relative to the person. You can run the sample app in either Simulator or on-device.\nNote\nSeeDesign considerations for vision and motionandMotionin the Human Interface Guidelines for guidance on continuously head-tracked entities.\nShow entities at a person’s head position\nTo launch the hummingbird feeder at the position of the wearer’s head, the sample usesAnchorEntitywith the anchoring target ofAnchoringComponent.Target.head. This target provides the center of the wearer’s head, rather than the position of the device itself. You can only useAnchorEntityin an immersive space. Although it allows you to anchor content to the wearer’s head, you can’t access its transform because there’s no authorization required. If you attempt to access the transform, the property returns the identity transform instead.\nNote\nYou can get the transform of anAnchorEntitywith a differentAnchoringComponent.Target, such as a hand, by using aSpatialTrackingSessionand requesting authorization from the person using the app.\nThe sample creates anAnchorEntitythat anchors to the wearer’s head, and sets theAnchoringComponent.TrackingModetoonceto stop tracking after the initial anchor. The head-positioned entity root contains both the feeder entity and the hummingbird entity, which the sample loads from Reality Composer Pro. The app adds the root entity as a subentity of the head anchor to track it. The sample then offsets the feeder from the center of the wearer’s head by setting the position.\nMove entities relative to device transform\nThis sample contains a hummingbird that reacts to the wearer while they move around. It achieves this by creating aSystemand usingqueryDeviceAnchorto update the entities in the scene with each scene update.\nYou can only usequeryDeviceAnchorin an immersive space, but it doesn’t require authorization.\nNote\nqueryDeviceAnchorgives you the transform of the device, not the wearer’s head. If you want to get the visual transform of the center of the wearer’s head, useAnchorEntity(.head).\nThe sample starts by creating a RealityKit system, which allows you to update the entities with each scene update. SeeImplementing systems for entities in a scenefor information on creating a system class and using components to query entities. In the system, the app creates a query for entities with theFollowComponentWorldTrackingProviderand anARKitSessionas follows:\nThen, the sample starts the session by using theARKitSessionto run theWorldTrackingProvider.\nThe sample adds a customComponentnamedFollowComponentto the root entity of the hummingbird entity, and then uses it to query the entities in the scene to apply the movement to.\nImportant\nMake sure to register both the system and the component.\nThe following example shows how to query the device anchor and move the entity accordingly:\nThe sample keeps the hummingbird at the top right of the wearer’s field of vision by setting the hummingbird’s position relative to its root entity and offsetting it on theyandzaxes.\nSee Also\nRealityKit and Reality Composer Pro",
    "https://developer.apple.com/documentation/visionos/diorama": "visionOS\nDiorama\nDiorama\nDiorama\nDiorama\nOverview\nUse Reality Composer Pro to compose, edit, and preview RealityKit content for your visionOS app. In your Reality Composer Pro project, you can create one or more scenes, each of which contains a hierarchy of virtual objects called entities that your app can efficiently load and display.\nIn addition to helping you compose entity hierarchies, Reality Composer Pro also gives you the ability to add and configure components — even custom components that you’ve written — to the entities in your scenes.\nYou can also design the visual appearance of entities usingShader Graph, a node-based visual tool for creating RealityKit materials. Shader Graph gives you a tremendous amount of control over the surface details and shape of entities. You can even create animated materials and dynamic materials that change based on the state of your app or user input.\nDiorama demonstrates many of RealityKit and Reality Composer Pro’s features. It displays an interactive, virtual topographical trail map, much like the real-world dioramas you find at trailheads and ranger stations in national parks. This virtual map has points of interest you can tap to bring up more detailed information. You can also smoothly transition between two trail maps: Yosemite and Catalina Island.\nImport assets for building the scene\nYour Reality Composer Pro project must contain assets, which you use to compose scenes for your app. Diorama’s project has several assets, including 3D models like the diorama table, trail map, some birds and clouds that fly over the map, and a number of sounds and images. Reality Composer Pro provides a library of 3D models you can use. Access the library by clicking the Add (+) button on the right side of the toolbar. Selecting objects from the library imports them into your project.\n\nDiorama uses custom assets instead of the available library assets. To use custom assets in your own Reality Composer Pro scenes, import them into your project in one of three ways: by dragging them to Reality Composer Pro’s project browser, using File > Import from the File menu, or copying the assets into the.rkassetsbundle inside your project’s Swift package.\n\nNote\nAlthough you can still load USDZ files and other assets directly in visionOS, RealityKit compiles assets in your Reality Composer Pro project into a binary format that loads considerably faster than loading from individual files.\nCreate scenes containing the app’s entities\nA single Reality Composer Pro project can have multiple scenes. Asceneis an entity hierarchy stored in the project as a.usdafile that you can load and display in aRealityView. You can use Reality Composer’s scenes to build an entire RealityKit scene, or to store reusable entity hierarchies that you can use as building block for composing scenes at runtime — the approach Diorama uses. You can add as many different scenes to your project as you need by selecting File > New > Scene, or pressing ⌘N.\nAt the top of the Reality Composer Pro window, there’s a separate tab for every scene that’s currently open. To open a scene, double-click the scene’s.usdafile in the project browser. To edit a scene, select its tab, and make changes using the hierarchy viewer, the 3D view, and the inspector.\n\nAdd assets to your scenes\nRealityKit can only include entities in a scene, but it can’t use every type of asset that Reality Composer Pro supports as an entity. Reality Composer Pro automatically turns some assets, like 3D models, into an entity when you place them in a scene. It uses other assets indirectly. It uses image files, for example, primarily to define the surface details of model entities.\nDiorama uses multiple scenes to group assets together and then, at runtime, combines those scenes into a single immersive experience. For example, the diorama table has its own scene that includes the table, the map surface, and the trail lines. There are separate scenes for the birds that flock over the table, and for the clouds that float above it.\n\nTo add entities to a scene, drag assets from the project browser to the scene’s hierarchy view or 3D view. If the asset you drag is a type that can be represented as an entity, Reality Composer Pro adds it to your scene. You can select any asset in the scene hierarchy or the 3D view and change its location, rotation, and scale using the inspector on the right side of the window or the manipulator in the 3D view.\nAdd components to entities\nRealityKit follows a design pattern called Entity Component System (ECS). In an ECS app, you store additional data on an entity using components and can implement entity behavior by writing systems that use the data from those components. You can add and configure components to entities in Reality Composer Pro, including both shipped components likePhysicsBodyComponent, and custom components that you write and place in the Sources folder of your Reality Composer Pro Swift package. You can even create new components in Reality Composer Pro and then edit them in Xcode. For more information about ECS, seeUnderstanding the modular architecture of RealityKit.\nDiorama uses custom components to identify which transforms are points of interest, to mark the birds so the app can make sure they flock together, and to control the opacity of entities that are specific to just one of the two maps.\nTo add a component to an entity, select that entity in the hierarchy view or 3D view. At the bottom right of the inspector window, click on the Add Component button. A list of available components appears and the first item in that list is New Component. This item creates a new component class, and optionally a new system class, and adds the component to the selected entity.\nIf you look at the list of components, you see thePointOfInterestComponentthat Diorama uses to indicate which transforms are points of interest. If the selected entity doesn’t already contain aPointOfInterestComponent, selecting that adds it to the selected entity. Each entity can only have one component of a specific type. You can edit the values of the existing component in the inspector, which changes what shows up when you tap that point of interest in the app.\n\nUse transforms to mark locations\nIn Reality Composer Pro, atransformis an empty entity that marks a point in space. A transform contains a location, rotation, and scale, and its child entities inherit those. But, transforms have no visual representation and do nothing by themselves. Use transforms to mark locations in your scene or organize your entity hierarchy. For example, you might make several entities that need to move together into child entities of the same transform, so you can move them together by moving the parent transform.\nDiorama uses transforms with aPointOfInterestComponentto indicate points of interest on the map. When the app runs, those transforms mark the location of the floating placards with the name of the location. Tapping on a placard expands it to show more detailed information. To turn transforms into an interactive view, the app looks for a specific component on transforms called aPointOfInterestComponent. Because a transform contains no data other than location, orientation, and scale, it uses this component to hold the data the app needs to display on the placards. If you open theDioramaAssembledscene in Reality Composer Pro and click on the transform calledCathedral_Rocks, you see thePointOfInterestComponentin the inspector.\n\nLoad a scene at runtime\nTo load a Reality Composer Pro scene, useload(named:in:), passing the name of the scene you want to load and the project’s bundle. Reality Composer Pro Swift packages define a constant that provides ready access to its bundle. The constant is the name of the Reality Composer Pro project with “Bundle” appended to the end. In this case, the project is calledRealityKitContent, so the constant is calledRealityKitContentBundle. Here’s how Diorama loads the map table in theRealityViewinitializer:\nTheload(named:in:)function is asynchronous when called from an asynchronous context. Because the content closure of theRealityViewinitializer is asynchronous, it automatically uses theasyncversion to load the scene.  Note that when using it asynchronously, you must call it using theawaitkeyword.\nCreate the floating view\nDiorama adds aPointOfInterestComponentto a transform to display details about interesting places. Every point of interest’s name appears in a view that floats above its location on the map. When you tap the floating view, it expands to show detailed information, which the app pulls from thePointOfInterestComponent. The app shows these details by creating a SwiftUI view for each point of interest and querying for all entities that have aPointOfInterestComponentusing this query declared inImmersiveView.swift:\nIn theRealityViewinitializer, Diorama queries to retrieve the points of interest entities and passes them to a function calledcreateLearnMoreView(for:), which creates the view and saves it for display when it’s tapped.\nCreate attachments for points of interest\nDiorama displays the information added to aPointOfInterestComponentin aLearnMoreView, which it stores as an attachment.Attachmentsare SwiftUI views that are also RealityKit entities and that you can place into a RealityKit scene at a specific location. Diorama uses attachments to position the view that floats above each point of interest.\nThe app first checks to see if the entity has a component calledPointOfInterestRuntimeComponent. If it doesn’t, it creates a new one and adds it to the entity. This new component contains a value you only use at runtime that you don’t need to edit in Reality Composer Pro.\nBy putting this value into a separate component and adding it to entities at runtime, Reality Composer Pro never displays it in the inspector. ThePointOfInterestRuntimeComponentstores an identifier called anattachment tag, which uniquely identifies an attachment so the app can retrieve and display it at the appropriate time.\nNext, Diorama creates a SwiftUI view called aLearnMoreViewwith the information from thePointOfInterestComponent, tags that view, and stores the tag in thePointOfInterestRuntimeComponent. Finally, it stores the view in anAttachmentProvider, which is a custom class that maintains references to the attachment views so they don’t get deallocated when they’re not in a scene.\nDisplay point of interest attachments\nAssigning a view to an attachment provider doesn’t actually display that view in the scene. The initializer forRealityViewhas an optional view builder calledattachmentsthat’s used to specify the attachments.\nIn theupdateclosure of the initializer, which RealityKit calls when the contents of the view change, the app queries for entities with aPointOfInterestRuntimeComponent, uses the tag from that component to retrieve the correct attachment for it, and then adds that attachment and places it above its location on the map.\nCreate custom materials with Shader Graph\nTo switch between the two different topographical maps, Diorama shows a slider that morphs the map between the two locations. To accomplish this, and to draw elevation lines on the map, theFlatTerrainentity in theDioramaAssembledscene uses aShader Graph material. Shader Graph is a node-based material editor that’s built into Reality Composer Pro. Shader Graph gives you the ability to create dynamic materials that you can change at runtime. Prior to Reality Composer Pro, the only way to implement a dynamic material like this was to create aCustomMaterialand write Metal shaders to implement the necessary logic.\nDiorama’sDynamicTerrainMaterialEnhanceddoes two things. It draws contour lines on the map based on height data stored in displacement map images, and it also offsets the vertices of the flat disk based on the same data. By interpolating between two different height maps, the app achieves a smooth transition between the two different sets of height data.\nWhen you build Shader Graph materials, you can give them input parameters calledpromoted inputsthat you set from Swift code. This allows you to implement logic that previously required writing a Metal shader. The materials you build in the editor can affect both the look of an entity using the custom surface output node, which equates to writing Metal code in a fragment shader, or the position of vertices using the geometry modifier output, which equates to Metal code running in a vertex shader.\n\nNode graphs can containsubgraphs, which are similar to functions. They contain reusable sets of nodes with inputs and outputs. Subgraphs contain the logic to draw the contour lines and the logic to offset the vertices. Double-click a subgraph to edit it. For more information about building materials using Shader Graph, seeExplore Materials in Reality Composer Pro.\nUpdate the Shader Graph material at runtime\nTo change the map,DynamicTerrainMaterialEnhancedhas a promoted input calledProgress. If that parameter is set to1.0, it displays Catalina Island. If it’s set to0, it displays Yosemite. Any other number shows a state in transition between the two. When someone manipulates the slider, the app updates that input parameter based on the slider’s value.\nImportant\nShader Graph material parameters are case-sensitive. If the capitalization is wrong, your code won’t actually update the material.\nThe app sets the value of the input parameter in a function calledhandleMaterial()that the slider’s.onChangedclosure calls. That function retrieves theShaderGraphMaterialfrom the terrain entity and callssetParameter(name:value:)on it.\nSee Also",
    "https://developer.apple.com/documentation/visionos/making-your-app-compatible-with-visionos": "visionOS\nMaking your existing app compatible with visionOS\nMaking your existing app compatible with visionOS\nMaking your existing app compatible with visionOS\nMaking your existing app compatible with visionOS\nOverview\nA compatible iPadOS or iOS app links against the iOS SDK and runs in visionOS without needing to add a visionOS destination. Although visionOS provides a complete set of iOS frameworks for linking, some features of those frameworks might be unavailable due to hardware or usage differences. ReadDetermining whether to bring your app to visionOSfor details on behavior changes, API availability checks, and deprecated frameworks. To ensure your app runs correctly in visionOS, handle any unavailable features gracefully and provide workarounds wherever possible.\nRun as a compatible app in visionOS\nThe App Store makes compatible iPad and iPhone apps available in visionOS automatically after you sign the updated Apple Developer Program License Agreement. If you have an app in the iOS App Store, try downloading it on Apple Vision Pro and run it. If you built your app using the iOS SDK, Xcode 15 and later automatically adds a Designed for iPad runtime destination to your project.\n\nUse this destination to run your app and test its compatibility in visionOS. Depending on your app, you might need to make additional changes to account for features that are only found in the iOS SDK. You can test most of your app’s core functionality in Simulator, but some features are available only on a device.\nHandle SDK differences gracefully\nvisionOS contains most of the same technologies as iPadOS and iOS, but there are differences. In some cases, a feature you use in your app might not be available because of hardware differences or because of differences in how people use a visionOS device. As part of your testing, consider the impact of any unavailable features on your app’s overall experience. Whenever possible, work around unavailable features by disabling them or providing alternate ways to access the same content.\nThe following features aren’t available in compatible iPad and iPhone apps in visionOS. Use framework APIs listed inDetermining whether to bring your app to visionOSto determine when the features are available.\nCore Motion services\nCore Motion services\nBarometer and magnetometer data\nBarometer and magnetometer data\nAll location services except the standard service\nAll location services except the standard service\nHealthKit data\nHealthKit data\nVideo or still-photo capture\nVideo or still-photo capture\nCamera features like Auto Focus or flash\nCamera features like Auto Focus or flash\nRear-facing (selfie) cameras\nRear-facing (selfie) cameras\nNote\nAlthough device cameras are unavailable on Apple Vision Pro, compatible apps can capture photos of personas usingAVCaptureDevicewith the position of.frontor use continuity camera to capture photo and video from iPhone and iPad.\nIf your app uses an unsupported feature but can function without it, you can still bring your app to visionOS. Remove features that aren’t available and focus on bringing the rest of your content to the platform. For example, if you have an app that lets people write down notes and take pictures to include with those notes, disable the picture-taking ability in visionOS but let people add text and incorporate images from their library.\nIf your app relies on frameworks that behave differently in visionOS, update your code to handle those differences. Throughout your code, make sure you respond to unusual situations:\nUse availability checks.Availability checks give you a clear indication when you can’t use a feature, but some frameworks might have more subtle behavior. ReadDetermining whether to bring your app to visionOSfor a list of frameworks and features with availability checks.\nUse availability checks.Availability checks give you a clear indication when you can’t use a feature, but some frameworks might have more subtle behavior. ReadDetermining whether to bring your app to visionOSfor a list of frameworks and features with availability checks.\nRemove deprecated code.If your app currently uses deprecated APIs or frameworks, update your code to use appropriate replacements. ReadDetermining whether to bring your app to visionOSfor a list of deprecated frameworks.\nRemove deprecated code.If your app currently uses deprecated APIs or frameworks, update your code to use appropriate replacements. ReadDetermining whether to bring your app to visionOSfor a list of deprecated frameworks.\nHandle error conditions.If a function throws an exception or returns an error, handle the error. Use error information to adjust your app’s behavior or provide an explanation of why it can’t perform certain operations.\nHandle error conditions.If a function throws an exception or returns an error, handle the error. Use error information to adjust your app’s behavior or provide an explanation of why it can’t perform certain operations.\nHandlenilor empty values gracefully.Validate objects and return values before you try to use them.\nHandlenilor empty values gracefully.Validate objects and return values before you try to use them.\nUpdate your user interface.Provide appropriate messaging in your interface when a feature is unavailable, or remove feature-specific views entirely if you can do so cleanly. Don’t leave empty views where the feature was.\nUpdate your user interface.Provide appropriate messaging in your interface when a feature is unavailable, or remove feature-specific views entirely if you can do so cleanly. Don’t leave empty views where the feature was.\nAdapt to device differences\nApple frameworks take a device-agnostic approach whenever possible to minimize issues when you use them on different device types. Apple devices come in a variety of shapes and sizes, with different sets of features. Rather than build your app for a specific device, make sure it adapts to any device and can gracefully handle differences.\nBuild robustness into your app during the design process. Avoid assumptions that might cause your app to break when it runs on a new device, and make sure your app adapts dynamically to different conditions. For example:\nDon’t assume the device type or idiom is always iPhone, iPad, or iPod Touch.Avoid decisions based on the current idiom. If you do rely on the current idiom, provide reasonable defaults for unknown idioms.\nDon’t assume the device type or idiom is always iPhone, iPad, or iPod Touch.Avoid decisions based on the current idiom. If you do rely on the current idiom, provide reasonable defaults for unknown idioms.\nDesign your app to handle unavailable hardware or features.Specific hardware and features might be unavailable for many different reasons. For example, a feature might be unavailable when your app runs in Simulator. Perform availability checks whenever possible, and handle unavailable features gracefully.\nDesign your app to handle unavailable hardware or features.Specific hardware and features might be unavailable for many different reasons. For example, a feature might be unavailable when your app runs in Simulator. Perform availability checks whenever possible, and handle unavailable features gracefully.\nDesign your windows and views to adapt dynamically.Build your interface to adapt dynamically to any size using SwiftUI or Auto Layout. Assume the size of your app can change dynamically.\nDesign your windows and views to adapt dynamically.Build your interface to adapt dynamically to any size using SwiftUI or Auto Layout. Assume the size of your app can change dynamically.\nDon’t assume the device has a specific number of displays.People can connect iPad and iPhone to an external display, and visionOS devices use two displays to create a stereoscopic version of your app’s content.\nDon’t assume the device has a specific number of displays.People can connect iPad and iPhone to an external display, and visionOS devices use two displays to create a stereoscopic version of your app’s content.\nDon’t make assumptions based on the available frameworks or symbols.The presence or absence of frameworks or code symbols is an unreliable way to identify a device type and can change in later software updates. SeeDetermining whether to bring your app to visionOSfor information on using availability checks to identify which features are available for a given framework.\nDon’t make assumptions based on the available frameworks or symbols.The presence or absence of frameworks or code symbols is an unreliable way to identify a device type and can change in later software updates. SeeDetermining whether to bring your app to visionOSfor information on using availability checks to identify which features are available for a given framework.\nDon’t assume your app runs in the background.visionOS doesn’t support location, external accessory, or Bluetooth-peripheral background execution modes.\nDon’t assume your app runs in the background.visionOS doesn’t support location, external accessory, or Bluetooth-peripheral background execution modes.\nDon’t assume that background apps are hidden.In visionOS, the windows of background apps remain visible, but are dimmed when no one looks at them. The only time app windows disappear is when one app presents an immersive space.\nDon’t assume that background apps are hidden.In visionOS, the windows of background apps remain visible, but are dimmed when no one looks at them. The only time app windows disappear is when one app presents an immersive space.\nAudit your interface code\nTo minimize disruptions, visionOS runs your compatible iPad or iPhone app in an environment that matches an iPad as much as possible. Windows and views retain the same appearance that they have in iPadOS or iOS, and the system sizes your app’s window to fit an iPad whenever possible.\nWhen building your app’s interface, make choices that ensure your app runs well in visionOS too. Adopt the following best practices for your interface-related code:\nSupport iPad and iPhone in the same app.Create one app that supports both device types, rather than separate apps for each device. SwiftUI and UIKit support adaptable interfaces, and Xcode provides tools to help you visualize your interface at different supported sizes.\nSupport iPad and iPhone in the same app.Create one app that supports both device types, rather than separate apps for each device. SwiftUI and UIKit support adaptable interfaces, and Xcode provides tools to help you visualize your interface at different supported sizes.\nOrganize your interface using scenes.Scenes are a fundamental tool for managing your app’s interface. Use the scene types in SwiftUI and UIKit to assemble and manage the views you display in windows.\nOrganize your interface using scenes.Scenes are a fundamental tool for managing your app’s interface. Use the scene types in SwiftUI and UIKit to assemble and manage the views you display in windows.\nAdapt your interface to any size.Design your interface to adapt naturally to different sizes. For an introduction to SwiftUI views and layout, seeDeclaring a custom view. For information about laying out views in UIKit, seeView layout.\nAdapt your interface to any size.Design your interface to adapt naturally to different sizes. For an introduction to SwiftUI views and layout, seeDeclaring a custom view. For information about laying out views in UIKit, seeView layout.\nDon’t access screen details.visionOS provides reasonable values forUIScreenobjects, but don’t use those values to make decisions. Instead, create an adaptive layout or useAuto Layoutto make your app look good on all platforms.\nDon’t access screen details.visionOS provides reasonable values forUIScreenobjects, but don’t use those values to make decisions. Instead, create an adaptive layout or useAuto Layoutto make your app look good on all platforms.\nDon’t rely on the status bar for layouts.statusBarFramereturnsCGRectZeroon visionOS. Usesafe areasto calculate layout instead.\nDon’t rely on the status bar for layouts.statusBarFramereturnsCGRectZeroon visionOS. Usesafe areasto calculate layout instead.\nSpecify the supported interface orientations.Add theUISupportedInterfaceOrientationskey to your app’sInfo.plistfile to specify the interface orientations it supports. Support all interface orientations whenever possible. visionOS adds an interface rotation for your app button only when this key is present. To display your app’s interface in a particular orientation at launch, add theUIPreferredDefaultInterfaceOrientationkey to your app’sInfo.plistfile. Set the value of the key to one of the values in your app’sUISupportedInterfaceOrientationskey. Add~ipador~iphoneto the key name, for example,UIInterfaceOrientationPortrait~ipad, to specify device-specific orientation preferences.\nSpecify the supported interface orientations.Add theUISupportedInterfaceOrientationskey to your app’sInfo.plistfile to specify the interface orientations it supports. Support all interface orientations whenever possible. visionOS adds an interface rotation for your app button only when this key is present. To display your app’s interface in a particular orientation at launch, add theUIPreferredDefaultInterfaceOrientationkey to your app’sInfo.plistfile. Set the value of the key to one of the values in your app’sUISupportedInterfaceOrientationskey. Add~ipador~iphoneto the key name, for example,UIInterfaceOrientationPortrait~ipad, to specify device-specific orientation preferences.\nAdopt vector-based images when possible.Vector-based images scale well to different sizes while retaining a crisp appearance. If you use bitmap-based assets, make them the exact size you need. Don’t use oversized assets, which require extra work to display at the correct size.\nAdopt vector-based images when possible.Vector-based images scale well to different sizes while retaining a crisp appearance. If you use bitmap-based assets, make them the exact size you need. Don’t use oversized assets, which require extra work to display at the correct size.\nUpdate hover effects in custom views.Hover effects convey the focused view or control in your interface. Standard system views apply hover effects as needed. For custom views and controls, verify that the hover effects look appropriate in visionOS. Add or update the content shape for your hover effects if needed. The following example uses SwiftUI to add a rectangle-shaped hover effect to a rectangle in a view:\nUpdate hover effects in custom views.Hover effects convey the focused view or control in your interface. Standard system views apply hover effects as needed. For custom views and controls, verify that the hover effects look appropriate in visionOS. Add or update the content shape for your hover effects if needed. The following example uses SwiftUI to add a rectangle-shaped hover effect to a rectangle in a view:\nCreate adaptive layouts in UIKit\nIf your UIKit app uses hardcoded values or relies onUIScreenfor layout, the first step to migrating your app to visionOS is to use an adaptable layout. When you make decisions using device details, your app might produce inconsistent or erroneous results on an unknown device type, or it might fail altogether. Find solutions that rely on environmental information, rather than the device type. For example, SwiftUI and UIKit start layout using the app’s window size, which isn’t necessarily the same size as the device’s display.\nNote\nDevice-specific information is available when you absolutely need it, but validate the information you receive and provide reasonable default behavior for unexpected values.\nThink about ways to create adaptive layouts using the following techniques:\nUse stack views.UIStackViewobjects adjust the position of their contained views automatically when interface dimensions change. Alternatively,Auto Layoutconstraints let you specify the rules that determine the size and position of the views in your interface.\nUse stack views.UIStackViewobjects adjust the position of their contained views automatically when interface dimensions change. Alternatively,Auto Layoutconstraints let you specify the rules that determine the size and position of the views in your interface.\nStay within layout margins.ReadPositioning content within layout marginsto set up constraints that respect layout margins and don’t crowd other content.\nStay within layout margins.ReadPositioning content within layout marginsto set up constraints that respect layout margins and don’t crowd other content.\nRespect the safe area.Place views so they’re not obstructed by other content. Each view has alayout guidethat helps you create constraints to position your views within the safe area, which adapt to the current device automatically. ReadPositioning content relative to the safe areafor guidance.\nRespect the safe area.Place views so they’re not obstructed by other content. Each view has alayout guidethat helps you create constraints to position your views within the safe area, which adapt to the current device automatically. ReadPositioning content relative to the safe areafor guidance.\nAdapt based on changes in UITraitCollection.Write code to adjust your app’s layout according to changes in elements of the iOS user interface, such as size class, display scale, and layout direction. ReadUITraitCollectionfor more information.\nAdapt based on changes in UITraitCollection.Write code to adjust your app’s layout according to changes in elements of the iOS user interface, such as size class, display scale, and layout direction. ReadUITraitCollectionfor more information.\nTest specific scenarios before uploading your app\nThe following App Store features for iOS continue to work when your app runs in visionOS:\nIn-app purchases and subscriptions\nIn-app purchases and subscriptions\nApp capabilities and entitlements\nApp capabilities and entitlements\nOn-demand resources\nOn-demand resources\nApp thinning\nApp thinning\nWhen you use app thinning to optimize your app for different devices and operating systems, the App Store selects the resources and content that offer the best fit for visionOS devices. It then removes any other resources to create a streamlined installation of your app. When you export your app from Xcode 15 or later, you can test the thinning support using the visionOS virtual thinning target.\nSee Also\niOS migration and compatibility",
    "https://developer.apple.com/documentation/visionos/determining-whether-to-bring-your-app-to-visionos": "visionOS\nDetermining whether to bring your app to visionOS\nDetermining whether to bring your app to visionOS\nDetermining whether to bring your app to visionOS\nDetermining whether to bring your app to visionOS\nOverview\nIf you have an existing app that runs in iPadOS or iOS and you want to bring it to visionOS, you can choose to either run your compatible app as-is or create a visionOS-specific version.\nvisionOS supports most of the same technologies as iOS, so many apps built to run on iPad or iPhone can run unmodified on Apple Vision Pro. When a compatible app runs in visionOS, it retains the same appearance it has in iPadOS or iOS, and its content appears in a window in the person’s surroundings. Apps built specifically for visionOS adopt the standard system appearance, look more natural on the platform, and support 3D content and immersive experiences.\nThe visionOS SDK has many differences from the iOS SDK. Frameworks and features might behave differently or be unavailable when your app runs in visionOS. Use this article to ensure your app behaves as you expect on every platform.\nDecide whether your app suits the platform\nIn some cases, you may decide your app is not well-suited to visionOS due to platform differences. For example, consider the following types of apps:\nApps that act as containers for app extensions.This includes apps where the primary purpose is to deliver custom keyboard extensions, device drivers, sticker packs, SMS and MMS message-filtering extensions, call directory extensions, or widgets.\nApps that act as containers for app extensions.This includes apps where the primary purpose is to deliver custom keyboard extensions, device drivers, sticker packs, SMS and MMS message-filtering extensions, call directory extensions, or widgets.\nNavigation-based apps.This includes apps that follow a person’s location changes, such as apps that offer turn-by-turn directions or navigation.\nNavigation-based apps.This includes apps that follow a person’s location changes, such as apps that offer turn-by-turn directions or navigation.\nSelfie or camera-based apps.This includes apps where the primary purpose is to capture images or video from the device’s cameras. The device cameras are unavailable, but continuity camera and persona capture are available on Apple Vision Pro.\nSelfie or camera-based apps.This includes apps where the primary purpose is to capture images or video from the device’s cameras. The device cameras are unavailable, but continuity camera and persona capture are available on Apple Vision Pro.\nOpt out of running on visionOS\nIf you don’t want your app to run on Apple Vision Pro, change your app’s availability in App Store Connect:\nSelect your app in App Store Connect.\nSelect your app in App Store Connect.\nNavigate to the Pricing and Availability information.\nNavigate to the Pricing and Availability information.\nDisable the “Make this app available on Apple Vision Pro” option.\nDisable the “Make this app available on Apple Vision Pro” option.\nWhen you remove your app’s availability for Apple Vision Pro, the App Store stops making your iOS app available for visionOS. People who already downloaded your app on their Vision Pro can still run it in visionOS, but they can’t download it again and won’t receive any updates. This setting doesn’t affect the version of your app built natively using the visionOS SDK.\nFor information on managing your app’s availability for Apple Vision Pro, seeManage availability of iPhone and iPad apps on Apple Vision Pro.\nRun as a compatible app on visionOS\nvisionOS runs compatible iPad and iPhone apps by linking against the iOS SDK to provide continuous access to existing content right away. visionOS supports most of the same technologies as iOS, so many apps built to run on iPad or iPhone can run unmodified on Apple Vision Pro. ReadMaking your existing app compatible with visionOSto learn how to make your app run successfully as a compatible app in visionOS.\nCreate a visionOS version of your app\nApps built specifically for visionOS adopt the standard system appearance, and they look more natural on the platform. Creating a version of your app specifically for visionOS gives you the opportunity to add elements that work well on the platform, such as 3D content and immersive experiences. ReadBringing your existing apps to visionOSfor more information on how to modify your app to run successfully as a visionOS app and how to update your interface to take advantage of visionOS-specific elements.\nCheck framework availability in visionOS\nThe following frameworks are deprecated in their entirety in both iOS and visionOS. If your app still uses these frameworks, stop using them immediately. The reference documentation for each framework includes information about how to update your code.\nAccounts\nAccounts\nAddress Book\nAddress Book\nAddress Book UI\nAddress Book UI\nAssets Library\nAssets Library\nGLKit\nGLKit\niAd\niAd\nNewsstand Kit\nNewsstand Kit\nNotification Center\nNotification Center\nImportant\nvisionOS removed many deprecated symbols entirely, turning these deprecation warnings into missing-symbol errors on the platform.\nHere’s a list of frameworks and features that behave differently in visionOS:\nActivityKit.Available on iOS only. Check theareActivitiesEnabledproperty ofActivityAuthorizationInfoto determine if Live Activities are authorized.\nActivityKit.Available on iOS only. Check theareActivitiesEnabledproperty ofActivityAuthorizationInfoto determine if Live Activities are authorized.\nAirPlay.visionOS hides AirPlay sharing buttons in system interfaces, and you can’t use AirPlay features from compatible apps.\nAirPlay.visionOS hides AirPlay sharing buttons in system interfaces, and you can’t use AirPlay features from compatible apps.\nApp extensions.visionOS doesn’t load App Clips, device drivers, device activity monitors, keyboard extensions, Messages app extensions, photo-editing app extensions, SMS and call-reporting extensions, or widgets.\nApp extensions.visionOS doesn’t load App Clips, device drivers, device activity monitors, keyboard extensions, Messages app extensions, photo-editing app extensions, SMS and call-reporting extensions, or widgets.\nApple Watch features.visionOS ignores watchOS apps and WatchKit extensions in your iOS or iPadOS app. Face sharing inClockKitdoes nothing in visionOS.\nApple Watch features.visionOS ignores watchOS apps and WatchKit extensions in your iOS or iPadOS app. Face sharing inClockKitdoes nothing in visionOS.\nARKit.This framework requires you to use different APIs for iOS and visionOS, as visionOS can’t display windows that contain ARKit views. Check theisSupportedproperty of your configuration object to determine availability of augmented reality features. In visionOS, ARKit views such asARVieware never available, so isolate interface code containing those views to the iOS version of your app. For information about how to bring an ARKit app to visionOS, seeBringing your ARKit app to visionOS.\nARKit.This framework requires you to use different APIs for iOS and visionOS, as visionOS can’t display windows that contain ARKit views. Check theisSupportedproperty of your configuration object to determine availability of augmented reality features. In visionOS, ARKit views such asARVieware never available, so isolate interface code containing those views to the iOS version of your app. For information about how to bring an ARKit app to visionOS, seeBringing your ARKit app to visionOS.\nAudio and video.visionOS doesn’t support Picture in Picture or AV routing features. Check the availability of video features before using them. Be prepared for audio playback to stop automatically when your app moves to the background.\nAudio and video.visionOS doesn’t support Picture in Picture or AV routing features. Check the availability of video features before using them. Be prepared for audio playback to stop automatically when your app moves to the background.\nAutomatic Assessment Configuration.Check for error values when you configure anAEAssessmentSessionobject. The framework returns an error if you try to start a test in visionOS.\nAutomatic Assessment Configuration.Check for error values when you configure anAEAssessmentSessionobject. The framework returns an error if you try to start a test in visionOS.\nAVFoundation.Identify what cameras are available using theAVCaptureDevice.DiscoverySessionclass. Don’t assume the presence of specific cameras. Capture interfaces aren’t available in visionOS. Use availability checks to determine which services are present.\nAVFoundation.Identify what cameras are available using theAVCaptureDevice.DiscoverySessionclass. Don’t assume the presence of specific cameras. Capture interfaces aren’t available in visionOS. Use availability checks to determine which services are present.\nCellular telephony and CallKit.You can still implement Voice-over-IP (VoIP) services usingCallKitandCore Telephony. Phone number verification, call-blocking, and other cellular-related services are unavailable.\nCellular telephony and CallKit.You can still implement Voice-over-IP (VoIP) services usingCallKitandCore Telephony. Phone number verification, call-blocking, and other cellular-related services are unavailable.\nContacts.Use theCNContactStoreclass to determine your app’s authorization status.\nContacts.Use theCNContactStoreclass to determine your app’s authorization status.\nCore Bluetooth.Use theCBCentralManagerandCBPeripheralManagerclasses to determine feature availability and your app’s authorization status.\nCore Bluetooth.Use theCBCentralManagerandCBPeripheralManagerclasses to determine feature availability and your app’s authorization status.\nCore Haptics.visionOS plays audio feedback instead of haptic feedback. Call thecapabilitiesForHardware()method of the haptic engine to determine the available features.\nCore Haptics.visionOS plays audio feedback instead of haptic feedback. Call thecapabilitiesForHardware()method of the haptic engine to determine the available features.\nCore Location.You can request location using the standard location service, but most other services are unavailable. Use availability checks to determine which services are present. The Always authorization level is unavailable and automatically becomes When in Use authorization. Check the properties ofCLLocationManagerto determine the availability of location services.\nCore Location.You can request location using the standard location service, but most other services are unavailable. Use availability checks to determine which services are present. The Always authorization level is unavailable and automatically becomes When in Use authorization. Check the properties ofCLLocationManagerto determine the availability of location services.\nCore Motion.Barometer data is unavailable, but most other sensors are available. Use availability checks to determine which sensors you can use. Check the properties ofCMMotionManagerto determine the availability of accelerometers, gyroscopes, magnetometers, and other hardware sensors.\nCore Motion.Barometer data is unavailable, but most other sensors are available. Use availability checks to determine which sensors you can use. Check the properties ofCMMotionManagerto determine the availability of accelerometers, gyroscopes, magnetometers, and other hardware sensors.\nCore NFC.Check thereadingAvailableproperty of your reader session to determine if NFC tag reading is available.\nCore NFC.Check thereadingAvailableproperty of your reader session to determine if NFC tag reading is available.\nDevice management.Calls to theManagedSettingsandManagedSettingsUIframeworks do nothing in visionOS.\nDevice management.Calls to theManagedSettingsandManagedSettingsUIframeworks do nothing in visionOS.\nEventKit.Use theEKEventStoreclass to determine your app’s authorization status.\nEventKit.Use theEKEventStoreclass to determine your app’s authorization status.\nExposure Notification.Use theENManagerclass to determine your app’s authorization status.\nExposure Notification.Use theENManagerclass to determine your app’s authorization status.\nGame controllers.visionOS delivers game controller events only when someone is looking at your app. To require a game controller as an input device, add theGCRequiresControllerUserInteractionkey with the visionOS value to your app’sInfo.plist.\nGame controllers.visionOS delivers game controller events only when someone is looking at your app. To require a game controller as an input device, add theGCRequiresControllerUserInteractionkey with the visionOS value to your app’sInfo.plist.\nHandoff.visionOS doesn’t attempt to hand off user activities to other devices.\nHandoff.visionOS doesn’t attempt to hand off user activities to other devices.\nHomeKit.You can’t add accessories using a QR code from Apple Vision Pro. Check the properties ofHMHomeManagerto determine your app’s authorization status.\nHomeKit.You can’t add accessories using a QR code from Apple Vision Pro. Check the properties ofHMHomeManagerto determine your app’s authorization status.\nLocal Authentication.Use theLAContextclass to determine the authentication policies you can use.\nLocal Authentication.Use theLAContextclass to determine the authentication policies you can use.\nMapKit.User-tracking features that involve heading information aren’t available in visionOS.\nMapKit.User-tracking features that involve heading information aren’t available in visionOS.\nMedia Player.Some APIs are unavailable in visionOS. Use theMPMediaLibraryclass to determine your app’s authorization status.\nMedia Player.Some APIs are unavailable in visionOS. Use theMPMediaLibraryclass to determine your app’s authorization status.\nMetrics.You can useMetricKitto gather on-device diagnostic logs and generate reports, but you can’t gather metrics in visionOS.\nMetrics.You can useMetricKitto gather on-device diagnostic logs and generate reports, but you can’t gather metrics in visionOS.\nMulti-Touch.The system reports a maximum of two simultaneous touch inputs — one for each of the person’s hands. All system gesture recognizers handle these inputs appropriately, including for zoom and rotation gestures that require multiple fingers. If you have custom gesture recognizers that require more than two points of interaction, update them to support only one or two touches in visionOS.\nMulti-Touch.The system reports a maximum of two simultaneous touch inputs — one for each of the person’s hands. All system gesture recognizers handle these inputs appropriately, including for zoom and rotation gestures that require multiple fingers. If you have custom gesture recognizers that require more than two points of interaction, update them to support only one or two touches in visionOS.\nMusicKit.Some APIs are unavailable in visionOS.\nMusicKit.Some APIs are unavailable in visionOS.\nNearby Interaction.The framework does nothing in visionOS. Check thedeviceCapabilitiesproperty of your session to determine whether features are available.\nNearby Interaction.The framework does nothing in visionOS. Check thedeviceCapabilitiesproperty of your session to determine whether features are available.\nParental controls.Calls to theFamilyControlsframework do nothing in visionOS.\nParental controls.Calls to theFamilyControlsframework do nothing in visionOS.\nPencilKit.visionOS doesn’t report touches of typeUITouch.TouchType.pencil, but it does report other types of touches.\nPencilKit.visionOS doesn’t report touches of typeUITouch.TouchType.pencil, but it does report other types of touches.\nPhotoKit.Use thePHPhotoLibraryclass to determine your app’s authorization status.\nPhotoKit.Use thePHPhotoLibraryclass to determine your app’s authorization status.\nProximityReader.Check theisSupportedproperty of the card reader object to determine if Tap to Pay on iPhone is available.\nProximityReader.Check theisSupportedproperty of the card reader object to determine if Tap to Pay on iPhone is available.\nPush to Talk.These services are unavailable in visionOS. Check for errors when creating aPTChannelManager.\nPush to Talk.These services are unavailable in visionOS. Check for errors when creating aPTChannelManager.\nReplayKit.Check theisAvailableproperty ofRPScreenRecorderto determine if screen recording support is available.\nReplayKit.Check theisAvailableproperty ofRPScreenRecorderto determine if screen recording support is available.\nRoomPlan.Check theisSupportedproperty of theRoomCaptureSessionobject to determine if LiDAR scanning is available on the device.\nRoomPlan.Check theisSupportedproperty of theRoomCaptureSessionobject to determine if LiDAR scanning is available on the device.\nSafari Services.A link that presents aSFSafariViewControllernow opens a new scene in the Safari app.\nSafari Services.A link that presents aSFSafariViewControllernow opens a new scene in the Safari app.\nScreen Time.Calls to theScreen Timeframework do nothing in visionOS.\nScreen Time.Calls to theScreen Timeframework do nothing in visionOS.\nSensor-related features.TheSensorKitframework is unavailable in the native visionOS SDK and calls to it do nothing when running a compatible app. Use theSRSensorReaderclass to determine your app’s authorization status for compatible apps.\nSensor-related features.TheSensorKitframework is unavailable in the native visionOS SDK and calls to it do nothing when running a compatible app. Use theSRSensorReaderclass to determine your app’s authorization status for compatible apps.\nSocial media.Calls to theSocialframework do nothing in visionOS.\nSocial media.Calls to theSocialframework do nothing in visionOS.\nSpeech.Use theSFSpeechRecognizerclass to determine if speech recognition is available.\nSpeech.Use theSFSpeechRecognizerclass to determine if speech recognition is available.\nSystem interfaces.Authorization prompts, Sign in with Apple prompts, and other system-provided interfaces run asynchronously outside of your app’s process. Because these interfaces don’t run modally in your app, your app might not receive immediate responses.\nSystem interfaces.Authorization prompts, Sign in with Apple prompts, and other system-provided interfaces run asynchronously outside of your app’s process. Because these interfaces don’t run modally in your app, your app might not receive immediate responses.\nUser Notifications.Use thegetNotificationSettings(completionHandler:)method ofUNUserNotificationCenterto determine your app’s authorization status.\nUser Notifications.Use thegetNotificationSettings(completionHandler:)method ofUNUserNotificationCenterto determine your app’s authorization status.\nVehicle features.The system doesn’t call your app’sCarPlaycode. Calls you make usingCarKeydo nothing in visionOS.\nVehicle features.The system doesn’t call your app’sCarPlaycode. Calls you make usingCarKeydo nothing in visionOS.\nVisionKit.TheDataScannerViewControllerAPIs are unavailable, but other features are still available. Data scanners do nothing inVisionKitin visionOS.\nVisionKit.TheDataScannerViewControllerAPIs are unavailable, but other features are still available. Data scanners do nothing inVisionKitin visionOS.\nWatch Connectivity.The framework supports connections only between an iPhone and Apple Watch. Call theisSupported()method of theWCSessionobject to determine if the framework is available.\nWatch Connectivity.The framework supports connections only between an iPhone and Apple Watch. Call theisSupported()method of theWCSessionobject to determine if the framework is available.\nNote\nReviewBuilding spatial experiences for business apps with enterprise APIs for visionOSfor information on enterprise APIs and how to request the entitlements.\nSee Also\niOS migration and compatibility",
    "https://developer.apple.com/documentation/visionos/presenting-windows-and-spaces": "visionOS\nPresenting windows and spaces\nPresenting windows and spaces\nPresenting windows and spaces\nPresenting windows and spaces\nOverview\nAn app’s scenes, which contain views that people interact with, can take different forms. For example, a scene can fill a window, a tab in a window, or an entire screen. Some scenes can even place views throughout a person’s surroundings. How a scene appears depends on its type, the platform, and the context.\nWhen someone launches your app, SwiftUI looks for the firstWindowGroup,Window, orDocumentGroupin your app declaration and opens a scene of that type, typically filling a new window or the entire screen, depending on the platform. For example, the following app running in macOS presents a window that contains aMailViewerview:\nIn visionOS, you can alternatively configure your app to open the firstImmersiveSpacethat the app declares. In any case, specific platforms and configurations enable you to open more than one scene at a time. Under those conditions, you can use actions that appear in the environment to programmatically open and close the scenes in your app.\nCheck for multiple-scene support\nIf you share code among different platforms and need to find out at runtime whether the current system supports displaying multiple scenes, read thesupportsMultipleWindowsenvironment value. The following code creates a button that’s hidden unless the app supports multiple windows:\nThe value that you read depends on both the platform and how you configure your app:\nIn macOS, this property returnstruefor any app that uses the SwiftUI app lifecycle.\nIn macOS, this property returnstruefor any app that uses the SwiftUI app lifecycle.\nIn iPadOS and visionOS, this property returnstruefor any app that uses the SwiftUI app lifecycle and has the Information Property List keyUIApplicationSupportsMultipleScenesset totrue, andfalseotherwise.\nIn iPadOS and visionOS, this property returnstruefor any app that uses the SwiftUI app lifecycle and has the Information Property List keyUIApplicationSupportsMultipleScenesset totrue, andfalseotherwise.\nFor all other platforms and configurations, the value returnsfalse.\nFor all other platforms and configurations, the value returnsfalse.\nIf your app only ever runs in one of these situations, you can assume the associated behavior and don’t need to check the value.\nEnable multiple simultaneous scenes\nYou can always present multiple scenes in macOS. To enable an iPadOS or visionOS app to simultaneously display multiple scenes — includingImmersiveSpacescenes in visionOS — add theUIApplicationSupportsMultipleSceneskey with a value oftruein theUIApplicationSceneManifestdictionary of your app’s Information Property List. Use the Info tab in Xcode for your app’s target to add this key:\n\nApps on other platforms can display only one scene during their lifetime.\nOpen windows programmatically\nSome platforms provide built-in controls that enable people to open instances of the window-style scenes that your app defines. For example, in macOS people can choose File > New Window from the menu bar to open a new window. SwiftUI also provides ways for you to open new windows programmatically.\nTo do this, get theopenWindowaction from the environment and call it with an identifier, a value, or both to indicate what kind of window to open and optionally what data to open it with. The following view opens a new instance of the previously defined mail viewer window when someone clicks or taps the button:\nWhen the action runs on a system that supports multiple scenes, SwiftUI looks for a window in the app declaration that has a matching identifier and creates a new scene of that type.\nImportant\nIfsupportsMultipleWindowsisfalseand you try to open a new window, SwiftUI ignores the action and logs a runtime error.\nIn addition to opening more instances of an app’s main window, as in the above example, you can also open other window types that your app’s body declares. For example, you can open an instance of theWindowthat displays connectivity information:\nOpen a space programmatically\nIn visionOS, you open an immersive space — a scene that you can use to present unbounded content in a person’s surroundings — in much the same way that you open a window, except that you use theopenImmersiveSpaceaction. The action runs asynchronously, so you use theawaitkeyword when you call it, and typically do so from inside aTask:\nBecause your app operates in a Full Space when you open anImmersiveSpacescene, you can only open one scene of this type at a time. If you try to open a space when one is already open, the system logs a runtime error.\nYour app can display any number of windows together with an immersive space. However, when you open a space from your app, the system hides all windows that belong to other apps. After you dismiss your space, the other apps’ windows reappear. Similarly, the system hides your app’s windows if another app opens an immersive space.\nDesignate a space as your app’s main interface\nWhen visionOS launches an app, it opens the first window group, window, or document scene that the app’s body declares, just like on other platforms. This is true even if you first declare a space. However, if you want to open your app into an immersive space directly, specify a space as the default scene for your app by adding theUIApplicationPreferredDefaultSceneSessionRolekey to your app’s information property list and setting its value toUISceneSessionRoleImmersiveSpaceApplication. In that case, visionOS opens the first space that it finds in your app declaration.\nImportant\nBe careful not to overwhelm people when starting your app with an immersive space. For design guidance, seeImmersive experiences.\nClose windows programmatically\nPeople can close windows using system controls, like the close button built into the frame around a macOS window. You can also close windows programmatically. Get thedismissWindowaction from the environment, and call it using the identifier of the window that you want to dismiss:\nIn iPadOS and visionOS, the system ignores the dismiss action if you use it to close a window that’s your app’s only open scene.\nClose spaces programmatically\nTo close a space, call thedismissImmersiveSpaceaction. Like the corresponding open space action, the close action operates asynchronously and requires theawaitkeyword:\nYou don’t need to specify an identifier for this action, because there can only ever be one space open at a time. Like with windows, you can’t dismiss a space that’s your app’s only open scene.\nTransition between a window and a space\nBecause you can’t programmatically close the last open window or immersive space in a visionOS app, be sure to open a new scene before closing the old one. Pay particular attention to the sequencing when moving between a window and an immersive space, because the space’s open and dismiss actions run asynchronously.\nFor example, consider a chess game that begins by displaying a start button in a window. When someone taps the button, the app dismisses the window and opens an immersive space that presents a chess board. The following button demonstrates proper sequencing by opening the space and then closing the window:\nIn the above code, it’s important to include thedismissWindowaction inside the task, so that it waits until theopenImmersiveSpaceaction completes. If you put the action outside the task — either before or after — it might execute before the asynchronous open action completes, when the window is still the only open scene. In that case, the system opens the space but doesn’t close the window.\nSee Also\nSwiftUI",
    "https://developer.apple.com/documentation/visionos#Design": "visionOS\nOverview\nvisionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.\n\nDeveloping for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app’s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS.\nExpand your app into immersive spaces\nStart with a familiar window-based experience to introduce people to your content. From there, addSwiftUIscene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences.\nBuild your app’s 3D content withRealityKitand Reality Composer Pro, and display it with aRealityView. In an immersive experience, useARKitto integrate your content with the person’s surroundings.\n\nExplore new kinds of interaction\nPeople can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures.SwiftUIprovides built-in support for these standard gestures, so rely on them for most of your app’s input. When you want to go beyond the standard gestures, useARKitto create custom gestures.\nTap to select\nPinch to rotate\nManipulate objects\nCreate custom gestures\nDive into featured sample apps\nExplore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash.\nTopics\nApp construction\nDesign\nSwiftUI\nRealityKit and Reality Composer Pro\nARKit\nVideo playback\nXcode and Simulator\nPerformance\niOS migration and compatibility\nEnterprise APIs for visionOS\nvisionOS\nOverview\nExpand your app into immersive spaces\nExplore new kinds of interaction\nDive into featured sample apps\nTopics",
    "https://developer.apple.com/documentation/visionos/tracking-images-in-3d-space": "visionOS\nTracking preregistered images in 3D space\nTracking preregistered images in 3D space\nTracking preregistered images in 3D space\nTracking preregistered images in 3D space\nOverview\nUse ARKit’s support for tracking 2D images to place 3D content in a space. ARKit provides updates to the image’s location as it moves relative to the person. If you supply one or more reference images in your app’s asset catalog, people can use a real-world copy of that image to place virtual 3D content in your app. For example, if you design a pack of custom playing cards and provide those assets to people in the form of a real-world deck of playing cards, they can place unique content per card in a fully immersive experience.\nThe following example tracks a set of images loaded from an app’s asset catalog:\nIf you know the real-world dimensions of the images you’re tracking, use thephysicalSizeproperty to improve tracking accuracy. TheestimatedScaleFactorproperty provides information about how the scale of the tracked image differs from the expected physical size you provide.\nSee Also\nARKit",
    "https://developer.apple.com/documentation/visionos#App-construction": "visionOS\nOverview\nvisionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.\n\nDeveloping for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app’s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS.\nExpand your app into immersive spaces\nStart with a familiar window-based experience to introduce people to your content. From there, addSwiftUIscene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences.\nBuild your app’s 3D content withRealityKitand Reality Composer Pro, and display it with aRealityView. In an immersive experience, useARKitto integrate your content with the person’s surroundings.\n\nExplore new kinds of interaction\nPeople can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures.SwiftUIprovides built-in support for these standard gestures, so rely on them for most of your app’s input. When you want to go beyond the standard gestures, useARKitto create custom gestures.\nTap to select\nPinch to rotate\nManipulate objects\nCreate custom gestures\nDive into featured sample apps\nExplore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash.\nTopics\nApp construction\nDesign\nSwiftUI\nRealityKit and Reality Composer Pro\nARKit\nVideo playback\nXcode and Simulator\nPerformance\niOS migration and compatibility\nEnterprise APIs for visionOS\nvisionOS\nOverview\nExpand your app into immersive spaces\nExplore new kinds of interaction\nDive into featured sample apps\nTopics",
    "https://developer.apple.com/documentation/visionos/capturing-screenshots-and-video-from-your-apple-vision-pro-for-2d-viewing": "visionOS\nCapturing screenshots and video from Apple Vision Pro for 2D viewing\nCapturing screenshots and video from Apple Vision Pro for 2D viewing\nCapturing screenshots and video from Apple Vision Pro for 2D viewing\nCapturing screenshots and video from Apple Vision Pro for 2D viewing\nOverview\nUse screenshots and short videos of your visionOS app to showcase your user interface, highlight functionality, and demonstrate usage. Help people understand what to expect from an immersive experience by recording content from Apple Vision Pro that includes your app and its surroundings.\nThe system renders content with spatial effects and optimizations for viewing during immersive experiences. Techniques that improve rendering performance during normal operation — such asfoveated rendering, which reduces image quality in the peripheral — don’t translate well to 2D displays. To produce high-resolution content for people to view on 2D displays, the system needs to render without these effects and drop some optimizations. Use Developer Capture in Reality Composer Pro to notify the system to reconfigure rendering, and capture screenshots or high-resolution video, including sound, for up to 60 seconds from Apple Vision Pro.\nNote\nFor guidance on the screenshots and previews you include in your app’s product page, seeSubmit your apps to the App Store for Apple Vision Pro.\nPair your Apple Vision Pro to Xcode\nBefore capturing screenshots and video from your device, pair it with a Mac that has Xcode and the visionOS SDK installed. For instructions on pairing your device, seeRunning your app in Simulator or on a device.\nPrepare to capture your app and its surroundings\nSelect a well-lit location that’s free from clutter. Avoid including objects that might distract the audience or get in the way of your app‘s windows and 3D content. Include enough detail in the scene to provide context and anchoring points. Avoid material that you don’t have permission to capture, including people, screens, branded products, logos, artwork, and other intellectual property.\nUse the version of your app that you intend to share with your audience. Build and install your app using a release configuration. This configuration enables code optimizations for better runtime performance and disables the generation of debugging information. Debug configurations typically disable code optimizations and might include UI you donʼt intend to share. Don’t use them to record video for previews you intend to share. Build schemes manage the build configuration Xcode uses during build actions, for more information seeCustomizing the build schemes for a project.\nPlan the tasks you intend to capture ahead of time and keep them short and focused. Launch your app and go to the state where you plan to begin the capture. Reduce unnecessary processing overhead on Apple Vision Pro by quitting other apps and avoiding background tasks.\nTo capture screenshots or video from a device, select your device from the capture dialog in Reality Composer Pro:\nLaunch Reality Composer Pro. Choose Open Developer Tool > Reality Composer Pro from the Xcode menu.\nLaunch Reality Composer Pro. Choose Open Developer Tool > Reality Composer Pro from the Xcode menu.\nChoose File > Developer Capture to bring up the Developer Capture dialog.\nChoose File > Developer Capture to bring up the Developer Capture dialog.\nSelect the device to capture from the pop-up menu.\nSelect the device to capture from the pop-up menu.\n\nIf you see the message “Preparing, wait for the device to be ready”. You can click the info button that appears to the right of the pop-up menu for more information.\nCapture screenshots\nTo begin capturing screenshots from Apple Vision Pro, click the button with the still camera icon in the capture dialog. The system begins your capture session:\n\nTo capture a screenshot immediately, without a countdown, press the spacebar. Click the countdown button to capture a screenshot after a 3 second countdown. Continue to keep relevant content centered and in frame for screenshots. The aspect ratio of screenshots crops content that appears at the sides of an experience.\nThe status area of the capture dialog displays the time remaining before the system ends the capture session. Click the stop button from your Mac to end the capture session yourself.\nCapture video\nTo begin capturing video from the device, click the video camera button in the Developer Capture dialog. This begins a countdown. When the countdown reaches 0, the capture session begins. As the capture process happens, the video changes because the system reconfigures to render content for viewing in 2D. You might notice reduced responsiveness from the device during the session as it devotes more processing to render and capture the video.\nWhile recording on the device, perform your planned interactions. Keep relevant content centered and in frame. The aspect ratio of the video you capture crops content that appears at the sides of an experience. Keep your head stable, and use slow, steady movement to transition the focus of the device when necessary. When viewing the video you capture in 2D, small head movements appear amplified and might be jarring to the audience.\nThe capture session ends when the elapsed time exceeds 60 seconds. You can click the record button again from your Mac to end the session sooner.\nNote\nTo begin a capture, the device must have a stable connection to your Mac and start at low power and thermal levels to stay below thresholds necessary to achieve consistent frame rates. When capturing multiple sessions, you might need to wait between each session.\nReview the captured video file\nEach recording session creates a QuickTime Movie file (.mov) and saves it to the desktop of your Mac. The file includes video captured at 30 FPS using 10-bit HEVC in HDTV Rec. 709 color space with system audio recorded in 32-bit floating-point linear PCM.\nReview the video to make sure that it includes all the content you planned and it doesn’t include any unexpected elements. Ensure that the transitions and animations are smooth and frame rates are consistent.\nUse additional video-editing tools to trim, edit, and apply post-processing, such as stabilization, to the video to create a high-quality preview.\nSee Also\nRealityKit and Reality Composer Pro",
    "https://developer.apple.com/documentation/visionos#Expand-your-app-into-immersive-spaces": "visionOS\nOverview\nvisionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.\n\nDeveloping for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app’s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS.\nExpand your app into immersive spaces\nStart with a familiar window-based experience to introduce people to your content. From there, addSwiftUIscene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences.\nBuild your app’s 3D content withRealityKitand Reality Composer Pro, and display it with aRealityView. In an immersive experience, useARKitto integrate your content with the person’s surroundings.\n\nExplore new kinds of interaction\nPeople can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures.SwiftUIprovides built-in support for these standard gestures, so rely on them for most of your app’s input. When you want to go beyond the standard gestures, useARKitto create custom gestures.\nTap to select\nPinch to rotate\nManipulate objects\nCreate custom gestures\nDive into featured sample apps\nExplore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash.\nTopics\nApp construction\nDesign\nSwiftUI\nRealityKit and Reality Composer Pro\nARKit\nVideo playback\nXcode and Simulator\nPerformance\niOS migration and compatibility\nEnterprise APIs for visionOS\nvisionOS\nOverview\nExpand your app into immersive spaces\nExplore new kinds of interaction\nDive into featured sample apps\nTopics",
    "https://developer.apple.com/documentation/visionos/creating-your-first-visionos-app": "visionOS\nCreating your first visionOS app\nCreating your first visionOS app\nCreating your first visionOS app\nCreating your first visionOS app\nOverview\nIf you’re new to visionOS, start with a new Xcode project to learn about the platform features, and to familiarize yourself with visionOS content and techniques. When you build an app for visionOS, SwiftUI is an excellent choice because it gives you full access to visionOS features. Although you can also use UIKit to build portions of your app, you need to use SwiftUI for many features that are unique to the platform.\nNote\nDeveloping for visionOS requires a Mac with Apple silicon.\nIn any SwiftUI app, you place content onscreen using scenes. A scene contains the views and controls to display onscreen. Scenes also define the appearance of those views and controls when they appear onscreen. In visionOS, you can include both 2D and 3D views in the same scene, and you can present those views in a window or as part of the person’s surroundings.\nScene with a window\nScene with a window and 3D objects\nStart with a new Xcode project and add features to familiarize yourself with visionOS content and techniques. Run your app in Simulator to verify your content looks like you expect, and run it on device to see your 3D content come to life.\nOrganize your content around one or more scenes, which manage your app’s interface. Each scene contains the views and controls you want to display, and the scene type determines whether your content adopts a 2D or 3D appearance. SwiftUI adds 3D scene types specifically for visionOS, and also adds 3D elements and layout options for all scene types.\nCreate your Xcode project\nCreate a new project in Xcode by choosing File > New > Project. Navigate to the visionOS section of the template chooser, and choose the App template. When prompted, specify a name for your project along with other options.\nWhen creating a new visionOS app, you can configure your app’s initial scene types from the configuration dialog. To display primarily 2D content in your initial scene, choose a Window as your initial scene type. For primarily 3D content, choose a Volume. You can also add an immersive scene to place your content in the person’s surroundings.\n\nInclude a Reality Composer Pro project file when you want to create 3D assets or scenes to display from your app. Use this project file to build content from primitive shapes and existing USDZ assets. You can also use it to build and test custom RealityKit animations and behaviors for your content.\nModify the existing window\nBuild your initial interface using standard SwiftUI views. Views provide the basic content for your interface, and you customize the appearance and behavior of them using SwiftUI modifiers. For example, the.backgroundmodifier adds a partially transparent tint color behind your content:\nTo learn more about how to create and configure interfaces using SwiftUI, seeSwiftUI Essentials.\nHandle events in your views\nMany SwiftUI views handle interactions automatically — all you do is provide code to run when the interactions occur. You can also add SwiftUI gesture recognizers to a view to handle tap, long-press, drag, rotate, and zoom gestures. The system automatically maps the following types of input to your SwiftUI event-handling code:\n\nIndirect input. The person’s eyes indicate the target of an interaction. To start the interaction, the person touches their thumb and forefinger together on one or both hands. Additional finger and hand movements define the gesture type.\n\nDirect input. When a person’s finger occupies the same space as an onscreen item, the system reports an interaction. Additional finger and hand movements define the gesture type.\n\nKeyboard input. People can use a connected mouse, trackpad, or keyboard to interact with items, trigger menu commands, and perform gestures.\nFor more information about handling interactions in SwiftUI views, seeHandling User Inputin theSwiftUI Essentialstutorial.\nBuild and run your app\nBuild and run your app in Simulator to see how it looks. Simulator for visionOS has a virtual background as the backdrop for your app’s content. Use your keyboard and your mouse or trackpad to navigate around the environment and interact with your app.\nTap and drag the window bar below your app’s content to reposition the window in the environment. Move the pointer over the circle next to the window bar to reveal the window’s close button. Move the cursor to one of the window’s corners to turn the window bar into a resizing control.\nNote\nApps don’t control the placement of windows in the space. The system places each window in its initial position, and updates that position based on further interactions with the app.\nFor additional information about how to interact with your app in Simulator, seeInteracting with your app in the visionOS simulator.\nSee Also\nApp construction",
    "https://developer.apple.com/documentation/visionos#ARKit": "visionOS\nOverview\nvisionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.\n\nDeveloping for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app’s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS.\nExpand your app into immersive spaces\nStart with a familiar window-based experience to introduce people to your content. From there, addSwiftUIscene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences.\nBuild your app’s 3D content withRealityKitand Reality Composer Pro, and display it with aRealityView. In an immersive experience, useARKitto integrate your content with the person’s surroundings.\n\nExplore new kinds of interaction\nPeople can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures.SwiftUIprovides built-in support for these standard gestures, so rely on them for most of your app’s input. When you want to go beyond the standard gestures, useARKitto create custom gestures.\nTap to select\nPinch to rotate\nManipulate objects\nCreate custom gestures\nDive into featured sample apps\nExplore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash.\nTopics\nApp construction\nDesign\nSwiftUI\nRealityKit and Reality Composer Pro\nARKit\nVideo playback\nXcode and Simulator\nPerformance\niOS migration and compatibility\nEnterprise APIs for visionOS\nvisionOS\nOverview\nExpand your app into immersive spaces\nExplore new kinds of interaction\nDive into featured sample apps\nTopics",
    "https://developer.apple.com/documentation/visionos/creating-a-performance-plan-for-visionos-app": "visionOS\nCreating a performance plan for your visionOS app\nCreating a performance plan for your visionOS app\nCreating a performance plan for your visionOS app\nCreating a performance plan for your visionOS app\nOverview\nPerformance tuning is an important part of the development process, regardless of platform. Performance tuning means making your app run as efficiently as possible, so it does more work in less time and with fewer system resources. Efficiency is especially important on devices that can support multiple apps in an immersive experience. Apps that consume too many resources, can push the device beyond thermal limits. When this occurs, the system takes steps to cool down to a more acceptable level. This can have a noticeable visual impact and be disorienting for the wearer.\nAs you start development, set aggressive goals and evaluate progress throughout the development cycle. Automate the collection of performance metrics as much as possible and look at data over time to see if performance is improving or declining. When you detect a significant decrease in performance, take immediate steps to correct it. When you start fine-tuning early in development, you have more time to make needed changes to algorithms and approaches.\nFor more information on performance tuning, seeImproving your app’s performance.\nSet performance and power targets\nPerformance isn’t a single metric that you measure and improve. Typically, you choose several metrics and set goals for each of them. For example, consider:\nMake sure your app launches quickly; this is your first chance to make a good impression.\nYour interface needs to respond quickly to interactions, even while doing other work. Minimize the time it takes to start tasks. For example, make sure audio and video start without noticeable delays.\nFor an immersive experience with realtime rendering, it’s important to maintain consistently high frame rates. Help maintain these rates by avoiding unnecessary changes that result in more frequent updates to the shared render server. Measure things like update rates, stalls, and hangs in both the render server and your app. Only render the content you need, and optimize the textures and other resources you use during drawing.\nWhen the device begins to reach thermal limits, the system reduces CPU or GPU usage and performance degrades over time. Avoid this thermal ceiling by prioritizing and spreading out work, limiting the number of simultaneous threads your app maintains, and turning off hardware-related features likeCore Locationwhen you don’t need them.\nMake the app do as much as possible using the smallest amount of hardware resources. Minimize task-based overhead.\nUse as little free memory as possible. Don’t allocate or deallocate memory during critical operations, which might make your app appear slow.\nAfter you choose the metrics you want, set realistic goals and prioritize them, so you know which ones matter the most. Performance tuning often involves making tradeoffs between competing goals. For example, if you reduce CPU usage by caching computed data or pre-load assets to improve responsiveness, you increase your app’s memory usage. Make these kinds of tradeoffs carefully, and always measure the results of any changes to learn whether they were successful. In some cases, you might find the sacrifice isn’t worthwhile.\nConsider how people will use your app. If your app runs in the Shared Space, consider more conservative targets and goals for system resources. If you expect people to use your app for longer periods of time, factor this extended use into your targets and goals when choosing metrics.\nIdentify the code flows and user scenarios to test\nAfter you choose the metrics to collect, decide which portions of your app to test. Choose features that are repeatable, measurable, and reliable to test. Repeatable automated tests allow you to compare the results and know the comparisons represent the exact same task. Focus on places where your app executes code, but don’t ignore places where your app hands off data to the system and waits. If your app spends a significant amount of time waiting for information, consider eliminating the requests altogether or batching them to achieve better performance.\nFocus your tuning efforts on the parts of your app that people use the most, or that have the most impact on overall system performance, including:\nUser-facing workflows\nUser-facing workflows\nKey algorithms\nKey algorithms\nTask that allocate or deallocate memory\nTask that allocate or deallocate memory\nBackground and network-based tasks\nBackground and network-based tasks\nCustom Metal shaders\nCustom Metal shaders\nChoose actions that people perform frequently or that correspond to important features. For example, if your app lets someone add a new contact, test the workflow for creating the contact, editing the contact, and saving the results. Test your app with a particular feature enabled and disabled to determine whether the feature is solely responsible for any performance impacts. Choose lightweight workflows such as how your app performs at idle time, and also heavyweight workflows, for example, ones that involve user interactions and your app’s responses. For launch times, gather metrics for both hot and cold launches — that is, when the app is already resident in memory and when it is not.\nConsider thermal and environmental factors\nConsider how environmental factors impact your app. The characteristics of your physical environment can affect system load and thermals of the device. Consider the effect that ambient room temperature, the presence of other people, and the number and type of real-world objects can have on the your app‘s algorithms. Try to test in different settings to get an idea of whether you need to optimize for these scenarios or not.\nUse Xcode’s thermal inducers to mimic the device hitting its thermal limits and consider how your app responds to fair, serious, and critical thermal notifications. You might need to have different performance goals when under thermal pressure, and prioritize optimizing for power or find ways to dynamically lower your app‘s complexity in response to thermal pressure to give a smoother experience, even if latency is a bit higher.\nChoose tools to collect performance data\nThere are many tools and APIs you can use to collect performance-related data for your visionOS app. Use a variety of tools to make sure you have the data you need:\nMonitor the CPU, memory, disk and network gauges in the Debug navigator to track system resources utilization.\nProfile your app to gather performance data on most metrics. Instruments lets you profile your app’s code execution, find memory leaks, track memory allocations, analyze file-system or graphics performance, SwiftUI performance, and much more. Use the RealityKit Trace template to monitoring and investigate render server stalls and bottlenecks on visionOS.\nUseXCTestAPIs to collect performance data.\nUseMetricKitto gather on-device app diagnostics and generate reports.\nReview diagnostic logs for hangs, disk and energy usage, and crashes in the Xcode Organizer.\nReview statistics on the contents of your RealityKit scenes. Use this information to optimize your 3D models and textures.\nAdd signposts to your code to generate timing information you can view in Instruments. For more information, seeRecording performance data.\nInclude log messages to report significant events and relevant data for those events. For more information, seeGenerating log messages from your code.\nGet feedback from testers about their experiences with beta versions of your app. Fill out the Test Information page for your beta version, and request that testers provide feedback about the performance of your app.\nProfile on a physical device\nIn general, profile and analyze performance on a physical device rather than in Simulator. Even if something works well in Simulator, it might not perform as well on devices for all use cases. Simulator doesn’t support some hardware features and APIs. There are differences in the rendering pipeline for Simulator running on macOS, so rendering performance characteristics will be different. Other pipelines such as input delivery and audio or video playback are also different. There are, however, some insights you can gain profiling in Simulator, such as CPU stalls, that help you spot areas to investigate and address.\nBuild automated test cases and run them regularly\nXcode comes with tools to help you automate the collection of performance data:\nUse theXCTestframework to build test cases to collect performance metrics. XCTest lets you gather several different metrics, including the time it takes to perform operations, the amount of CPU activity that occurs during the test, details about memory or storage use, and more.\nUse theXCTestframework to build test cases to collect performance metrics. XCTest lets you gather several different metrics, including the time it takes to perform operations, the amount of CPU activity that occurs during the test, details about memory or storage use, and more.\nUse Instruments to collect metrics for specific interactions with your app. Record those interactions and play them back later to collect a new set of metrics.\nUse Instruments to collect metrics for specific interactions with your app. Record those interactions and play them back later to collect a new set of metrics.\nWrite custom scripts to gather performance-related data using system command-line tools. Integrate these scripts into your project’s build process to automate their execution.\nWrite custom scripts to gather performance-related data using system command-line tools. Integrate these scripts into your project’s build process to automate their execution.\nConfigure Xcode to run test cases each time you build your app, or create a separate target to run test cases or custom scripts on demand. Integrate your performance tests into your Xcode Cloud workflows, or your own custom continuous integration solution.\nNote\nCollect performance data using a production version of your app to obtain more accurate results. Debug builds contain additional code to support debugging operations and logging. You can collect data from debug builds too, but keep those metrics separate from production-build metrics.\nFor information about how to write test cases for your app, seeTesting. For information about how to automate testing with Xcode Cloud, seeXcode Cloud.\nSee Also\nPerformance",
    "https://developer.apple.com/documentation/visionos/building-spatial-experiences-for-business-apps-with-enterprise-apis": "visionOS\nBuilding spatial experiences for business apps with enterprise APIs for visionOS\nBuilding spatial experiences for business apps with enterprise APIs for visionOS\nBuilding spatial experiences for business apps with enterprise APIs for visionOS\nBuilding spatial experiences for business apps with enterprise APIs for visionOS\nOverview\nNote\nThis article is associated with WWDC24 session 10139:Introducing enterprise APIs for visionOS.\nYou can use the entitlements that enterprise APIs for visionOS offer to create even more powerful enterprise solutions and spatial experiences for your visionOS app. The enterprise APIs for visionOS consist of two distinct categories.\nThe first category of APIs provides enhanced sensor access and improves the visual capabilities of Apple Vision Pro, including access to the main camera, improved capture and streaming, and enhanced functionality through the camera that allows you to see what the wearer sees.\nCapture input data from the forward-facing main camera.\nAccess a composite feed of what an Apple Vision Pro wearer is seeing (physical world and digital content).\nScan barcodes and QR codes with the ability to decode contents and locate spatial positions.\nThe second category focuses on platform control to help you get the most out of visionOS. These entitlements provide advanced machine learning (ML) capabilities using the Apple Neural Engine for your custom ML model, enhanced object-tracking capabilities for faster object detection, and the ability to tune the performance of your apps to achieve even more compute functionality for intensive workloads.\nSpecifically target Apple Neural Engine (ANE) for machine learning tasks, similar to iOS.\nOptimize known object detection and tracking using configurable parameters.\nUse increased power of the CPU and GPU for high-compute needs, with a tradeoff of increased thermal usage and reduced battery life.\nStream USB UVC devices connected to the Developer strap.\nNote\nEach of these entitlements allows a device to operate outside the default configuration. When using these features, be aware that they may impact the performance of other apps.\nRequest the entitlements\nIf you’re interested in using the enterprise APIs for visionOS in your app, the Account Holder of your Apple Developer Program and/or Apple Developer Enterprise Program can submit anentitlement request.\nTo be eligible, your app needs to:\nBe for use in a business setting only\nBe for use in a business setting only\nMeet specific criteria associated with usage for each API\nMeet specific criteria associated with usage for each API\nEnterprise APIs for visionOS are eligible for business use only. You can distribute apps that you develop with the enterprise APIs for visionOS privately as proprietary in-house apps or custom apps using Apple Business Manager. For more information on distributing custom apps, seeSet distribution methods.\nYou can also request access to the entitlements forDevelopment Onlypurposes. With this access, you can build and run apps on your registered test devices with development provisioning profiles.\nConfigure your app’s Xcode project\nTo use entitlements, you need to include both the entitlement file and a corresponding license file in your app. After Apple approves your app for one or more entitlements, you receive a license file, along with additional instructions.\nYou add the license file to your app’s Xcode project. Placing the license file within your project and adding it to your build target allows Xcode to compile it within your app and then validate it when checking your entitlements.\nNote\nThe license file comes with an expiration date, so you need to renew it before then to ensure your entitlements continue to function.\nTo add an entitlement in Xcode:\nSelect your project in the Project navigator.\nSelect your project in the Project navigator.\nSelect the applicable target and then click the Signing & Capabilities tab.\nSelect the applicable target and then click the Signing & Capabilities tab.\nClick the Add Capability button (+) and type the name of the entitlement you want to add, such asmain camera access.\nClick the Add Capability button (+) and type the name of the entitlement you want to add, such asmain camera access.\nDouble-click the entitlement to add it to the Signing section. This creates a.entitlementsfile with the relevant capability, such ascom.apple.developer.arkit.main-camera-access.allowfor “Main camera access”.\nDouble-click the entitlement to add it to the Signing section. This creates a.entitlementsfile with the relevant capability, such ascom.apple.developer.arkit.main-camera-access.allowfor “Main camera access”.\nAfter you add the.licenseand.entitlementsfiles, you can implement and test the APIs you have approval to use. For more information, seeAdding capabilities to your app.\nSee Also\nEnterprise APIs for visionOS",
    "https://developer.apple.com/documentation/visionos/improving-accessibility-support-in-your-app": "visionOS\nImproving accessibility support in your visionOS app\nImproving accessibility support in your visionOS app\nImproving accessibility support in your visionOS app\nImproving accessibility support in your visionOS app\nOverview\nvisionOS is an immersive platform that supports people of all abilities. Even though experiences incorporate stunning visual content and hand- and eye-tracking technologies, people can engage with content in other ways. In fact, the platform supports people in many different situations, including those who are blind, have low vision, have limited mobility, or have limb differences. With the help of assistive technologies, people can interact with all of your app’s content.\nDuring development, enable VoiceOver and other assistive features and test your app’s accessibility support. Make sure people can navigate your app’s interface intuitively, and that all of the necessary elements are present. Improve the descriptive information for those elements to communicate their intended purpose. And make sure your app adapts to changing conditions, such as changes to the Dynamic Type setting while your app is running.\nDefault font size\nIncreased font size\nFor general information about supporting accessibility, seeAccessibility. For design guidance, seeHuman Interface Guidelines > Accessibility.\nAdd accessibility traits to RealityKit entities\nVoiceOver and other assistive technologies rely on the accessibility information that your app’s views and content provide. SwiftUI and UIKit provide default information for the standard system views, but RealityKit doesn’t provide default information for the entities in your scenes.\nTo configure the accessibility information for a RealityKit entity, add an instance ofAccessibilityComponentto the entity. Use this component to specify the same values you specify for the rest of your app’s views. The following example shows how to create this component and add it to an entity:\nPeople can use VoiceOver to initiate specific types of actions on your entities. Assign a value to thesystemActionsproperty of your component if your entity supports the incrementing or decrementing of its value, or supports activation with a gesture other than a standard tap. You don’t need to set a system action if you let people interact with the entity using a standard single-tap gesture.\nThe following example uses the content of aRealityViewto determine when activation events occur on the view’s entities. After subscribing to the view’s activation events, the code sets up an asynchronous task to handle incoming events. When a new event occurs, the task executes the custom code to handle a collision.\nAdd support for Direct Gesture mode\nWhen VoiceOver is active in visionOS, people use hand gestures to navigate your app’s interface and inspect elements. To prevent your app’s code from interfering with VoiceOver interactions, the system doesn’t deliver hand input to your app during this time. However, a person can perform a special VoiceOver gesture to enable Direct Gesture mode, which leaves VoiceOver enabled but restores hand input to your app.\nAdd VoiceOver announcements to your code to communicate the results of meaningful events. VoiceOver speaks these announcements at all times, but they are particularly useful when Direct Gesture mode is on. The following example posts an announcement when a custom gesture causes an interaction with a game piece:\nProvide alternatives to input that involves physical movement\nReduced mobility can affect a person’s ability to interact with your app’s content. When designing your app’s input model, avoid experiences that require specific body movements or positions. For example, if your app supports custom hand gestures, add menu commands for each gesture so someone can enter them using a keyboard or assistive device.\nSome assistive technologies let people interact with your app using only their eyes. Using these technologies they can select, scroll, long press, or drag items in your interface. Even if you support other types of interactions, give people a way to access all of your app’s behavior using only these interactions.\nAvoid head-anchored content\nSome assistive technologies allow people to navigate or view your app’s interface using head movements. As the person’s head moves, the assistive technology focuses on the item directly in front of them. Content that follows the movements of the person’s head interferes with the behavior of these assistive technologies.\nWhen designing your interface, place content in windows or anchor it to locations other than the virtual camera. If you do need head-anchored content, provide an alternative solution when relevant assistive technologies are in use. For example, you might move head-anchored content to an anchor point that doesn’t follow the person’s head movements.\nTo determine when to change the anchoring approach for your content, check theaccessibilityPrefersHeadAnchorAlternativeenvironment variable in SwiftUI, or get theprefersHeadAnchorAlternativeproperty. This environment variable istruewhen an assistive technology is in use that conflicts with head-anchored content. Adapt your content to use alternate anchoring mechanisms at that time.\nLimit motion effects in your content\nMotion effects on any immersive device can be jarring, even for people who aren’t sensitive to motion. Limit the use of motion effects that incorporate rapid movement, bouncing or wave-like movement, zooming animations, multi-axis movement, spinning, or rotations. When the person wearing the device is sensitive to motion effects, eliminate the use of these effects altogether.\nThe Reduce Motion system setting lets you know when to provide alternatives for all of your app’s motion effects. Access this setting using theaccessibilityReduceMotionenvironment variable in SwiftUI or with theisReduceMotionEnabledproperty in UIKit. When the setting istrue, provide suitable alternatives for motion effects or eliminate them altogether. For example, show a static snapshot of the ocean instead of a video.\nFor more information, seeHuman Interface Guidelines > Motion.\nInclude captions for audio content\nFor people who are deaf or hard of hearing, provide high-quality captions for your app’s content. Captions are a necessity to some, but are practical for everyone in certain situations. For example, captions are useful to someone watching a video in a noisy environment. Remember to include captions not just for text and dialogue, but also for music and sound effects in your content. For Spatial Audio content, include information in your captions that indicates the direction of various sounds.\nAVKitandAVFoundationprovide built-in support for displaying captioned content. These frameworks configure the font, size, color, and style of the captions automatically according to the person’s accessibility settings. For example, the frameworks adopt the current Dynamic Type setting when displaying text.\nIf you have a custom video engine, check theisClosedCaptioningEnabledaccessibility setting to determine when to display captions. To get the correct appearance information for your captioned content, adoptMedia Accessibilityin your project. This framework provides you with the optimal font, color, and opacity information to apply to captioned text and images.\nSee Also\nDesign",
    "https://developer.apple.com/documentation/visionos/swift-splash": "visionOS\nSwift Splash\nSwift Splash\nSwift Splash\nSwift Splash\nOverview\nApple Vision Pro’s ability to combine virtual content seamlessly with the real world allows for many kinds of interactive virtual experiences. Swift Splash leverages RealityKit and Reality Composer Pro to create a virtual water slide by combining modular slide pieces. When the builder finishes their ride, they can release an adventurous goldfish to try it out.\nSwift Splash uses multiple Reality Composer Scenes to create prepackaged entity hierarchies that represent each of the slide pieces the player connects to construct their ride. It demonstrates how to hide and reveal sections of the entity hierarchy based on the current state of the app. For example, each slide piece contains an animated fish entity that’s hidden until the ride runs and the fish arrives at that particular piece. While Swift Splash is a fun, game-like experience, the core idea of assembling virtual objects out of predefined parts can also be used as the basis for a productivity or creation app.\nSwift Splash scenes include Shader Graph materials built in Reality Composer Pro to change the appearance of the ride at runtime. Each piece can be configured to display in one of three materials: metal, wood, or plastic. Other Shader Graph materials create special effects, such as the movement of the water and the flashing lights on the start and end pieces. Even particle effects are included in some of these prepackaged entities, such as the fireworks that play when the goldfish crosses the finish line.\nBuild slide pieces in Reality Composer Pro\nSlide pieces are the building blocks of Swift Splash. The Reality Composer project contains a separate scene for each one. In addition to the 3D models that make up the slide piece, each scene contains a number of other entities the app uses to animate and place the slide piece.\n\nIn the hierarchy viewer on the left side of the screenshot above, there are two transform entities calledconnect_inandconnect_out. These transforms mark the points where the slide piece connects to the next or previous piece. Swift Splash uses these transforms to place new pieces at the end of the existing slide, as well as to snap pieces to other slide pieces when you manually move them near each other.\nSlide pieces demonstrate the two primary mechanisms Swift Splash uses to find entities at runtime. For some entities, such asconnect_in, Swift Splash uses a naming convention and retrieves the entities by name or suffix when it needs to use them. In other cases, such as when names aren’t unique or the retrieving code needs configuration values, Swift Splash uses a custom component to mark and retrieve entities.\nFor example, animated entities that appear when the ride runs contain a component calledRideAnimationComponent. The app uses this component to determine if the entity is an animation that plays while the ride is running. The component also stores additional state the app needs to implement the ride animation, such as a property calleddurationthat specifies when to start the animations on the next connected slide piece.\nRideAnimationComponentalso includes a property calledisPersistent. Persistent ride animations stay visible at all times but only animate when the ride is running, such as the animated door on the start piece. Nonpersistent ride animations, such as the fish swimming through a slide piece, display only while the ride is running and the fish swims through that particular piece.\nAvoid duplicate materials with material references\nMany of Swift Splash’s slide pieces use the same materials. For example, the shader graph material that changes pieces from metal to wood to plastic is shared by all but one of the slide pieces. To avoid having duplicate copies of each material, Swift Splash leverages USDmaterial referencesto share materials between multiple entities in multiple scenes.\nThe Reality Composer Pro project contains a separate scene for each shared material, containing only that one material. Other track pieces create references to that material. If you change the original material, it affects all of the entities that reference it. For example, a scene calledM_RainbowLights.usdacontains the materialM_RainbowLights, and bothStartPiece.usdaandEndPiece.usdareference that material.\n\nParallelize the asset load\nTo maximize load speed and make the most efficient use of available compute resources, Swift Splash parallelizes loading scenes from the Reality Composer project using aTaskGroup. The app creates a separateTaskfor each of the scenes it needs to load.\nThe app then uses an async iterator to wait for and receive the results.\nFor more information on task groups, seeConcurrencyinThe Swift Programming Language.\nEach of these loaded pieces acts as a template. When the player adds a new piece of that type, the app clones the piece loaded from Reality Composer Pro and adds the clone to the scene.\nSpecify sort ordering for transparent entities\nWhen multiple entities have more than one overlapping, nonopaque material, RealityKit’s default depth-sorting can cause it to draw those entities in the wrong order. As a result, some entities may not be visible from certain angles or in certain positions relative to other transparent entities. The default depth sorting is based on the center of the entity’s bounding box, which may result in the incorrect drawing order when there are multiple overlapping materials with any amount of transparency. You can see an example of this by looking at the start piece in Reality Composer Pro, or by watching the video below.\nThe following video demonstrates the problem. If the three boxes are the bounding boxes for three different transparent entities, and the small spheres are the box centers, the sphere that’s closest to the camera changes as the camera moves around the boxes, which changes the order that RealityKit’s default depth sorting algorithm draws them.\nSwift Splash assigns aModelSortGroupComponentto each of the transparent entities to manually specify the relative depth sorting. To fix the transparency issues in the start piece in the video above, Swift Splash instructs RealityKit to draw the opaque parts of the fish first, its transparent goggles second, the water third, the glass globe fourth, and the selection glow shell last. Swift Splash does this by assigning aModelSortGroupComponentto each of the overlapping entities using the sameModelSortGroup, but with a different order specified.\nTraverse connected track pieces\nThe root entity for all of the individual slide pieces has aConnectableComponent. This custom component marks the entity as one that can be connected or snapped to other connectable entities. At runtime, the app adds aConnectableStateComponentto each slide piece it adds. The component stores state information for the track piece that doesn’t need to be edited in Reality Composer Pro. Among the state information that this component stores is a reference to the next and previous piece.\nTo iterate through the entire ride, ignoring any disconnected pieces, the app gets a reference to the start piece and then iterates untilnextPieceisnil. This iteration, similar to iterating a linked list, repeats many times throughout the app. One example is the function that calculates the duration of the built ride by iterating through the individual pieces and adding up the duration of their animations.\nInteract with the ride\nTo build and edit the ride, players interact with Swift Splash in two different ways. They interact with SwiftUI windows to perform certain tasks, such as adding a new piece or deleting an existing piece of the ride. They also manipulate slide pieces using standard visionOS gestures, including taps, double taps, drags, and rotates. The player taps on a piece to select or deselect it. When a player double taps a piece, they select that piece without deselecting any other selected pieces. When someone drags a piece, it moves around the immsersive space, snapping together with other pieces if placed near one. A two-finger rotate gesture spins the selected track piece or pieces on the Z-axis.\nSwift Splash handles all of these interactions using standard SwiftUI gestures targeted to an entity. To support any of these gestures at any time, the app declares them usingSimultaneousGesture. The code for all of the gestures are contained inTrackBuildingView, which controls the app’s immersive space. Here’s how the app defines the rotation gesture:\nBecause multiple tap gestures on the sameRealityViewexecute with a different number of taps, multiple gestures may be called at once. If a player double taps an entity, for example, both the single tap and the double tap gesture code get called, and the app has to determine which one to execute. Swift Splash makes this determination by using a Boolean state variable. If a player single taps, it sets that variable — calledshouldSingleTap— totrue. Then it waits for a period of time before executing the rest of its code. IfshouldSingleTapgets set tofalsewhile it’s waiting, the code doesn’t execute. When SwiftSplash detects a double tap gesture, it setsshouldSingleTaptofalse, preventing the single-tap code from firing when it executes the double-tap code.\nSee Also",
    "https://developer.apple.com/documentation/visionos/building-an-immersive-media-viewing-experience": "visionOS\nBuilding an immersive media viewing experience\nBuilding an immersive media viewing experience\nBuilding an immersive media viewing experience\nBuilding an immersive media viewing experience\nOverview\nvisionOS provides powerful features for building immersive media playback apps. It supports playing 3D video and Spatial Audio, which helps bring the content to life and makes the viewer feel like they’re part of the action. Starting in visionOS 2, you can take your app’s playback experience even further by creating custom environments using RealityKit and Reality Composer Pro.\nTheDestination Videosample includes a custom environment, Studio. The Studio environment provides a large, open space that’s specifically designed to provide an optimal media viewing experience, as shown in the following image.\n\nDefine a video docking location\nDestination Video usesAVPlayerViewControllerto present video, which enables the app to provide a playback experience across platforms that matches system apps like TV and Music. In visionOS,AVPlayerViewControllerparticipates in the system docking behavior. When you play video in a full-window player then open an immersive experience, the system docks the video screen in a fixed location and presents streamlined playback controls that keep your focus on the content.\n\nThe system determines the docking location for the scene by default. In visionOS 2, you can customize this location by specifying a custom docking region.\nThe environment in Destination Video anchors the video player in front of the walkway at the top of the staircase. To have the video player dock to this location, the project defines aPlayerentity and adds aDockingRegionComponentto it in Reality Composer Pro’s Inspector. This component defines the bounding region for the video player, which has a depth of0and uses a fixed 2.4:1 aspect ratio. Because the aspect ratio is fixed, to configure the docking region’s size use thewidthproperty.\n\nReality Composer Pro provides a template to set up a configuration for Docking Region and related media reflections. You can access this template from the Insert menu by selecting Insert > Environment > Video Dock.\nChoose a location for your docking region that provides a comfortable viewing angle to avoid causing strain or discomfort during longer viewing sessions. Avoid placing objects between the viewer and the video. Using Reality Composer Pro to define the docking region helps to visualize how it looks in context. Review your environment and docking region placement on Apple Vision Pro to get an appropriate sense of scale and layout.\nEnhance the realism of your scene with reflections\nTo make your environment feel like a real, dynamic space, enable reflections from the video player on your environment surfaces. Reality Composer Pro supports two types of reflections that it exposes as Shader Graph nodes:\nA direct reflection of media content. Apply this reflection type on glossy surfaces like metals, mirrors, and water.\nA softer falloff of media content. Apply this reflection type to rougher, more organic surfaces like concrete or wood floor.\nDestination Video uses both types of reflections in its custom environment to create a more realistic experience that better grounds the video player in the scene.\n\nTo learn more about how the custom environment uses reflections, seeEnabling video reflections in an immersive environment.\nDefine the virtual scene lighting\nWhen Destination Video presents the Studio environment in a progressive immersion style, the scene provides a source of indirect lighting to the system. RealityKit represents this light source using an instance ofVirtualEnvironmentProbeComponent. To configure this lighting, the Studio scene defines anEnvironmentProbeentity and adds a Virtual Environment Probe component to it in Reality Composer Pro’s Inspector. It defines the source as a blend between the project’s light and dark image-based lighting (IBL) files, as shown below.\n\nWhen the app presents the light or dark variant, it looks up the probe and configures the source with an appropriate blend value. The app passes a value of0.0for the light variant and1.0for the dark, which effectively toggles which image the component uses as its source.\nEnhance audio immersion\nBy default, visionOS reverberates spatial audio sources by simulating the acoustics of the user’s real environment. When you present a custom environment in a progressive or fully immersive space, you can enhance the level of immersion by applying reverb that matches the visuals of your scene.\nNote\nWhen your app presents an environment in an immersive space using theprogressiveimmersion style, turning the Digital Crown blends the acoustics of the real and virtual spaces to match the visual level of immersion.\nThe Studio environment defines aReverbentity and adds aReverbComponentto it in Reality Composer Pro’s Inspector. The component defines a singlereverbproperty to indicate a specific preset to apply. There are several high-quality reverb presets to choose from including various rooms, hall, and outside spaces. The app uses theVery Large Roompreset, which best fits the environment’s visuals.\n\nDefine a reverb component in your scene even if it doesn’t provide custom audio. The system still uses the reverb preset to spatialize system sounds such as UI interactions.\nNote\nTo optimize the viewing experience, in most cases lower the volume of environment sounds, or stop their playback altogether, when video playback begins.\nSpecify content brightness and surroundings effects\nDestination Video presents the Studio environment by opening an immersive space in theprogressiveimmersion style. This style works well for media apps because people can customize their level of immersion by turning the Digital Crown - from no immersion to fully immersive.\nTo enhance the media presentation and create a more immersive experience when presenting the Studio environment, the app customizes the space in the following ways:\nIt specifies a content brightness value for the immersive space, which indicates the overall brightness of the scene. The system uses this value to tailor the video presentation to best fit its surroundings.\nIt specifies a content brightness value for the immersive space, which indicates the overall brightness of the scene. The system uses this value to tailor the video presentation to best fit its surroundings.\nIt sets a custom tint color for the video passthrough of the user’s hands and surroundings. The app defines tint color that matches the light or dark variant of the environment, and sets the appropriate tint color using thepreferredSurroundingsEffect(_:)view modifier.\nIt sets a custom tint color for the video passthrough of the user’s hands and surroundings. The app defines tint color that matches the light or dark variant of the environment, and sets the appropriate tint color using thepreferredSurroundingsEffect(_:)view modifier.\nPresent a custom scene in the environment picker\nIn visionOS 2,AVPlayerViewControllerautomatically displays a button for a person to pick an environment. When a person presses the button, the system displays a list of viewing environments in which they can watch the video. Selecting an item from the list opens the environment, and docks the player into its ideal viewing location within the scene. By default, the environment picker lists the most recently used system environments, but you can configure the list to show your custom environments as well.\nDestination Video adds the Studio environment’s light and dark variants to the list by attaching the newimmersiveEnvironmentPicker(content:)view modifier to the player view. This modifier takes aViewBuilderthat defines a button for each environment entry you’re adding.\nThis sample passes the modifier a custom view that defines two buttons: one for the light variant and one for the dark. Each button displays a title, thumbnail image, and subtitle that indicates which variant it is.\nAfter adding these buttons to the environment picker, they appear alongside the system environments:\n\nSee Also",
    "https://developer.apple.com/documentation/visionos#Performance": "visionOS\nOverview\nvisionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.\n\nDeveloping for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app’s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS.\nExpand your app into immersive spaces\nStart with a familiar window-based experience to introduce people to your content. From there, addSwiftUIscene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences.\nBuild your app’s 3D content withRealityKitand Reality Composer Pro, and display it with aRealityView. In an immersive experience, useARKitto integrate your content with the person’s surroundings.\n\nExplore new kinds of interaction\nPeople can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures.SwiftUIprovides built-in support for these standard gestures, so rely on them for most of your app’s input. When you want to go beyond the standard gestures, useARKitto create custom gestures.\nTap to select\nPinch to rotate\nManipulate objects\nCreate custom gestures\nDive into featured sample apps\nExplore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash.\nTopics\nApp construction\nDesign\nSwiftUI\nRealityKit and Reality Composer Pro\nARKit\nVideo playback\nXcode and Simulator\nPerformance\niOS migration and compatibility\nEnterprise APIs for visionOS\nvisionOS\nOverview\nExpand your app into immersive spaces\nExplore new kinds of interaction\nDive into featured sample apps\nTopics",
    "https://developer.apple.com/documentation/visionos/analyzing-the-performance-of-your-visionos-app": "visionOS\nAnalyzing the performance of your visionOS app\nAnalyzing the performance of your visionOS app\nAnalyzing the performance of your visionOS app\nAnalyzing the performance of your visionOS app\nOverview\nTo maintain the sense of immersion on Apple Vision Pro, the system attempts to provide the device displays with up-to-date imagery at a constant rate and respond to interactions with minimum latency. Any visual choppiness or delay in responsiveness interferes with the spatial experience. Higher power consumption over extended periods of time, or extreme power consumption over shorter periods of time, can trigger thermal mitigations that also impact the quality of the experience. It’s important to minimize your app’s use of system resources to ensure your app performs well on the platform. Many of the same best practices and optimization procedures you use developing for other Apple platforms apply when developing for visionOS as well. For more information about optimizing your app on other platforms, seeImproving your app’s performance.\nTo get useful information specific to rendering bottlenecks, high system power use, and other issues that effect the responsiveness of your visionOS app, profile your app with the RealityKit Trace template in Instruments. This template helps you identify:\nComplex content or content with frequent updates that cause the render server to miss deadlines and drop frames.\nComplex content or content with frequent updates that cause the render server to miss deadlines and drop frames.\nContent and tasks that result in high system power use.\nContent and tasks that result in high system power use.\nLong running tasks on the the main thread that interfere with efficient processing of input events.\nLong running tasks on the the main thread that interfere with efficient processing of input events.\nTasks running on other threads that don’t complete in time to sync back to the main thread for view hierarchy updates.\nTasks running on other threads that don’t complete in time to sync back to the main thread for view hierarchy updates.\nNote\nYou can profile using a real device or a simulator, but to get the most accurate and actionable information, use a real device. Software and hardware differences between a simulator on your Mac and a real device prevent you from relying on timing information. Simulated devices are useful for quick iteration and improving performance aspects that aren’t based on time.\nOpen a new trace document\nTo create a new trace document:\nSelect your app’s scheme and a visionOS run destination from the Xcode project window.\nSelect your app’s scheme and a visionOS run destination from the Xcode project window.\nChoose Product > Profile.\nChoose Product > Profile.\nChoose RealityKit Trace template\nChoose RealityKit Trace template\nSelect the Choose button.\nSelect the Choose button.\n\nAlternatively, launch Instruments and choose a target app from the template selection dialog.\nThe RealityKit Trace template includes the following instruments:\nCaptures frame render times and lifespans for frames the visionOS render server generates. This instrument indicates when frames miss rendering deadlines and provides average CPU and GPU render rates.\nCaptures comprehensive timing information from the entire render pipeline including rendering, commits, animations, physics, and spatial systems. This instrument identifies potential bottlenecks in your app’s process or in the render server as a result of your app’s content and indicates areas of moderate and high system power usage that require optimization.\nCaptures and displays Runloop execution details.\nProfiles running threads on all cores at regular intervals for all processes.\nCaptures and displays periods of time when the main thread is unresponsive.\nRecords Metal app events.\nConsider adding other instruments to your trace for specific investigations. For example, you can use the Thermal State instrument to record device thermal states to check if thermal pressures are throttling performance.\nProfile your workflows\nClick the record button at the top left of the window to start capturing profile data. Perform the actions in your app that you want to investigate. When you complete the actions, click the record button again to stop recording.\nTo investigate performance issues or analyze system power impact, profile your app in isolation to understand your app’s impact on system performance and ensure you get the most actionable information. For apps that run alongside other apps, profile your app again with those other apps running to understand how people experience your app in conjunction with other apps.\nInspect frame rendering performance\nTo maintain a smooth visual experience, the system tries to render new frames for the Apple Vision Pro at 90 frames per second (FPS). The system renders at other frame rates depending on the content it displays and the current surroundings. Each frame has a deadline for rendering based on the target frame rate. Not meeting these deadlines results in dropped frames. This creates a poor spatial experience overall. People tend to notice it in the visual performance of Persona and SharePlay experiences, video playback, and scrolling. The RealityKit Frames instrument displays the time spent rendering each frame in the Frames section of its timeline:\n\nWhen you zoom out, you can identify areas with a high number of frame drops or with frames running close to the rendering deadline. The timeline uses green to identify frames that complete rendering before the deadline, orange for frames that complete rendering close to the deadline, and red for frames that don’t complete rendering that the renderer drops. Dropped frames contribute to a poor spatial experience, but frames that complete close to their rendering deadline indicate performance problems too. Hold the Option key and drag to zoom into a frame, or group of frames, to see their lifespan broken down in stages:\n\nThis provides you with insight into which portion of the rendering pipeline to investigate further. This timeline also includes sections that visualize the Average CPU Frame Time and Average GPU Frame Time to indicate the type of processing that computes the frames. A region of the timeline without a frame block indicates a period of time without changes to a person’s surroundings or app updates. The render server avoids computing new frames to send to the compositor during these periods which helps optimize power use.\nMonitor system power usage\nWhen thermal levels rise to levels that trigger thermal mitigations in the system, performance degrades and negatively impacts the responsiveness of your app. Optimize for power to avoid this negative impact. The timeline for the RealityKit Metrics instrument includes a System Power Impact section to identify areas of high power usage in your app:\n\nIf the timeline displays green, the tool considers your app’s impact on system power low enough to sustain. Regions that display orange or red indicate the system power usage could cause thermal levels to rise and trigger thermal mitigations. This decreases the availability of system resources, which can cause visual interruptions and responsiveness issues.\nNote\nIf the render server can’t maintain the target frame rate of 90 FPS due to thermal pressure, it might reduce its frame rate in half. When this occurs, all frames in the frames track show up as missing their rendering deadlines. Other factors can cause reduced frame rate, including the complexity and frequency of the content the system is processing. Use the Thermal State instrument to determine if thermal conditions are causing the rate limiting or if it’s due to other factors.\nIdentify bottlenecks\nThe Bottlenecks section of the timeline for the RealityKit Metrics instrument contains markers that indicate high overhead in your app or the render server that contribute to dropped frames and high system power use. When you encounter either of these issues, check if the timeline identifies bottlenecks you can address. Double-click on any of the markers to display more information in the detail area at the bottom of the instruments window. If the detail area is hidden, choose View > Detail Area > Show Detail Area to reveal it. The render server encounters bottlenecks in either the CPU or GPU. The instrument categorizes bottlenecks by their severity and type.\nTo filter the bottlenecks listed in the detail area to a particular time period, drag inside the timeline to select the region. To see an outline view of the bottlenecks organized by severity and type, select Summary: RealityKit Bottlenecks from the menu at the top left of the detail area. Click the arrow button to the right of the severity or type in the outline view to show the list of bottlenecks in that category.\nWhen you select a specific bottleneck, the extended detail provides recommendations for you to address the bottleneck – choose View > Show Extended Detail to reveal the extended detail if it’s hidden.\n\nExplore the metrics that relate to bottlenecks\nThe trace provides additional information you can use to identify changes to make in your app to address these bottlenecks. Click the expansion arrow for the RealityKit Metrics instrument timeline to reveal graphs specific to each major category of work. Use the metrics associated with these graphs to determine which RealityKit feature has the biggest impact on high CPU frame times in the app process or in the render server. When interpreting these graphs, lower indicates better performance and power. The metrics represent values from all apps running, so profile with just your app running when trying to optimize for these metrics.\n\nMetrics related to the cost of 3D RealityKit rendering in the render server. This includes the number of draw calls, triangles, and vertices from all apps.\nMetrics related to UI content rendering costs in the render server. This includes the total number of render passes, offscreen render passes, and translucent UI meshes from all apps.\nMetrics related to the costs of entity commits in the app and the render server. This includes the number of RealityKit entities shared with the render server from all apps, as well as the number of updates received from all apps over certain intervals.\nMetrics related to the cost of RealityKit animations in the app and the render server. This includes the number of skeletal animations, across all apps.\nMetrics related to the cost of RealityKit physics simulations, collisions, and hit testing in the app process and render server. This includes the number of rigid body counts and colliders in use, as well as the type of physics shapes that the UI and other 3D content use, across all apps.\nMetrics related to the costs of spatial algorithms in the render server. This includes the number of custom anchors, across all apps.\nTip\nThe graphs for some sections combine several individual metrics. The heading indicates this by displaying a graph count. Click on the bottom of the timeline’s heading and drag down to display individual graphs for each metric. For example, the 3D Render Timeline might display 13 Graphs in the heading; expanding that timeline exposes individual graphs for 3D Mesh Draw Calls, 3D Mesh Triangles, 3D Mesh Vertices, and the 10 additional metrics.\nThe timeline for your app’s process helps summarize information from the instruments about your process and the work the render server completes for your process.\n\nChoose an option from the pop-up in the timeline header to show different graphs in the timeline:\nTime each thread spends waiting or busy.\nTime the main thread is unresponsive.\nCPU usage and lifecycle status.\nOverhead attributed to RealityKit systems.\nWhen you select the timeline for your app’s process, you can choose instrument summaries and profile data to display in the detail area from the popup-button at its top-left:\n\nTo filter the information in the detail area by time, select periods of time in the timeline  above.\nDetect delays on the main thread\nSelect Hangs in your app’s process timeline to identify times in the trace that might have interaction delays. Use the RealityKit Metrics and Time Profiler summaries to better understand the work your app is doing. Choose the following options from the detail area pop-up menu:\nShows information from the Time Profiler instrument to determine what your app is doing during a hang.\nRealityKit System CPU times: Shows minimum, maximum, and average times the CPU spends on various RealityKit system operations.\nOptimize any 3D render updates, hit testing, and collision work you find. For more information about addressing hangs in your app, seeImproving app responsiveness.\nManage audio overhead\nUse the Audio Playback section of your process’s timeline to identify areas of high audio overhead. The system defaults to using spatial audio for your app when running on visionOS. It processes information in real time about your position, surroundings, and the current location of audio sources to generate an immersive audio experience. If you include too many concurrent audio sources that require the system to adapt audio sources to their location within a large space, the increased demand on system resources can lead to delays in the audio output.\nTo reduce the spatial audio work, limit:\nThe number of concurrently playing audio sources\nThe number of concurrently playing audio sources\nThe number of moving audio sources\nThe number of moving audio sources\nThe size of the soundstage\nThe size of the soundstage\nConsider creating a pool of audio players to limit the maximum number of players your app uses. Place players on stationary entities, instead of moving entities, when appropriate. Initializing several audio players at the same time causes a high overhead that affects other aspects of the system, such as rendering performance. Consider the other tasks the system completes during these allocations and space them out over time. For more information, seeCreate a great spatial playback experience.\nProfile custom materials for optimization\nUse the Metal System Trace template to profile your custom materials in isolation before adding more visual effects to them. Examine the Metal GPU cost and try to optimize for GPU ALU Instructions and GPU Texture reads and writes per frame. For more information on using this template, seeAnalyzing the performance of your Metal app.\nIn order to use the Metal Application instrument to profile your custom materials accurately, set a fixed performance state:\nClick and hold the record button and select Recording Options.\nClick and hold the record button and select Recording Options.\nSelect the options for the Metal Application instrument.\nSelect the options for the Metal Application instrument.\nSet the Counter Set to Performance Limiters.\nSet the Counter Set to Performance Limiters.\nSet the Performance State to Minimum or Medium.\nSet the Performance State to Minimum or Medium.\n\nA person’s eye position, the distance from the person to the content, and the amount of content the app displays all impact the amount of GPU work done for custom materials from Reality Composer Pro. To compare traces, keep these factors as consistent as possible. For a custom material you use in a Fully Immersive Space, try to remove all other content from the scene while profiling to isolate the overhead of the material. For custom materials you use for other types of content, consider anchoring them to a person’s head in a test scene so the distance from the person remains fixed.\nNote\nUsing an unlit or inexpensive custom material becomes even more important when your app usesMetaland theCompositor Servicesframework to present a fully immersive experience due to the number of pixels your app must render.\nSee Also\nPerformance",
    "https://developer.apple.com/documentation/visionos/happybeam": "visionOS\nHappy Beam\nHappy Beam\nHappy Beam\nHappy Beam\nOverview\nIn visionOS, you can create fun, dynamic games and apps using several different frameworks to create new kinds of spatial experiences: RealityKit, ARKit, SwiftUI, and Group Activities. This sample introduces Happy Beam, a game where you and your friends can hop on a FaceTime call and play together.\nYou’ll learn the mechanics of the game where grumpy clouds float around in the space, and people play by making a heart shape with their hands to project a beam. People aim the beam at the clouds to cheer them up, and a score counter keeps track of how well each player does cheering up the clouds.\nDesign the game interface in SwiftUI\nMost apps in visionOS launch as a window that opens different scene types depending on the needs of the app.\nHere you see how Happy Beam presents a fun interface to people by using several SwiftUI views that display a welcome screen, a coaching screen that gives instructions, a scoreboard, and a game-ending screen.\nWelcome window\nInstructions\nScoreboard\nEnding window\n\n\n\n\nThe following shows you the primary view in the app that displays each phase of gameplay:\nWhen 3D content starts to appear, the game opens an immersive space to present content outside of the main window and in a person’s surroundings.\nTheHappyBeamcontainer view declares a dependency onopenImmersiveSpace:\nIt later uses that dependency to open the space from the app’s declaration when it’s time to start showing 3D content:\nDetect a heart gesture with ARKit\nThe Happy Beam app recognizes the centralheart-shaped handsgesture using ARKit’s support for 3D hand tracking in visionOS. Using hand tracking requires a running session and authorization from the wearer. It uses theNSHandsTrackingUsageDescriptionuser info key to explain to players why the app requests permission for hand tracking.\n\nHand-tracking data isn’t available when your app is only displaying a window or volume. Instead, it’s available when you present an immersive space, as in the previous example.\nYou can detect gestures using ARKit data with a level of accuracy that depends on your use case and intended experience. For example, Happy Beam could require strict positioning of finger joints to closely resemble a heart shape. Instead, however, it prompts people to make a heart shape and uses a heuristic to indicate when the gesture is close enough.\nThe following checks whether a person’s thumbs and index fingers are almost touching:\nSupport several kinds of input\nTo support accessibility features and general user preferences, include multiple kinds of input in an app that uses hand tracking as one form of input.\nHappy Beam supports several kinds of input:\nInteractive hands input from ARKit with the custom heart gesture.\nDrag gesture input to rotate the stationary beam on its platform.\nAccessibility components from RealityKit to support custom actions for cheering up the clouds.\nGame Controller support to make control over the beam more interactive from Switch Control.\nDisplay 3D content with RealityKit\nThe 3D content in the app comes in the form of assets that you can export from Reality Composer Pro. You place each asset in theRealityViewthat represents your immersive space.\nThe following shows how Happy Beam generates clouds when the game starts, as well as materials for the floor-based beam projector. Because the game uses collision detection to keep score — the beam cheers up grumpy clouds when they collide — you make collision shapes for each model that might be involved.\nAdd SharePlay support for multiplayer gaming experiences\nYou use the Group Activities framework in visionOS to support SharePlay during a FaceTime call. Happy Beam uses Group Activities to sync the score, active players list, and the position of each player’s projected beam.\nNote\nDevelopers using theApple Vision Pro developer kitcan test spatial SharePlay experiences on-device by installing thePersona Preview Profile.\nUse a reliable channel to send information that’s important to be correct, even if it can be slightly delayed as a result. The following shows how Happy Beam updates the game model’s score state in response to a score message:\nUse an unreliable messenger for sending data with low-latency requirements. Because the delivery mode is unreliable, some messages might not make it. Happy Beam uses the unreliable mode to send live updates to the position of the beam when each participant in the call chooses the Spatial option in FaceTime.\nThe following shows how Happy Beam serializes beam data for each message:\nSee Also",
    "https://developer.apple.com/documentation/visionos#iOS-migration-and-compatibility": "visionOS\nOverview\nvisionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.\n\nDeveloping for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app’s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS.\nExpand your app into immersive spaces\nStart with a familiar window-based experience to introduce people to your content. From there, addSwiftUIscene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences.\nBuild your app’s 3D content withRealityKitand Reality Composer Pro, and display it with aRealityView. In an immersive experience, useARKitto integrate your content with the person’s surroundings.\n\nExplore new kinds of interaction\nPeople can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures.SwiftUIprovides built-in support for these standard gestures, so rely on them for most of your app’s input. When you want to go beyond the standard gestures, useARKitto create custom gestures.\nTap to select\nPinch to rotate\nManipulate objects\nCreate custom gestures\nDive into featured sample apps\nExplore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash.\nTopics\nApp construction\nDesign\nSwiftUI\nRealityKit and Reality Composer Pro\nARKit\nVideo playback\nXcode and Simulator\nPerformance\niOS migration and compatibility\nEnterprise APIs for visionOS\nvisionOS\nOverview\nExpand your app into immersive spaces\nExplore new kinds of interaction\nDive into featured sample apps\nTopics",
    "https://developer.apple.com/documentation/visionos/adopting-best-practices-for-privacy": "visionOS\nAdopting best practices for privacy and user preferences\nAdopting best practices for privacy and user preferences\nAdopting best practices for privacy and user preferences\nAdopting best practices for privacy and user preferences\nOverview\nTo protect user privacy, the system handles camera and sensor inputs without passing the information to apps directly. Instead, the system enables your app to seamlessly interact with a user’s surroundings and to automatically receive input from the user. For example, the system handles the eye- and hand-position data needed to detect interactions with your app’s content. Similarly, the system provides a way to automatically alter a view’s appearance when someone looks at it, without your app ever knowing what the user is looking at.\nIn the few cases where you actually need access to hand position or information about the user’s surroundings, the system requires you to obtain authorization from the user first.\n\nImportant\nIt’s your responsibility to protect any data your app collects, and to use it in responsible and privacy-preserving ways. Don’t ask for data that you don’t need, be transparent about how you use the data you acquire, and respect the choices of the person whose data it is.\nFor information about how to specify the privacy data your app uses, seeDescribing data use in privacy manifests. For general information about privacy, seeProtecting the User’s Privacy.\nAdopt the system-provided input mechanisms\nOn Apple Vision Pro, people use their eyes and hands to interact with the items they see in front of them. Where they look determines where the system applies focus, and a tap gesture with either hand generates a touch event on that focused item. The system can also detect when someone’s fingers interact with virtual items in the person’s field of vision. When you adopt the standard UIKit and SwiftUI event-handling mechanisms, you get all of these interactions automatically.\nFor most apps, the system-provided gesture recognizers are sufficient for responding to interactions. Although you can get the position of someone’s hands with ARKit, doing so isn’t necessary for most apps. Collect hand-position data only when the system doesn’t offer what you need. For example, you might use hand-position data to attach 3D content to the person’s hands. Some other things to remember about hand-position data:\nPeople can deny your request for access to hand-position data. Be prepared to handle situations where the data isn’t available.\nPeople can deny your request for access to hand-position data. Be prepared to handle situations where the data isn’t available.\nYou must present an immersive space to access hand data. When you open an immersive space, the system hides other apps.\nYou must present an immersive space to access hand data. When you open an immersive space, the system hides other apps.\nFor information about how to handle the standard-system events, see theSwiftUIandUIKitdocumentation.\nProvide clear messaging around privacy-sensitive features\nThe following ARKit features require you to provide a usage description string in your app’sInfo.plistfile:\nWorld-tracking data\nWorld-tracking data\nHand-tracking data\nHand-tracking data\nOther privacy-sensitive technologies in visionOS also require you to supply usage description strings. For example, you provide usage descriptions for the Core Location features you adopt. These strings communicate why your app needs the data, and how you plan to use the data to help the person using your app. The first time you request authorization to use the technology, the system prompts the person to grant or deny access to your app. The system includes your usage-description string in the dialog it displays.\nFor information about requesting access to ARKit data, seeARKit. For guidance on how to craft good messages around privacy-friendly features, seeHuman Interface Guidelines.\nSee Also\nDesign",
    "https://developer.apple.com/documentation/visionos/designing-realitykit-content-with-reality-composer-pro": "visionOS\nDesigning RealityKit content with Reality Composer Pro\nDesigning RealityKit content with Reality Composer Pro\nDesigning RealityKit content with Reality Composer Pro\nDesigning RealityKit content with Reality Composer Pro\nOverview\nUse Reality Composer Pro to visually design, edit, and preview RealityKit content. In Reality Composer Pro, you can create one or more scenes, which  act as a container for RealityKit content. Scenes contain hierarchies of entities, which are virtual objects such as 3D models.\n\nIn addition to helping you compose scenes, Reality Composer Pro also gives you the ability to add and configure components — even custom components that you’ve written — to the entities in your scenes and also lets you create complex materials and effects using a node-based material editor called Shader Graph.\nLaunch Reality Composer Pro\nWhen you create a visionOS project in Xcode, it also contains a default Reality Composer Pro project namedRealityKitContentwithin the Packages folder, which is a Swift package. TheRealityKitContentpackage can include images, 3D models, and other assets like audio and video files. The assets you add to your project go in theRealityKitContent.rkassetsbundle, while your source code goes into its Sources directory. The package also contains a file calledPackage.realitycomposerpro, which is the actual Reality Composer Pro project.\nTo launch Reality Composer Pro, double-click thePackage.realitycomposerprofile in the Project navigator, or click the Open in Reality Composer Pro button. If your project doesn’t already have a Reality Composer Pro project, you can launch Reality Composer Pro directly by choosing Xcode > Open Developer Tool > Reality Composer Pro.\nFor efficiency, store all of your RealityKit assets in Reality Composer Pro projects. Xcode compiles Reality Composer Pro projects into a more efficient format when you build your app.\nNote\nLoading assets from a.realityfile is considerably faster and more resource efficient than loading individual asset files.\nOrient yourself in Reality Composer Pro\nThe Reality Composer Pro window has several sections. The top-half displays the active scene. If you have multiple scenes, the window shows a tab bar at the top with one tab for each open scene. Ascenein Reality Composer Pro is an entity hierarchy stored in a.usdafile.\nThe left side of the top pane contains the hierarchy browser, which shows a tree representation of the entities in the active scene. You can toggle it using the top-left toolbar button to reveal errors and warnings. The middle pane is the 3D View, which shows a 3D representation of the active scene. The top-right is the inspector, which shows configurable values for the item selected in the 3D view, hierarchy view, or Shader Graph, depending on which has focus.\nTip\nA Reality Composer Pro scene can represent an entire RealityKit scene, and you can have multiple scenes in your Reality Composer Pro project, each driving a differentRealityViewin the same app. A scene can also contain a collection of entities to use as a building block. For example, if you had an airplane model, you might build a scene for it that contains its 3D model, a particle effect to make smoke come out its engine, and audio entities or components that represent the various sounds a plane makes. Your app could then load those combined assets and use them together anywhere it needs.\nThe bottom half of Reality Composer Pro contains the following four tabs:\nDisplays all of the assets in your project.\nAn advanced, node-based material editor.\nA tool for combining sound assets.\nInformation about the currently open scene, such as the number of entities, vertices, and animations it contains.\n\nReality Composer Pro projects start with a single empty scene calledScenewhich is stored in a file calledScene.usda. You can create as many additional scenes as you need by choosing File > New > Scene. New scenes open as tabs along the top of the window, and they also appear in the Project Browser as.usdafiles.\nIf you close a scene’s tab and need to re-open it, double-click on the scene’s.usdafile in the Project Browser. If you no longer need a scene, delete its.usdafile from the Project Browser or remove it from your project’s.rkassetsbundle in Xcode.\nTo delete a scene:\nClose the scene tab by selecting File > Close Tab\nClose the scene tab by selecting File > Close Tab\nSelect the scene’s.usdafile in the Project Browser\nSelect the scene’s.usdafile in the Project Browser\nControl-click the scene’s.usdafile  the Project Browser.\nControl-click the scene’s.usdafile  the Project Browser.\nChoose Delete from the contextual menu.\nChoose Delete from the contextual menu.\nClick Move to Trash.\nClick Move to Trash.\nThis removes the scene’s.usdaand the scene tab at the top of the window.\nAdd assets to your project\nIn Reality Composer Pro, you design scenes by first importing assets into your project. Then add assets to scenes and move, rotate, and scale them. The Project Browser tab displays all of the asset files in your project. You can add new assets by dragging them to the Project Browser or by choosing File > Import and select the assets to add to your project. To add an asset from the Project Browser to the current scene, drag it to the 3D view in the center of the window, or to the hierarchy view in the top-left of the window.\nNote\nReality Composer Pro projects can contain assets not used in any scene. Such assets are still compiled into your app and can be loaded at runtime and take full advantage of the efficient loading process for.realityfiles.\nReality Composer Pro can represent many assets as entities, but it can’t represent all assets that way; for example:\nUSDZ models do become an entity or entity hierarchy when you add them to a scene.\nUSDZ models do become an entity or entity hierarchy when you add them to a scene.\nImage files do not become an entity. Reality Composer Pro only uses image assets indirectly, such as being the source texture for materials you build in Shader Graph. If you drag assets that Reality Composer Pro can’t turn into an entity, nothing happens.\nImage files do not become an entity. Reality Composer Pro only uses image assets indirectly, such as being the source texture for materials you build in Shader Graph. If you drag assets that Reality Composer Pro can’t turn into an entity, nothing happens.\nAdd any 3D models, animations, sounds, and image files you need to your project. You can organize your assets into subfolders to make the Project Browser more manageable as your project grows in size.\nReality Composer Pro has a library of assets that you can use in your own apps. You can access the library by clicking the Add button (+) in the toolbar. Click the icon of the down-arrow inside a circle next to an asset to download the asset to Reality Composer Pro. When the download finishes, you can double-click or drag the asset into your project.\n\nImportant\nReality Composer Pro treats your imported assets as read-only.\nChanges you make to assets in a scene only affect that scene’s copy of the asset. The changes you make are stored in the scene’s.usdafile, not in the original asset. That means you can work without fear of inadvertently changing other scenes. If you plan to make significant changes to an imported 3D model, such as by replacing its materials with dynamic Shader Graph materials, import the model as a.usdcfile instead of as a.usdzfile, and then separately import just the supporting assets you need to avoid Xcode compiling assets that you don’t need into your app.\nCompose scenes from assets\nAll RealityKit entities in a scene exist at a specific position, orientation, and scale, even if that entity has no visual representation. When you click to select an entity in the 3D view or hierarchy view, Reality Composer Pro displays:\nA manipulator over the entity in the 3D view.\nA manipulator over the entity in the 3D view.\nAny configurable values from the entity’s components in the inspector on the right.You can use the manipulator to move, rotate, and scale the selected entity.\nAny configurable values from the entity’s components in the inspector on the right.\nYou can use the manipulator to move, rotate, and scale the selected entity.\nTo move the selected entity around the 3D scene, drag the small colored cone that corresponds to the axis you want to move it along. Alternatively, you can drag the entity itself to move it freely relative to your viewing angle.\nTo move the selected entity around the 3D scene, drag the small colored cone that corresponds to the axis you want to move it along. Alternatively, you can drag the entity itself to move it freely relative to your viewing angle.\nTo rotate the selected entity, click on the manipulator’s rotation control, which looks like a circle, and drag in a circular motion.Reality Composer Pro’s manipulator only shows one rotation control at a time.To rotate an entity on one of the other axes, click the cone corresponding to the axis you want to rotate. For example, if you want to rotate the entity on theXaxis, tap the red cone to bring up the red rotation handle for that axis.\nTo rotate the selected entity, click on the manipulator’s rotation control, which looks like a circle, and drag in a circular motion.\nReality Composer Pro’s manipulator only shows one rotation control at a time.\nReality Composer Pro’s manipulator only shows one rotation control at a time.\nTo rotate an entity on one of the other axes, click the cone corresponding to the axis you want to rotate. For example, if you want to rotate the entity on theXaxis, tap the red cone to bring up the red rotation handle for that axis.\nTo rotate an entity on one of the other axes, click the cone corresponding to the axis you want to rotate. For example, if you want to rotate the entity on theXaxis, tap the red cone to bring up the red rotation handle for that axis.\nTo scale the selected entity uniformly, click the rotation circle and drag away from the entity origin to scale it up, or toward the entity origin to scale it down. Because it scales uniformly, it doesn’t matter which rotation handle Reality Composer Pro is showing.\nTo scale the selected entity uniformly, click the rotation circle and drag away from the entity origin to scale it up, or toward the entity origin to scale it down. Because it scales uniformly, it doesn’t matter which rotation handle Reality Composer Pro is showing.\nNote\nIn the manipulator, Red indicates the X axis, Green indicates the Y axis, and Blue indicates the Z axis.\nAlternatively, you can make the same changes to the selected entity by typing new values into the transform component of the inspector. The transform component stores the position, rotation, and scale for an entity. The manipulator is just a visual way to change the values on this component.\n\nActivate and deactivate scene entities\nReality Composer Pro scenes can get quite complex and sometimes contain overlapping entities, which can be difficult to work with. To simplify a scene, you can deactivate entities to remove them from the 3D view. Deactivate entities by Control-clicking them and selecting Deactivate from the contextual menu. The entity still exists in your project and is shown in the hierarchy view, albeit grayed out and without any child entities. It won’t, however, appear in the 3D view. Xcode doesn’t compile deactivated entities into your app’s bundle, so it’s important to re-activate any entities your app needs before saving your project. To reactivate an entity, Control-click the entity in the hierarchy view and select Activate from the contextual menu.\nAdd components to entities\nRealityKit follows a design pattern called Entity-Component-System (ECS). In ECS, you store data on an entity using components and then implement entity behavior by writing systems that use the data from those components. You can add and configure components to entities in Reality Composer Pro, including both built-in components likeParticleEmitterComponent, and custom components that you write and place in the Sources folder of your Reality Composer Pro Swift package. You can also create new components in Reality Composer Pro and edit them in Xcode.\nFor more information about ECS, seeUnderstanding the modular architecture of RealityKit.\nTo add a component to an entity, select that entity in the hierarchy view or 3D view. At the bottom-right of the inspector window, click Add Component. A list of available components appears with New Component at the top. If you select the first item, Reality Composer Pro creates a new component class in the Sources folder, and optionally a new system class. It also adds the component to the selected entity. If you select any other item in the list, it adds that component to the selected entity if it doesn’t already have that component.\n\nCreate or modify entity hierarchies\nReality Composer Pro scenes are hierarchies of RealityKit entities. You can change the relationship between entities in the hierarchy browser except for parts of the hierarchy imported from a.usdzfile, which Reality Composer Pro treats as a read-only file.\nTo change the relationship between entities, or to create a relationship between two currently unrelated entities, use the hierarchy view and drag an entity onto the entity that you want it to be part of. If you want an entity to become a root entity, drag it to the Root transform at the top of the hierarchy view.\nModify or create new materials\nWhen you import a USDZ model into Reality Composer Pro, it creates a RealityKit material for every physically-based rendering (PBR) material the asset contains. Reality Composer Pro displays materials in the hierarchy view just like it displays entities, except it uses a paintbrush icon. Reality Composer Pro doesn’t display materials in the 3D view.\nNote\nThe library in Reality Composer Pro contains materials for several common real-world surfaces like metal, wood, and denim that you can import into your project.\nIf you select a PBR material in the hierarchy view, you can edit it using the inspector. You can replace images, colors, or values for any of the PBR attributes with another image, color, or value of your choosing. Any changes you make to a material affects any entity that’s bound to that material. You can also create new materials from scratch by clicking the Add button (+) at the bottom of the scene hierarchy and choosing Material.\n\nBuild materials in Shader Graph\nPBR materials are great at reproducing real-world surfaces. However, they can’t represent nonrealistic materials like cartoon shaders, and they can’t contain logic. This means that you can’t animate a PBR material or have it react to input from your app.\nReality Composer Pro offers a second type of material called acustom material. You can build and edit custom materials using the Shader Graph tab. Shader Graph provides a tremendous amount of control over materials and allows you to do things that would otherwise require writing Metal shaders. For more information on writing Metal shaders, seeMetal.\nNote\nRealityKit doesn’t represent Reality Composer Pro custom materials as an instance ofCustomMaterial, as you might expect. Instead, RealityKit represents these materials asShaderGraphMaterialinstances.\n\nThe materials you build in the editor can affect both the look of an entity and its shape. If you build a node graph and connect it to the Custom Surface pin on the output node, that node graph controls the surface appearance of the model and roughly equates to writing Metal code in a fragment shader. If you build a node graph and connect it to the Custom Geometry Modifier output pin, those nodes control the shape of the entity, which equates to Metal code running in a vertex shader.\nNodes represent values and operations and serve the same purpose as either a variable or constant, or a function in Metal. If you need the sine of a value, for example, connect the value’s output node to the input pin of aSinnode. Add new nodes to the graph by double-clicking the background of the Shader Graph view or click the New Node button on the right side of the screen.\nImportant\nSome nodes, likeSin, are universal and can be used with either output pin. Other nodes are specific to either the Custom Surface or Geometry Modifier outputs. If a node name starts with Geometry Modifier, you can only connect it to the Geometry Modifier output pin. If the node’s name starts with “Surface”, you can only connect it to the Custom Surface output pin.\nTo unlock the real power of Shader Graph, you need to be able to change values on the material from Swift code. Shader Graph allows you to do this by creatingpromoted inputs, which are parameters you can set and read from Swift to change your material at runtime. If you have a feature that you want to turn on and off, you might create a Boolean input parameter and have conditional logic based on its value. If you want to smoothly interpolate between two colors, you might create aFloatinput parameter and use it to control how to interpolate between the two colors. You can Control-click on a constant node and select Promote to turn it into a promoted input. You can also turn a promoted input back into a constant by Control-clicking it and selecting Demote.\nIf you don’t have an existing constant to promote, you can create new promoted inputs using the inspector. The New Input button only shows up in the inspector when you select a material in the hierarchy view but have no nodes selected in the Shader Graph tab.\n\nTo change the value of an input parameter from Swift code, usesetParameter(name:value:), passing the name of the parameter and the new value. Note that parameter names are case sensitive, so yournamestring must exactly match what you called the parameter in Shader Graph.\nFor examples of Shader Graph use, seeDioramaandHappy Beam.\nUse references to reuse assets\nIf your project has multiple scenes that share assets, you can use references to avoid creating duplicate assets. Areferenceacts like an alias in Finder — it points to the original asset and functions just as if it were another copy of that asset.\nCreate references using the inspector. By default, the references section is hidden for entities and materials that don’t have any references. To add a new reference to an asset or material that doesn’t have one, choose Reality Composer Pro > Settings and uncheck Hide Empty References.\n\nTo add a reference, click the Add button (+) at the bottom of the references section in the inspector, choose the.usdafile for the scene that contains the asset, then choose the asset you want to link to. After you do that, the selected entity or material becomes a copy of the one you linked to.\nImportant\nIf you make changes to a linked asset, those changes will affect every linked reference.\nPreview scenes on device\nIf you have an Apple Vision Pro connected to your Mac, choose Preview > Play or click the preview button in the Reality Composer Pro toolbar to view your scene on device. The Preview button is the left-most button on the right side of the toolbar — the one with an Apple Vision Pro icon. If you have multiple Apple Vision Pro devices connected, choose which device to use by clicking the pull-down menu next to the Preview button.\nLoad Reality Composer Pro scenes in RealityKit\nLoading a Reality Composer Pro scene is nearly identical to loading a USDZ asset from your app bundle, except you have to specify the Reality Composer Pro package bundle instead. You typically do this in themakeclosure of aRealityViewinitializer. Reality Composer Pro packages define a global constant that points to its bundle, which is named after the project with “Bundle” appended to it. In the default Xcode visionOS template, the Reality Composer Pro project is calledRealityKitContent, so the global bundle variable is calledrealityKitContentBundle:\nNote\nThe code above saves a reference to the root node. This isn’t required, but withRealityView, unlikeARViewon iOS and macOS, you don’t have ready access to the scene content, so it’s often handy to maintain your own reference to the root entity of your scene in your app’s data model.\nWhen RealityKit finishes loading the scene, thescenevariable contains the root entity of the scene you specified. Add it tocontentand RealityKit displays it to the user.\nSee Also\nRealityKit and Reality Composer Pro",
    "https://developer.apple.com/documentation/visionos/building_local_experiences_with_room_tracking": "visionOS\nBuilding local experiences with room tracking\nBuilding local experiences with room tracking\nBuilding local experiences with room tracking\nBuilding local experiences with room tracking\nOverview\nThis sample allows your app to keep track of rooms as discrete, identifiable places, and enables you to provide a customized virtual experience inside a specific room, and to get notified when someone enters or leaves the room. These customizations can be as simple as knowing when to stop room-specific animations, or to support the creation of location-specific virtual content such as in-game treasures, effects, or even portals to virtual worlds that contain other content.\nThis sample demonstrates how to use room tracking by enabling a person to place spheres in a space and continuously query the framework as to whether those spheres are in the same room as the person. As someone moves into, through, and out of the room, ARKit deliversRoomAnchorupdates that represent the latest knowledge of the current room. This structure provides acontains(_:)query method that you use to determine if the spheres are in the current room, and highlight them accordingly.\nThe app has anocclusion mode, in which the room geometry the framework renders is a transparent occluder that hides virtual objects outside the room. It also has awall selection mode, in which someone may select a specific wall for the purpose of replacing it with a video or virtual portal.\nNote\nThis app requires Xcode 16 and visionOS 2 or later, and an Apple Vision Pro. ARKit room tracking isn’t supported in Simulator.\nEnsure all data providers are in an authorized state\nYour app must request permission to use certain visionOS capabilities before being able to access data associated with them. For example, attempting to access theRoomTrackingProviderdisplays a permission sheet asking the user to authorize your app’s access. If the user has previously denied this request, the app displays an error message in the scene. For information about using aRoomTrackingProvider, seeSetting up access to ARKit data. For information about best practices for privacy, seeAdopting best practices for privacy and user preferences.\nNote\nTo use the room tracking capabilities in visionOS in your app, you need to provide theNSWorldSensingUsageDescriptionkey in your app’sInfo.plistalong with a description of why your app uses this feature. This sample already provides this key and description.\nConfigure room tracking\nSet up room tracking by first configuring anARKitSessioninstance, then add aWorldTrackingProviderand aRoomTrackingProviderto the session as shown in the following example:\nIn addition to instantiating the world and room tracking providers in theAppState, you need to create storage for the in-room anchors the app tracks:\nYou also need to create the materials the framework uses to render the in-room anchors:\nAllow a person to place room tracking anchors\nPlacing aroomAnchorobject in the room consists of two processes. The first phase allows the person to review the anchor, which the sample renders as a sphere in front of the device from the person’s perspective:\nThe second phase allows a person to place the sphere (aWorldAnchor) in their surroundings  with a tap gesture. Gestures such as this are SwiftUI view modifiers you apply to the room’sView, as shown below:\nAs a person places spheres in the room, they appear in green to indicate they’re anchors in the current room. If a person leaves the room, all of the room anchors in the previous room dim and become red to indicate a person has left the room. If there are anchors in the room a person enters into, they change color to indicate the person is currently in the room.\nThis changing state and the property of a room beingcurrentis what allows an app to make decisions about what actions, animations, or other processes make sense in a specific location.\nCheck the current room and respond to updates\nAs a person moves from room to room, ARKit’s room tracking process checks to see which room is current and reports back changes to the app through theRoomTrackingProviderpropertyanchorUpdates, which is an asynchronous sequence of all anchor updates. As these updates come in, aTaskview modifier in the app’sWorldAndRoomViewcalls a method that looks for anchors to update, as demonstrated here:\nFind and select walls\nRoom tracking also enables someone to find and select walls in the current room. You can use this as an additional interaction surface, such as creating a “portal” to another virtual space. The process of selecting a wall in a room is split into two modes: anunlocked modewhere actively looking at a specific wall causes ARKit to highlight it in blue, andlocked modewhere a person has selected a wall and it receives continuous updates from theRoomTrackingProvider. Theunlocked moderequires performing a ray cast query in the direction of the a person’s head, which returns the first wall that it hits, as shown here:\nKeep focus on the current room\nRoom tracking operates only in the current room a person is in. If someone leaves one room and enters another, the previous room is no longer valid, and the framework only updates mesh-room associations and plane-room associations for the current room. Only use the current room anchor and discard any noncurrent rooms.\nBe aware of limitations\nClutter in a room, large furniture elements, and very large spaces may interfere with ARKit’s ability to accurately detect walls and fully detect the dimensions of a room. In the case of very large indoor spaces, or in rooms with low-light conditions, the framework may only provide a floor mesh. Additionally, visionOS doesn’t support using room tracking outdoors or when Apple Vision Pro is in Travel Mode. In these cases, there’s no current room. For more information on implementing immersive experiences, see Human Interface Guidelines >Immersive experiences.\nWarning\nBe mindful of how much content you include in immersive scenes that use themixedstyle. Content that fills a significant portion of the screen, even if that content is partially transparent, can prevent the person from seeing potential hazards in their surroundings. If you want to immerse the person in your content, configure your space with thefullstyle. For more information, seeCreating fully immersive experiences in your app.\nSee Also\nARKit",
    "https://developer.apple.com/documentation/visionos/enabling-video-reflections-in-an-immersive-environment": "visionOS\nEnabling video reflections in an immersive environment\nEnabling video reflections in an immersive environment\nEnabling video reflections in an immersive environment\nEnabling video reflections in an immersive environment\nOverview\nRealityKit and Reality Composer Pro provide the tools to build immersive media viewing environments in visionOS. TheDestination Videosample uses these features to build a realistic custom environment called Studio. The environment adds to its realism and makes the video player feel grounded in the space by applying reflections of the player’s content onto the surfaces of the scene.\nRealityKit and Reality Composer Pro support two types of video reflections:\nSpecular reflections provide a direct reflection of the video content, and are typically useful to apply to glossy surfaces like metals and water.\nSpecular reflections provide a direct reflection of the video content, and are typically useful to apply to glossy surfaces like metals and water.\nDiffuse reflections provide a softer falloff of video content, and are useful to apply to rougher, more organic surfaces.\nDiffuse reflections provide a softer falloff of video content, and are useful to apply to rougher, more organic surfaces.\nThis article describes how to adopt reflections in your own environment, and shows how Destination Video’s Studio environment supports these effects to create a compelling media viewing experience.\nDefine a video docking location\nApps that useAVPlayerViewControllerto present video participate in system docking behavior. When you play a full-window video inside an immersive space, the system docks the video screen into a fixed location and presents streamlined playback controls. By default, the system determines the docking location for the scene, but starting in visionOS 2, you can customize this location by specifying a custom docking region.\nThe Studio environment defines a custom docking region that anchors the player to the walkway at the top of the staircase like shown below.\n\nTo create the docking region, the project defines aPlayerentity that contains aDockingRegionComponent. This component defines the bounding region for the player, which has a depth of 0 and uses a fixed 2.4:1 aspect ratio. You configure the docking region’s size through itswidthproperty, and you can optionally specify a preview video to display in the docking region’s space within Reality Composer Pro.\n\nTo provide an optimal viewing experience, the Studio environment minimizes objects between the viewer and the video. Additionally, it provides a comfortable viewing angle to avoid causing strain or discomfort during longer viewing sessions. Using Reality Composer Pro to define the docking region is a great way to visualize how it looks in context, but always review your environment on Apple Vision Pro to get a true sense of layout and scale.\nNote\nReality Composer Pro provides a template to set up a docking region and default video reflection configuration. You can access this template from the Insert menu by selecting Insert > Environment > Video Dock.\nDisplay specular video reflections\nSpecular reflections, like shown below, provide a direct reflection of the video’s content onto surrounding surfaces. You typically apply this type of reflection to glossy surfaces such as metals, mirrors, and water.\n\nTo enable this type of reflection, you define a material with theReflection Specular (RealityKit)node connected and apply it to a surface in your scene. The system automatically calculates the appropriate reflection based on your viewing angle relative to the docking region.\n\nThe output of this node contains the reflected color in the RGB channels, and a blend factor in the alpha channel, which you can use to composite the reflection with your existing material.\nDestination Video uses subtle specular reflections in its custom environment like shown below. Applying specular reflections helps to add depth and space to the experience. To learn more about how the environment uses specular reflections, open the Studio project in Reality Composer Pro to view its configuration.\n\nProvide diffuse video reflections\nDiffuse reflections provide a softer falloff of media content, which can be useful to apply to rougher, more organic surfaces like a concrete or wood floor. The image below shows a diffuse reflection from a video screen.\n\nYou enable diffuse reflections by adding a material on a surface with theReflection Diffuse (RealityKit)node connected.\n\nThis node requires the following inputs:\nThis UV samples the system-generated emitter texture that contains low-frequency light and color information from the docked video. The diffuse reflection node uses the UV to calculate where to show the soft light reflection.\nThis UV samples the provided attenuation mask texture. An attenuation texture contains a soft falloff mask that’s used to shape the light from the emitter. Use a higher bit-depth texture format, such as.exr,to reduce any possible banding artifacts.\nDestination Video’s custom environment applies diffuse reflections to the surfaces immediately surrounding the docked video screen as shown below:\n\nEnabling diffuse reflections enhances the level of immersion by making the video player feel grounded in the experience.\nTo calculate Emitter UVs, iterate over each vertex of the surface mesh, and sample a set number of random points on the docking region. The u-value and v-value of each of the random sample points on the docking region are weighted by measuring both the distance and the angle to the mesh vertex. The resulting emitter UV set is the average of the weighted docking region UV values.\nA visualization of the emitter UVs generated from the docking region.\nAn example that uses a debug texture to show how different colors from the docking region map on to the surface mesh.\nImportant\nThe number of random points sampled from the docking region can have a large impact on the overall computation time when generating emitter UVs. You can configure how many samples to use when calculating emitter UVs with theComputeDiffuseReflectionUVspython script.\nAttenuation UVs are a top-down projection of the attenuation texture onto the input geometry (UV-coordinate system). An attenuation texture contains a soft falloff mask that shapes the light from the emitter.\nA visualization of the attenuation UVs generated from the docking region.\nReality Composer Pro’s default attenuation texture on the visualization.\nThe attenuation texture contains a falloff pattern that shapes the the diffuse reflection on to the surface mesh. The image below shows the default Reality Composer Pro attenuation texture.\n\nThe default falloff pattern doesn’t extend all the way to the edges of the texture. In order to generate the attenuation UV set, calculate the edges of the falloff pattern from the texture. The image below shows the default falloff pattern in a standard UV-coordinate system, with the top-left point equal to(0,0)and the bottom-right point equal to(1,1).\n\nThe following four values define the attenuation UV set:\nThe UV-space value where the sharp line of the falloff pattern starts horizontally.\nThe UV-space value where the sharp line of the falloff pattern ends horizontally.\nThe UV-space value where the sharp line starts vertically.\nThe UV-space value where the falloff pattern ends in black.\nAfter calculating the attenuation texture, map it to the geometry. To visualize the attenuation texture mapping, the image below shows a square red mesh as the custom surface mesh that extends towards the user with sides that are equal to the width of the docking region.\n\nThe attenuation UVs are calculated from mapping the surface mesh, in world space, to the area defined by theuStart,uEnd,vStart, andvEndvalues, in the UV-coordinate space. The image below shows the surface mesh with the attenuation texture applied.\n\nNote\nWhen using theComputeDiffuseReflectionUVspython script for mapping using a custom attenuation texture, you only need to measure the theuStart,uEnd,vStart, andvEndvalues of your attenuation texture. If you’re using the default attenuation texture in Reality Composer Pro, the script uses the default values.\nTo learn more about how the environment sets up and applies diffuse reflections, open the Studio project in Reality Composer Pro to view its configuration.\nSee Also",
    "https://developer.apple.com/documentation/visionos/reducing-the-rendering-cost-of-your-ui-on-visionos": "visionOS\nReducing the rendering cost of your UI on visionOS\nReducing the rendering cost of your UI on visionOS\nReducing the rendering cost of your UI on visionOS\nReducing the rendering cost of your UI on visionOS\nOverview\nProvide an enjoyable experience and maintain a sense of immersion on Apple Vision Pro by minimizing visual choppiness and interaction latency. Performance bottlenecks that prevent timely rendering and responsive feedback interfere with the spatial experience overall and can cause disorientation or discomfort if they persist over an extended period of time. Your app’s processing and its views have an impact on the work the system does and its ability to meet rendering deadlines. To address bottlenecks in your SwiftUI or UIKit interfaces, first analyze your apps performance, then implement some of the following strategies to reduce CPU and GPU overhead. For more information on performance analysis, seeAnalyzing the performance of your visionOS app.\nMinimize transparency in overlapping views\nIf you have overlapping UI windows, avoid adding translucency to them. In SwiftUI, avoid setting the value of your view’sopacity(_:)below 1. In UIKit, avoid setting the value of your view’salphabelow 1. Subviews inheritalpha.\n\nWhen a view with transparent pixels overlaps another view, the system performs extra work to composite those views together. The GPU always renders foreground content, but when foreground content is transparent the GPU also renders the UI behind it. When the foreground content is fully opaque, the GPU doesn’t do this rendering.\nThe Translucent UI Meshes metric in the Core Animation section of the RealityKit Metrics instrument timeline helps you identify translucent UI content your app uses:\n\nOther visual effects that involveoverdraw, the need to draw pixels multiple times to produce a final result, also increase rendering work. In the Shared Space, overdraw can result from interactions with the content from other apps, so minimizing translucent content might have a greater impact. For design guidance, seeHuman Interface Guidelines > Windows. Visual effects can also cause offscreen passes. To reduce offscreen passes, seeReducing the rendering cost of your UI on visionOS.\nReduce the size of static UI views\nTo lower the GPU overhead of static UI content, reduce the size of UI elements in your view hierarchy. Making your UI content appear larger in a space requires rendering more pixels which requires more rendering work. People still have control over re-sizing content, so use sizes that are large enough for people to be comfortable interacting with, but not so large that they require unnecessary demand from the GPU.\nTo identify areas in your app where rendering your static UI content creates a lot of overhead for the GPU, check the RealityKit Metrics instrument for 3D Render GPU bottlenecks. Content you display in 2D windows still renders in a space with a 3D mesh. If you expect people to frequently launch multiple windows, account for the rendering cost of this in your design and profile your app with multiple windows open. For design guidance, seeHuman Interface Guidelines > Spatial layout.\nConsider the impact of dynamic content scaling\nThe system can provide sharper visuals from any angle and distance by altering the resolution of text or vector-based UI content based on where people are looking. Doing so requires drawing content more frequently and at potentially higher scales. SwiftUI and UIKit views enable this by default. To opt in with your custom Core Animation or Core Graphics rendering, enable thewantsDynamicContentScalingproperty of anyCALayerobject. Consider the performance and memory trade-offs and profile your app with this feature on and off to determine if the cost is worth the quality improvement.\nFor more information on dynamic content scaling, seeDrawing sharp layer-based content in visionOSand the section on Dynamic content scaling in the videoExplore rendering for spatial computing.\nReduce redraw and offscreen rendering\nThe performance cost of numerous redraws and offscreen render passes adds up. Offscreen passes can also contribute to additional overdraw. Some complex renderers require offscreen passes but you can avoid them in many cases. Learn more about offscreen render passes inCustomizing Render Pass Setup. To reduce the number of redraw and offscreen render passes your app performs:\nLower the update rates of animations.\nLower the update rates of animations.\nPause or stop animations.\nPause or stop animations.\nReduce the number of different views your app uses.\nReduce the number of different views your app uses.\nAvoid layouts that require redrawing overlapping layers when redrawing a different layer.\nAvoid layouts that require redrawing overlapping layers when redrawing a different layer.\nUse more efficient alternatives thanUIVisualEffectView. For example, use background colors in your SwiftUI and UIKit apps.\nUse more efficient alternatives thanUIVisualEffectView. For example, use background colors in your SwiftUI and UIKit apps.\nSimilar to other Apple platforms, app updates done in response to input events, timed animations, or other sources initiate UI redraw work in the render server. On visionOS, dynamic content scaling can cause redraw to be frequent in the absence of these updates. This makes it more important to reduce updates and animation timers.\nTo reduce the cost of redraws and render passes:\nAvoid large layers that might require redrawing a large number of pixels.\nAvoid large layers that might require redrawing a large number of pixels.\nMinimize your use of vector-based media content during active scrolling and animated interactions. Animating large vector-based media content is often expensive.\nMinimize your use of vector-based media content during active scrolling and animated interactions. Animating large vector-based media content is often expensive.\nReduce the resolution, type, and number of images in your content that scrolls or animates during interactions.\nReduce the resolution, type, and number of images in your content that scrolls or animates during interactions.\nThe number of offscreen render passes can negatively impact performance. Visual effects, like shadows, masking, rounded rectangles, blurs, and vibrancy increase the number of offscreen passes the system requires to render your content.\nTo identify frequent or expensive UI redraws, check the RealityKit Metrics instrument for Core Animation Encoding (CPU), Core Animation GPU (GPU), Core Animation Server Update (CPU), and Core Animation Client Commit (CPU) bottlenecks.\nMinimize view layouts and updates in your SwiftUI, UIKit, or custom Core Animation and Core Graphics rendering when you see a bottleneck related to UI redraw to make your app more efficient for the system to render.\n\nReview the Offscreen Prepares and Render Passes metrics in the Core Animation Render section of the RealityKit Metrics Instrument to identify the number of potential offscreen render passes and total render passes the render server does on behalf of your app. The section includes metrics on render passes for both offscreen visual effects and for each region of UI content to redraw.\nTo learn more about these offscreen passes, watch the videoDemystify and eliminate hitches in the render phase.\nAvoid expensive updates on the main thread\nTo avoid visible hitches and delays while people scroll and resize, reduce time consuming layout and view creations that occur on the app’s main thread. The SwiftUI Instrument template includes the View Body, View Properties, Core Animation Commits, Time Profiler, and Hangs instruments. Use the data these instruments collect to debug expensive commits and updates on your app’s main thread. The Core Animation Commits and Hangs instruments can help you locate areas of expensive work and updates on the app’s main thread that cause rendering delays:\n\nTo learn more about analyzing Hangs with Instruments, watchAnalyze hangs with Instruments.\nThe View Body and View Properties instruments provide metrics on the number of view body creations and property updates that occur:\n\nFor debugging purposes, you can add a call to the internal methodSelf._printChanges()from the body of the view to log information about the property that caused the view to update.\nSee Also",
    "https://developer.apple.com/documentation/visionos#Xcode-and-Simulator": "visionOS\nOverview\nvisionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.\n\nDeveloping for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app’s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS.\nExpand your app into immersive spaces\nStart with a familiar window-based experience to introduce people to your content. From there, addSwiftUIscene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences.\nBuild your app’s 3D content withRealityKitand Reality Composer Pro, and display it with aRealityView. In an immersive experience, useARKitto integrate your content with the person’s surroundings.\n\nExplore new kinds of interaction\nPeople can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures.SwiftUIprovides built-in support for these standard gestures, so rely on them for most of your app’s input. When you want to go beyond the standard gestures, useARKitto create custom gestures.\nTap to select\nPinch to rotate\nManipulate objects\nCreate custom gestures\nDive into featured sample apps\nExplore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash.\nTopics\nApp construction\nDesign\nSwiftUI\nRealityKit and Reality Composer Pro\nARKit\nVideo playback\nXcode and Simulator\nPerformance\niOS migration and compatibility\nEnterprise APIs for visionOS\nvisionOS\nOverview\nExpand your app into immersive spaces\nExplore new kinds of interaction\nDive into featured sample apps\nTopics",
    "https://developer.apple.com/documentation/visionos/implementing-object-tracking-in-your-visionos-app": "visionOS\nImplementing object tracking in your visionOS app\nImplementing object tracking in your visionOS app\nImplementing object tracking in your visionOS app\nImplementing object tracking in your visionOS app\nOverview\nWhen you implement object tracking in your visionOS app, you can seamlessly integrate real-world objects in people’s surroundings to enhance their immersive experiences. By tracking the 3D position and orientation of an object, or several objects, your app can augment them with virtual content.\nYou can use object tracking to provide virtual interactions with objects in a person’s surroundings, such as:\nGuiding someone through using an item’s features, reading about its history, or learning about its behaviors when they look at it in their surroundings.\nGuiding someone through using an item’s features, reading about its history, or learning about its behaviors when they look at it in their surroundings.\nHelping people troubleshoot issues with household items and appliances with a virtual manual.\nHelping people troubleshoot issues with household items and appliances with a virtual manual.\nCreating an immersive storytelling experience to make collectables and toys come to life.\nCreating an immersive storytelling experience to make collectables and toys come to life.\nTo integrate object tracking into your app, you start with a 3D model of a physical object, train a machine learning model in Create ML with that 3D model asset to obtain a reference object file, and then use the resulting reference object file to track the physical object in your app. The reference object file is a file format with a.referenceobjectextension, specifically for object tracking in visionOS.\n\nImplementing object tracking requires an Apple Vision Pro with visionOS 2 or later, and a Mac with Apple silicon and macOS 15 or later for the machine learning training in Create ML.\nEnsure your objects are suitable for object tracking\nObject tracking performs optimally for a specific set of object characteristics. For object tracking to work best in your app, make sure your object is rigid, nonsymmetrical, and stationary.\nSelect an object that maintains its shape and appearance during tracking. For example, a pair of scissors is challenging to track because it changes shape while a person uses it.\nSelect an object with a nonsymmetrical shape or texture, so that when you rotate the object, it doesn’t have the same appearance from different angles. For instance, a globe has a symmetrical shape, but has a nonsymmetrical texture on all sides, making it a suitable object. In contrast, a styrofoam cup has the same appearance on all sides when you rotate it, making it challenging to track.\nSelect an object that’s mostly stationary in a person’s surroundings. If you’re tracking a moving object, there can be a delay in following its position. For example, a pickleball racket constantly moves in different directions while a person plays with it, making it challenging to track.\nObtain a 3D model of your object\nYou useCreate MLto begin the machine learning training to obtain your reference object file. Create ML requires a 3D model asset in the USDZ file format that represents your real-world object. You can obtain your 3D model using computer-aided design (CAD) software to accurately model an object’s geometry and apply physically based rendering (PBR) materials to it, and save it in the USDZ file format. Using this method, the 3D model can realistically represent objects that consist of multiple parts made from different materials, like glass, metal, plastic, wood, and other common materials. This method is helpful for capturing objects that are entirely or partly transparent, shiny, or reflective. The better the 3D model represents the appearance of the physical object, the better the quality of tracking is in visionOS.\nAnother way to create your 3D model is by using the Object Capture feature in the Reality Composer app in iOS or iPadOS. You can use your iPhone or iPad to capture images of an object, and then save the USDZ file to import into your app. For more information about using the Object Capture feature to create a 3D model, seeMeet Object Capture for iOSandScanning objects using Object Capture.\nBefore beginning the training process in Create ML with the 3D model asset, keep the following guidelines in mind to ensure it works well for object tracking in visionOS:\nEnsure the 3D model is as photorealistic as possible — essentially a digital twin of your real-world object.\nEnsure the 3D model is as photorealistic as possible — essentially a digital twin of your real-world object.\nEnsure the scale of the 3D model is as precise as possible and matches its specified units. If the scale doesn’t match the real-world object, the augmentation appears offset in the viewing direction, and may appear either in front of or behind the object.\nEnsure the scale of the 3D model is as precise as possible and matches its specified units. If the scale doesn’t match the real-world object, the augmentation appears offset in the viewing direction, and may appear either in front of or behind the object.\nNote\nWhile training the machine learning model with the 3D model asset, Create ML ignores any animations, virtual cameras, and lights within the asset, treating them as static.\nTrain a machine learning model with the 3D model asset in Create ML\nObject tracking requires a reference object file to track the spatial location and orientation of the corresponding real-world object. You use Create ML to train a machine learning model to create a reference object file unique to your object. The training of machine learning models with your 3D asset and the creation of the reference object file both run locally on your Mac.\nTo set up the training:\nOpen Xcode and choose Xcode > Open Developer Tool > Create ML.\nOpen Xcode and choose Xcode > Open Developer Tool > Create ML.\nIn the dialog that appears, click New Document.\nIn the dialog that appears, click New Document.\nIn the Choose a Template dialog, select the Spatial category in the left pane, select the Object Tracking template, and click Next.\nIn the Choose a Template dialog, select the Spatial category in the left pane, select the Object Tracking template, and click Next.\nEnter your project name and other information, and click Next.\nEnter your project name and other information, and click Next.\nSelect a location for your project, and click Create.\nSelect a location for your project, and click Create.\nCreate ML opens a training configuration view with an empty 3D viewport. Drag the USDZ file of your 3D model asset into the 3D viewport.\nCreate ML opens a training configuration view with an empty 3D viewport. Drag the USDZ file of your 3D model asset into the 3D viewport.\nThe 3D viewport is an interactive space where you can view your 3D model asset from different angles. After it appears in the viewport, check the appearance of the 3D model asset and confirm that it matches the absolute dimensions of your real-world object. Also make sure that the dimensions of the 3D model asset at the bottom right of the viewport match the actual dimensions of your object. If the scale doesn’t match, one option is to use Reality Composer Pro to rescale the 3D model and then add the adjusted USDZ file to Create ML.\n\nThe next step is to select the best viewing angle for your real-world object. Consider how people view and interact with the object in your app, and decide which angle you need for tracking it. The “Viewing angles” setting appears below the 3D viewport, and has three viewing angles you can use: All Angles, Upright, or Front. It’s important to choose the best option for your object.\n\nThe All Angles option includes views from every angle. It works best for tracking objects that have a distinct and unique appearance from all sides, such as a patterned Christmas ornament that people see from all sides as it hangs on a tree.\nThe Upright option works only for tracking objects that stand upright on a surface, such as a microscope that sits on a counter and stays in the same position as people interact with it. This option disables tracking from the bottom viewing angle.\nThe Front option works only for tracking objects that stand upright on a surface where the back of the object isn’t visible, such as a coffee machine that sits on a counter while people operate it from the front. This option disables tracking from both the bottom and rear viewing angles.\nNote\nOnly choose the All Angles option if you want to track your object from all sides. The more restricted the viewing angle is, the more accurate the object tracking is in visionOS.\nIf there’s an object in a person’s surroundings that’s similar to the object you want to track, the object-tracking feature might recognize it and track it instead of your object. To prevent this from happening, add the similar object as a negative example when training the machine learning model with your reference object. Below the 3D viewport, choose More Options > Objects to avoid. Use this section to add USDZ samples of similar items to ensure the machine learning model doesn’t identify them as the object you want to track.\n\nCreate ML supports training multiple machine learning models in the same object-tracking project. In the Model Sources section in the left pane, you can click the Add button (+) to add more 3D model assets to your Create ML project. Use this feature to track multiple objects in your app at the same time.\n\nNote\nYou can track up to 10 different reference objects simultaneously without an impact to performance.\nAfter inspecting your 3D model asset and configuring the training settings, click Train to begin the training process. A progress bar indicates the amount of time until the machine learning training is complete. The machine learning training can take a few hours, depending on the configuration of your Mac. A more advanced processor and additional RAM significantly improve the training time.\nExport the reference object file\nWhen training is complete, Create ML provides the reference object file for you to use in your app. Click the Output tab and save the resulting reference object file.\nThe reference object file contains the machine learning model you trained, packaged with the 3D model asset, in the USDZ file format. You can use the USDZ file for visualizing the tracking quality by rendering it as an overlay on the real-world object, and as a guide for adding immersive effects. The USDZ file may take up a lot of space in your app if your 3D model asset is large, so you can remove it from the reference object file if you need to optimize space.\nYou use the Reference Object Compiler in Xcode to remove the USDZ data from the reference object file during the build process. Select your project in Xcode, click the Build Settings tab, and enable the Strip USDZ Files from Reference Object option. This setting contains theREFERENCEOBJECT_STRIP_USDZbuild flag. The default setting of the flag isNo, so Xcode copies any reference object files you add to the project as-is unless you change the setting.\n\nIntegrate the reference object file into your app\nAfter you generate the reference object file, you can set up object tracking in your app using Reality Composer Pro, RealityKit, or ARKit. For more information about each of these methods, seeUsing a reference object with Reality Composer Pro,Using a reference object with RealityKit, andUsing a reference object with ARKit.\nNote\nObject tracking works only in anImmersiveSpacewithin Xcode. Attempting to use object tracking in a window or volume results in a silent failure.\nFor more information about object tracking, seeExplore object tracking for visionOS. For an example of using ARKit for object tracking, seeExploring object tracking with ARKit.\nTopics\nObject tracking within an app\nSee Also\nRealityKit and Reality Composer Pro",
    "https://developer.apple.com/documentation/visionos#Overview": "visionOS\nOverview\nvisionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.\n\nDeveloping for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app’s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS.\nExpand your app into immersive spaces\nStart with a familiar window-based experience to introduce people to your content. From there, addSwiftUIscene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences.\nBuild your app’s 3D content withRealityKitand Reality Composer Pro, and display it with aRealityView. In an immersive experience, useARKitto integrate your content with the person’s surroundings.\n\nExplore new kinds of interaction\nPeople can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures.SwiftUIprovides built-in support for these standard gestures, so rely on them for most of your app’s input. When you want to go beyond the standard gestures, useARKitto create custom gestures.\nTap to select\nPinch to rotate\nManipulate objects\nCreate custom gestures\nDive into featured sample apps\nExplore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash.\nTopics\nApp construction\nDesign\nSwiftUI\nRealityKit and Reality Composer Pro\nARKit\nVideo playback\nXcode and Simulator\nPerformance\niOS migration and compatibility\nEnterprise APIs for visionOS\nvisionOS\nOverview\nExpand your app into immersive spaces\nExplore new kinds of interaction\nDive into featured sample apps\nTopics",
    "https://developer.apple.com/documentation/visionos/positioning-and-sizing-windows": "visionOS\nPositioning and sizing windows\nPositioning and sizing windows\nPositioning and sizing windows\nPositioning and sizing windows\nOverview\nvisionOS and macOS enable people to move and resize windows. In some cases, your app can use scene modifiers to influence a window’s initial geometry on these platforms, as well as to specify the strategy that the system employs to place minimum and maximum size limitations on a window. This kind of configuration affects both windows and volumes, which are windows with thevolumetricwindow style.\nYour ability to configure window size and position is subject to the following constraints:\nThe system might be unable to fulfill your request. For example, if you specify a default size that’s outside the range of the window’s resizability, the system clamps the affected dimension to keep it in range.\nThe system might be unable to fulfill your request. For example, if you specify a default size that’s outside the range of the window’s resizability, the system clamps the affected dimension to keep it in range.\nAlthough you can change the window’s content, you can’t directly manipulate window position or size after the window appears. This ensures that people have full control over their workspace.\nAlthough you can change the window’s content, you can’t directly manipulate window position or size after the window appears. This ensures that people have full control over their workspace.\nDuring state restoration, the system restores windows to their previous position and size.\nDuring state restoration, the system restores windows to their previous position and size.\nNote\nWindows in iPadOS occupy the full screen, or share the screen with another window in Slide Over or Split View. You can’t programmatically affect window geometry on that platform.\nSpecify initial window position\nmacOS\nvisionOS\nIn macOS, the first time your app opens a window from a particular scene declaration, the system places the window at the center of the screen by default. For scene types that support multiple simultaneous windows, the system offsets each additional window by a small amount to avoid fully obscuring existing windows.\nYou can override the default placement of the first window in macOS by applying thedefaultPosition(_:)scene modifier to indicate where to place the window relative to the screen bounds. For example, you can request that the system place a new window in the bottom trailing corner of the screen.\nThe system aligns the point in the window that corresponds to the specifiedUnitPointwith the point in the screen that corresponds to the same unit point. You can use a built-in unit point, likebottomTrailingin the above example, or define a custom one.\nYou can also usedefaultWindowPlacement(_:)to place windows.\nIn visionOS, the system places new windows automatically depending on the situation:\nWhen someone first launches an app from the Home View, the system places the app’s window where they’re looking.\nWhen someone first launches an app from the Home View, the system places the app’s window where they’re looking.\nWhen a running app opens a new window, the system places the new window in front of one of the app’s existing windows, offsetting each additional window by a small amount to avoid fully obscuring existing windows.\nWhen a running app opens a new window, the system places the new window in front of one of the app’s existing windows, offsetting each additional window by a small amount to avoid fully obscuring existing windows.\nYou can override the default placement of the window by applying thedefaultWindowPlacement(_:)scene modifier to indicate where to place the window. For example, you can request that the system place a new window on the trailing edge of the existing window.\nUse one of theWindowPlacement.Positionenumerations to choose the position of the window.\nSpecify initial window size\nYou can indicate a default initial size for a new window that the system creates from aScenedeclaration by applying one of the default size scene modifiers, likedefaultSize(width:height:). For example, you can request that new windows that aWindowGroupgenerates occupy 600 points in the x-dimension and 400 points in the y-dimension.\nThe system might clamp the actual size of the window, depending on both the window’s content and resizability settings.\nSpecify window resizability\nBoth macOS and visionOS provide interface controls that enable people to resize windows within certain limits. For example, people can use the control that appears when they look at the corner of a visionOS window to resize a window on that platform.\nYou can specify how the system limits window resizability. The default resizability for all scenes isautomatic. With that strategy,Settingswindows use thecontentSizestrategy, where both the minimum and maximum window size match the respective minimum and maximum sizes of the content that the window contains. Other scene types usecontentMinSizeby default, which retains the minimum size restriction, but doesn’t limit the maximium size.\nYou can specify one of these resizability strategies explicitly by adding thewindowResizability(_:)scene modifier to a scene. For example, people can resize windows from the following window group to between 100 and 400 points in both dimensions because the frame modifier imposes those bounds on the content view:\nYou can take this even further and enforce a specific size for a window with content that has a fixed size.\nSpecify a volume size\nWhen you create a volume, which is a window with thevolumetricstyle, you can specify the volume’s size using one of the three-dimensional default size modifiers, likedefaultSize(width:height:depth:in:). The following code creates a volume that’s one meter on a side:\nThe volume maintains this size for its entire lifetime. People can’t change the size of a volume at runtime.\nAlthough you can specify a volume’s size in points, it’s typically better to use physical units, like the above code, which specifies a size in meters. This is because the system renders a volume with fixed scaling rather than dynamic scaling, unlike a regular window, which means the volume appears more like a physical object than a user interface. For information about the different kinds of scaling, seeSpatial layout.\nSee Also\nSwiftUI",
    "https://developer.apple.com/documentation/visionos/destination-video": "visionOS\nDestination Video\nDestination Video\nDestination Video\nDestination Video\nOverview\nDestination Video is a multiplatform video-playbackSwiftUIapp for iOS, iPadOS, macOS, tvOS, and visionOS. People get a familiar media-browsing experience navigating the libraryʼs content and playing videos they find interesting.\nThe sample uses theTabViewstructure in SwiftUI to create an immersive, full-screen browsing experience with rich navigation hierarchy. While the app shares many of its views across platforms, it leverages platform-specific features to create a playback experience native to each platform. For example, it uses the SwiftUI window and scene customization APIs to create a more engaging and natural experience in macOS. This sample also demonstrates how to useSwiftDatato persist app data in a SwiftUI app.\nIn visionOS, the sample demonstrates how to play video within an immersive environment configured withReality Composer Pro. It also uses theGroup Activitiesframework to enable shared viewing experiences.\nImplement tab navigation\nDestination Video uses tab navigation with thesidebarAdaptablestyle, which optimizes the content browsing experience for each platform. In iPadOS, theTabViewwithsidebarAdaptablestyle allows people to toggle between the sidebar and tab bar. The full-screen browsing experience of a tab bar brings content to the forefront while the sidebar allows for easy access to deeper navigation hierarchy.\niPadOS\niOS\nmacOS\ntvOS\nvisionOS\n\n\n\nTo implement tab navigation, first declare aTabViewwith an explicit selection value using theinit(selection:content:)initializer. Add tabs within aTabViewby initializingTabstructures. Destination Video uses theinit(_:systemImage:value:content:)initializer to create each tab, then groups tabs within aTabSectionto declare a secondary tab hierarchy in theTabView.\nYou can also enable customization by adding thetabViewCustomization(_:)modifier to theTabViewand thecustomizationID(_:)modifier to each tab. Customization in Destination Video allows people to drag tabs from the sidebar to the tab bar, hide nonessential tabs, and rearrange tabs in the sidebar.\nFor more information, seeEnhancing your app’s content with tab navigation.\nCustomize windows in macOS\nIn macOS, the app supports multiple windows including a main window that shows the video collections and a separate video player window. You can customize the appearance and function of each window to create a more engaging experience.\n\nThe main window displays the app content — collections of videos — in aTabViewnavigation presented as a sidebar. Because the app doesn’t contain any additional toolbar items and the sidebar provides a visual indication of where a person is in the navigation hierarchy, the toolbar isn’t needed and unnecessarily takes up space. This sample removes the toolbar title and background using thetoolbar(removing:)andtoolbarBackgroundVisibility(_:for:)modifiers. This creates a full-window browsing experience for Destination Video running in macOS.\nOther window customizations in Destination Video include extending a window’s drag region, participating in a window’s zoom action, and modifying a window’s state restoration behavior. For more information, seeCustomizing window styles and state-restoration behavior in macOS.\nDisplay horizontally scrollable cards in tvOS\nDestination Video presents video cards in a horizontally scrollable list in the Watch Now tab. When a person taps on a video card, the app navigates to a view that shows detailed information about the video. In tvOS, each card implements thecardbutton style. When a person hovers on a card, it fully scales and lifts up.\nThis sample prevents the scroll view from clipping its content when the card expands using thescrollClipDisabled(_:)modifier. Additionally, this sample provides a title for the list by placing theScrollViewwithin aSectioncontainer and passing the title into theinit(content:header:)initializer. This allows the title to also lift and move as the card expands and lifts upon when a person hovers on it.\nFor more information about displaying content in tvOS, seeCreating a tvOS media catalog app in SwiftUI.\nPresent an immersive space\nBuilding video playback apps for visionOS provides new opportunities to enhance the viewing experience beyond the bounds of the player window. To add a greater level of immersion, this sample presents an immersive space that displays a scene around a person as they watch the video.\n\nIt defines the immersive space in theDestinationVideoapp structure.\nTheImmersiveSpacepresents an instance ofImmersiveEnvironmentView, which maps a texture to the inside of a sphere that it displays around a person. The app presents it using the.progressiveimmersion style, which lets people change the amount of immersion they experience by turning the Digital Crown on the device.\nPlay video in a full-window player\nOne of the most exciting features of visionOS is its ability to play 3D video along with Spatial Audio, which adds a deeper level of immersion to the viewing experience. Playing 3D content in your app requires that you displayAVPlayerViewControllerfull window. When you present the player this way, the system automatically docks it into the ideal viewing position, and presents streamlined playback controls that keep the person’s focus on the content.\n\nNote\nIn iOS or tvOS, you typically present video in a full-screen presentation using thefullScreenCover(isPresented:onDismiss:content:)modifier. This API is available in visionOS; however, the recommended way to present the player for full-window playback is to set it as the root view of your app’s window group.\nDestination Video’sContentViewdisplays the app’s library by default. It observes changes to the player model’spresentationproperty, which indicates whether the app requests inline or full-window playback. When the presentation state changes tofullWindow, the view redraws the UI to display the player view in place of the library.\nWhen someone selects the Play Video button on the detail view, the app calls the player model’sloadVideo(_: presentation:)method requesting thefullWindowpresentation option.\nAfter the player model successfully loads the video content for playback, it updates itspresentationvalue tofullWindow, which causes the app to replaceDestinationTabswithPlayerView.\nTo dismiss the full-window player in visionOS, people tap the Back button in the player UI. To handle this action, the app’sPlayerViewControllerDelegatetype defines anAVPlayerViewControllerDelegateobject that handles the dismissal.\nWhen the delegate receives this call, it clears the media from the player model and resets the presentation state back to its default value, which results in the Destination Video app redisplaying theDestinationTabsview.\nConfigure the Spatial Audio experience\nMedia playback apps require common configuration of their capabilities and audio session. In addition to performing the steps outlined inConfiguring your app for media playback, Destination Video also adopts newAVAudioSessionAPI to customize a person’s Spatial Audio experience.\nAfter the app successfully loads a video for playback, it configures the Spatial Audio experience for the current presentation. For the inline player view, it sets the experience to a small, focused sound stage where the audio originates from the location of the view. When displaying a video full window, it sets the experience to a large, fully immersive sound stage.\nCustomize an environment using RealityKit and Reality Composer Pro\nIn visionOS, Destination Video provides a custom environment, called Studio.\nTo optimize the viewing experience in the Studio environment, this sample implements the following:\nCustomizes the docking location for the video player in a custom environment.\nEnhances the reflections of the video content on glossy surfaces in the surrounding environment.\nEnhances the reflections of the video content on organic surfaces in the surrounding environment.\nConfigures the virtual scene lighting.\nApplies reverb for enhanced audio immersion.\nIn visionOS, a person can select the environment in which they watch a video by tapping on the environment picker menu presented byAVPlayerViewController. The Studio environment has light and dark variants. This sample adds them to the list of environments that appear in the environment picker menu using theimmersiveEnvironmentPicker(content:)modifier.\nFor more information, seeBuilding an immersive media viewing experienceandEnabling video reflections in an immersive environment.\nProvide a shared viewing experience\nOne of the best ways to enhance your app’s playback experience is to make that experience shareable with others. You can use theAVFoundationand theGroup Activitiesframeworks to buildSharePlayexperiences that bring people together even when they can’t be in the same location.\nThe Destination Video app creates an experience where people can watch videos with others across devices and platforms. It defines a group activity calledWatchingActivitythat adopts theGroupActivityprotocol.  When people have a FaceTime call active and they play a video in the full-window player, it becomes eligible for playback for everyone on the call.\nThe app’sWatchingCoordinatoractor manages Destination Video’s SharePlay functionality. It observes the activation of newWatchingActivitysessions. After aWatchingActivitysession starts, theWatchingCoordinatorsets theGroupSessioninstance on the player object’sAVPlaybackCoordinator.\nWith the player configured to use the group session, when the app loads new videos, they become eligible to share with people in the FaceTime call.\nSee Also",
    "https://developer.apple.com/documentation/visionos/understanding-transforms": "visionOS\nUsing transforms to move, scale, and rotate entities\nUsing transforms to move, scale, and rotate entities\nUsing transforms to move, scale, and rotate entities\nUsing transforms to move, scale, and rotate entities\nOverview\nRealityKitEntityobjects exist in a tree, and each entity can have any number of subentities. (The entities themselves can standalone, or can be in a single container.) Every entity in the tree stores its own transform component. The transform contains thetranslation,scale, andorientationrelative to its container entity. Therootof each tree is an entity without a container entity.\nEach entity exists in its own coordinate system that defines the origin and orientation of the three ordinal directions (the x, y, and z axes). The coordinate system is relative to its container coordinate system and is defined by its transform.\nArrange entities with transforms\nA root entity has no parent entity. Its location in the scene is either controlled by SwiftUI or placed via aSpatialTrackingSession. SwiftUI provides a root entity for the volume defined by theRealityView. The root entity defines the root coordinate system.\nNote\nIn addition to a spatial tracking session, apps can use anARSessionwith any number of data providers. The available list can be found in the typeDataProvider, and a full list of anchor types is found inAnchoringComponent.Target.\nEach entity added to the tree adds a new coordinate system defined by itstransformand is relative to its container entity. Each of the coordinate systems relate to each other by the hierarchy of entities and their transforms. For example a hierarchy of entities built with this code:\nThis reality view has three entitiesB,A, and the root entity. These three entities form a tree, with one root entity at the center of the reality view’s volume. Entity A is a subentity of the root, and B is a subentity of A.\n\nThe reality view provides the root entity, which is located at the center of a volumetric window or near the floor in an immersive space. Use theadd(_:Entity)method on thecontentsupplied by the reality view to add entities as subentites of that root entity. The coordinate system defined byBis0.1units along thex-axisof the coordinate system defined byA. The coordinate system defined byAis0.05units along thex-axisdefined by the root. With three entities there are three coordinate systems.\n\nIn this example there are two cubes. Each cube has eight corners, and each corner is0.025units away from the origin. The cubes appear in different locations in the scene because the system applies thetransformto each corner of the cubes moving them from the local coordinate system (also calledmodel space) to the world coordinate system. For example, the top, right, forward corner of the cube is at{0.025, 0.025, 0.025}inmodel space. The entity is translated by{0.05, 0.0, 0.0}The top-right-forward corner is then at{0.075, 0.025, 0.025}.\nBuild a simple entity to experiment with\nTo be visible, an entity must have aMeshDescriptorand aMaterial. AMeshDescriptorcontains the description of a mesh. In this case, the mesh contains all of the vertices and how they connect into triangles. A Material specifies the color and appearance of the entity.\nThe previous example usedgenerateBox(size:)to generate the mesh. This convenience obscures what the transform does. The remaining examples use a mesh built from scratch.\nAll entities have a coordinate space, often calledmodel space. This coordinate system determines the location of thevertices.\nThe code below builds an entity with the following properties:\nThe entity has one material.\nThe entity has one material.\nThe mesh has 8 vertices and 12 triangles. Each vertex is one corner of the cube. Each triangle is one half of each side and involves three of the vertices. There are six sides, with two triangles each, for a total of 12 triangles. These vertices are in the model space coordinate system. This function builds the entity.\nThe mesh has 8 vertices and 12 triangles. Each vertex is one corner of the cube. Each triangle is one half of each side and involves three of the vertices. There are six sides, with two triangles each, for a total of 12 triangles. These vertices are in the model space coordinate system. This function builds the entity.\nNote\nFor more information about constructing meshes, seeMeshDescriptor. When you set thetransformon an entity, the system transforms the mesh vertices to the new coordinate system.\nAdd the cube entity to a reality view\nYou add thecubeentity to the volumetric window via SwiftUI like this:\nThe cube entity appears at the center of the volume, the origin of the volume’s root entity.\nMove the cube with a transform\nTo move the cube use theTransformcomponent with thetranslationargument:\nApplying thisTransformto the cube moves all eight vertices in thexdirection by0.1.\nThe system moves the entity from its ‘model space’ origin to the location in world space. To achieve that effect, RealityKit performs some linear algebra behind the scenes to ‘transform’ the points into world space. The left matrix is theTransformconverted to a matrix. The right vertex is the list of vertices. Here is the full multiplication for the transform and the first vertex.\n\n\nThe left matrix is a direct representation of theTransformyou made earlier and applied to thecube. The right matrix is the first vertex from the cube represented by a vector with1.0in the last position. Performing that multiplication (thedot(_:_:)product of each row of the matrix with the vertex) yields:\n\nThe net effect is that the new vertices have0.1added to theirxcomponent. This approach generalizes to all other forms of transformation that you use to manipulate entities in RealityKit:\nScale the cube with a transform\nTo scale an entity use thetransformproperty. To apply a uniform scale of 2 to the entity change the code, like this:\nThat yields a matrix multiplication that looks like this:\n\nAfter the multiplication yields a transformed vertex like this:\n\nAfter multiplying all of the vertices by the scale matrix, the cube is twice as large in each direction (0.2versus0.1):\nCombine transforms\nThese two operations combine into a single operation with theTransformtype like this:\nThat transform yields a matrix multiplication for all the vertices, laid out as column vectors. The multiplication looks like this:\n\nThe order is important: scale first then translate.\nImportant\nMultiplying matrixes isn’t commutative, which means thatA*Bis not equal toB*A.\nMultiplying these two transformation matrices in the order shown above yields this result:\n\nThe result scales the model by2uniformly and translates the model by0.1in thexdirection:\nSwitching that order yields a different matrix:\n\nThis resulting matrix yields a similar uniform scale of2, but the translation is scaled by2as well. The net result of this matrix is to scale the model uniformly by2and move it in the positivexdirection by0.2:\nImportant\nMatrix multiplication is associative, which means that you can move the parenthesis around. SoA*B*Ccan be done asA*  (B*C)  or (A*B) *C. This allows thetransformto be one matrix application instead of two.\nMultiply thescalematrix by thetranslationmatrix to get the combinedtransformmatrix. RealityKit then applies the combined matrix to the vertices:\n\nWhich yields a matrix like this:\n\nThe scaled and translated vertices yield a cube that is twice as large in each direction and moved0.1units to the right.\nRotate entities\nTo rotate the cube 45° (π/4 radians), use theTransformtype with therotationargument like this:\nThis causes the cube to rotate 45° around its origin along thex-axis. The matrix for this rotation looks like this:\nRotation around any other axis is achieved in the same way. For example, to rotate 45° around the axis through the top-right corner of the cube you could use:\nImportant\nThenormalize(_:)function returns a vector pointing in the same direction with a length of 1.0. Make sure to normalize theaxisargument when creating quaternions.\nThat code performs a rotation that looks like this:\nApplying this transformation matrix to the full set of vertices yields this new set of transformed vertices:\n\nNotice that the fourth and sixth vertex didn’t change. The axis of rotation goes through those two vertices so nothing changes on that axis.\nCombine rotation, translation, and scale in one transform\nRotation combined with other transforms might yield unexpected results depending on the order of the application. You can combine all three transformations in theTransforminitializer like this:\nThe order of these transforms istranslationfollowed byrotationthenscale.\nSee Also\nRealityKit and Reality Composer Pro",
    "https://developer.apple.com/documentation/visionos/drawing-sharp-layer-based-content": "visionOS\nDrawing sharp layer-based content in visionOS\nDrawing sharp layer-based content in visionOS\nDrawing sharp layer-based content in visionOS\nDrawing sharp layer-based content in visionOS\nOverview\nIf your app uses Core Animation layers directly, update your layer code to draw a high-resolution version of your content when appropriate. SwiftUI and UIKit views use Core Animation layers to manage interface content efficiently. When a view draws its content, the underlying layer captures that content and caches it to improve subsequent render operations.\nCore Animation on most Apple platforms rasterizes your layer at the same resolution as the screen, but Core Animation on visionOS can rasterize at different resolutions to maximize both content clarity and performance. The system follows the person’s eyes and renders content immediately in front of them at the highest possible resolution. Outside of this focal area, the system renders content at progressively lower resolutions to reduce GPU workloads. Because the content is in the person’s peripheral vision, these lower resolutions don’t impact the content’s clarity. As the person’s eyes move around, the system redraws content at different resolutions to match the change in focus.\nA figure at 2x resolution\nA figure at 8x resolution\nIf you deliver content using customCALayerobjects, you can configure your custom layers to support drawing at different resolutions. If you don’t perform this extra configuration step, each layer rasterizes its content at a @2x scale factor, which is good enough for most content and matches what the layer provides on a Retina display. However, if you opt in to drawing at different resolutions, the layer rasterizes its content at up to @8x scale factor in visionOS, which adds significant detail to text and vector-based content.\nRequest dynamic scaling for custom layers\nDynamic content scaling is off by default for all Core Animation layers, and frameworks or apps must turn on this support explicitly. If your interface uses only SwiftUI or UIKit views, you don’t need to do anything to support this feature. SwiftUI and UIKit enable it automatically for views that benefit from the added detail, such as text views and image views with SF Symbols or other vector-based artwork. However, the frameworks don’t enable the feature for all views, includingUIViewandView.\nIf your visionOS interface includes custom Core Animation layers, you can enable thewantsDynamicContentScalingproperty of anyCALayerobjects that contain vector-based content. Setting this property totruetells the system that you support rendering your layer’s content at different resolutions. However, the setting is not a guarantee that the system applies dynamic content scaling to your content. The system can disable the feature if your layer draws using incompatible functions or techniques.\nThe following example shows how to enable this feature for aCATextLayerobject. After configuring the layer, set thewantsDynamicContentScalingproperty totrueand add the layer to your layer hierarchy.\nDynamic content scaling works best when the layer contains text or vector-based content. Don’t enable the feature if you do any of the following in your layer:\nYou set the layer’s content using thecontentsproperty.\nYou set the layer’s content using thecontentsproperty.\nYou draw primarily bitmap-based content.\nYou draw primarily bitmap-based content.\nYou redraw your layer’s contents repeatedly over a short time period.\nYou redraw your layer’s contents repeatedly over a short time period.\nTheCAShapeLayerclass ignores the value of thewantsDynamicContentScalingproperty and always enables dynamic content scaling. For other Core Animation layers, you must enable the feature explicitly to take advantage of it.\nDraw the layer’s content dynamically\nDynamic content scaling requires you to draw your layer’s contents using one of the prescribed methods. If you define a custom subclass ofCALayer, draw your layer’s content in thedraw(in:)method. If you use aCALayerDelegateobject to draw the layer’s content, use the delgate’sdraw(_:in:)method instead.\nWhen you enable dynamic content scaling for a layer, the system captures your app’s drawing commands for playback later. As the person’s eyes move, the system draws the layer at higher resolutions when someone looks directly at it, or at lower resolutions otherwise. Because the redraw operations implicitly communicate what the person is looking at, the system performs them outside of your app’s process. Letting the system handle these operations maintains the person’s privacy while still giving your app the benefits of high-resolution drawing.\nSome Core Graphics routines are incompatible with dynamic content scaling. Even if you enable dynamic content scaling for your layer, the system automatically disables the feature if your layer uses any of the following:\nCore Graphics shaders.\nCore Graphics shaders.\nAPIs that set intent, quality, or other bitmap-related properties. For example, don’t callCGContextSetInterpolationQuality.\nAPIs that set intent, quality, or other bitmap-related properties. For example, don’t callCGContextSetInterpolationQuality.\nACGBitmapContextto draw content.\nACGBitmapContextto draw content.\nIf your app creates timer-based animations, don’t animate layer changes using your drawing method. CallingsetNeedsDisplay()on your layer repeatedly in a short time causes the system to draw the layer multiple times in quick succession. Because visionOS needs a little extra time to draw a layer at high resolution, each redraw request forces it to throw away work. A better option is to animate layer-based properties to achieve the same effect, or use aCAShapeLayerto animate paths when needed.\nModify layer hierarchies to improve performance\nThe backing store for a layer consumes more memory at higher resolutions than at lower resolutions. Measure your app’s memory usage before and after you enable dynamic content scaling to make sure the increased memory cost is worth the benefit. If your app’s memory usage increases too much, limit which layers adopt dynamic content scaling. You can also reduce the amount of memory each layer uses in the following ways:\nMake your layer the smallest size possible. Larger layers require significantly more memory, especially at higher resolutions. Make the size of the layer match the size of your content by eliminating padding or extra space.\nMake your layer the smallest size possible. Larger layers require significantly more memory, especially at higher resolutions. Make the size of the layer match the size of your content by eliminating padding or extra space.\nSeparate complex content into different layers. Instead of drawing everything in a single layer, build your content from multiple layers and arrange them hierarchically to achieve the same result. Enable dynamic content scaling only in the layers that actually need it.\nSeparate complex content into different layers. Instead of drawing everything in a single layer, build your content from multiple layers and arrange them hierarchically to achieve the same result. Enable dynamic content scaling only in the layers that actually need it.\nApply special effects using layer properties whenever possible. Applying effects during drawing might require you to increase the layer’s size. For example, apply scale and rotation effects to the layer’stransformproperty, instead of during drawing.\nApply special effects using layer properties whenever possible. Applying effects during drawing might require you to increase the layer’s size. For example, apply scale and rotation effects to the layer’stransformproperty, instead of during drawing.\nDon’t draw your layer’s content at different resolutions in advance and cache the images. Maintaining multiple images requires more memory. If you do cache images, draw them only at @2x scale factor.\nDon’t draw your layer’s content at different resolutions in advance and cache the images. Maintaining multiple images requires more memory. If you do cache images, draw them only at @2x scale factor.\nDon’t use your drawing code to draw a single image. If your layer’s content consists of an image, assign that image to the layer’scontentsproperty directly.\nDon’t use your drawing code to draw a single image. If your layer’s content consists of an image, assign that image to the layer’scontentsproperty directly.\nComplex drawing code can also lead to performance issues. A layer with many strokes can render quickly at lower scale factors, but might be computationally too complex to render at larger scales. If a complex layer doesn’t render correctly at higher resolutions, turn off dynamic content scaling and measure the render times again.\nSee Also\nApp construction",
    "https://developer.apple.com/documentation/visionos#topics": "visionOS\nOverview\nvisionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.\n\nDeveloping for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app’s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS.\nExpand your app into immersive spaces\nStart with a familiar window-based experience to introduce people to your content. From there, addSwiftUIscene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences.\nBuild your app’s 3D content withRealityKitand Reality Composer Pro, and display it with aRealityView. In an immersive experience, useARKitto integrate your content with the person’s surroundings.\n\nExplore new kinds of interaction\nPeople can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures.SwiftUIprovides built-in support for these standard gestures, so rely on them for most of your app’s input. When you want to go beyond the standard gestures, useARKitto create custom gestures.\nTap to select\nPinch to rotate\nManipulate objects\nCreate custom gestures\nDive into featured sample apps\nExplore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash.\nTopics\nApp construction\nDesign\nSwiftUI\nRealityKit and Reality Composer Pro\nARKit\nVideo playback\nXcode and Simulator\nPerformance\niOS migration and compatibility\nEnterprise APIs for visionOS\nvisionOS\nOverview\nExpand your app into immersive spaces\nExplore new kinds of interaction\nDive into featured sample apps\nTopics",
    "https://developer.apple.com/documentation/visionos/displaying-video-from-connected-devices": "visionOS\nDisplaying video from connected devices\nDisplaying video from connected devices\nDisplaying video from connected devices\nDisplaying video from connected devices\nOverview\nApple’s audiovisual frameworks allow your visionOS app to access video from USB video class (UVC) devices connected with theDeveloper Strapfor Apple Vision Pro. You can use this functionality to display realtime video in your app. For example, a medical researcher can view the output from an endoscopic camera during a procedure. This article outlines the requirements to access UVC devices in visionOS, while the sample code project shows a picker for every device connected to Vision Pro and displays the selected device’s video feed.\nConfigure the sample code project\nIn the Xcode project, replaceEnterprise.licensewith your license file. The sample app requires a valid license file to display the selected video feed.\nRequest the entitlement\nUVC device access is a part of enterprise APIs for visionOS, a collection of APIs that unlock capabilities for enterprise customers. To use UVC device access, apply for theUVC Device Access on visionOSentitlement. For more information, including how to apply for this entitlement, seeBuilding spatial experiences for business apps with enterprise APIs for visionOS.\nAdd usage descriptions for camera access\nTo help protect people’s privacy, visionOS limits app access to cameras and other sensors in Apple Vision Pro. You need to add anNSCameraUsageDescriptionto your app’s information property list to provide a usage description that explains how your app uses the data these sensors provide. People see this description when your app prompts for access to camera data.\nCreate the device picker\nUse anAVCaptureDevice.DiscoverySessionobtain an array of connected devices.\nNext, observewasConnectedNotificationandwasDisconnectedNotificationto update the array when a device connects or disconnects.\nRender a picker with an option for each device:\nDisplay the selected device’s video feed\nConfigure anAVCaptureSessionto captureAVCaptureDeviceInputfrom the selected device and output it to anAVCaptureVideoDataOutput.\nCallstartRunning()on the capture session to start the flow of data from the capture session’s inputs to its outputs.\nAVCaptureSessiondelivers a steady stream of updates to theAVCaptureVideoDataOutputSampleBufferDelegateassigned to theAVCaptureVideoDataOutput. Each update includes aCMSampleBufferthat contains the latest video frame from the device. Render theCMSampleBufferto anAVSampleBufferDisplayLayerusing the layer’sAVSampleBufferVideoRenderer.\nAdd theAVSampleBufferDisplayLayerto aUIViewand use aUIViewRepresentableto display theUIViewin a SwiftUI view.\nDisplay an error when denying access to the camera\nIf the person hasn’t granted camera access, the sample app prompts people to grant access in the Settings app. For more information about providing camera access in your app, seeRequesting authorization to capture and save media.\nSee Also\nEnterprise APIs for visionOS",
    "https://developer.apple.com/documentation/visionos#Dive-into-featured-sample-apps": "visionOS\nOverview\nvisionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.\n\nDeveloping for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app’s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS.\nExpand your app into immersive spaces\nStart with a familiar window-based experience to introduce people to your content. From there, addSwiftUIscene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences.\nBuild your app’s 3D content withRealityKitand Reality Composer Pro, and display it with aRealityView. In an immersive experience, useARKitto integrate your content with the person’s surroundings.\n\nExplore new kinds of interaction\nPeople can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures.SwiftUIprovides built-in support for these standard gestures, so rely on them for most of your app’s input. When you want to go beyond the standard gestures, useARKitto create custom gestures.\nTap to select\nPinch to rotate\nManipulate objects\nCreate custom gestures\nDive into featured sample apps\nExplore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash.\nTopics\nApp construction\nDesign\nSwiftUI\nRealityKit and Reality Composer Pro\nARKit\nVideo playback\nXcode and Simulator\nPerformance\niOS migration and compatibility\nEnterprise APIs for visionOS\nvisionOS\nOverview\nExpand your app into immersive spaces\nExplore new kinds of interaction\nDive into featured sample apps\nTopics",
    "https://developer.apple.com/documentation/visionos/understanding-the-realitykit-modular-architecture": "visionOS\nUnderstanding the modular architecture of RealityKit\nUnderstanding the modular architecture of RealityKit\nUnderstanding the modular architecture of RealityKit\nUnderstanding the modular architecture of RealityKit\nOverview\nRealityKit is a 3D framework designed for building apps, games, and other immersive experiences. Although it’s built in an object-oriented language and uses object-oriented design principles, the architecture of RealityKit avoids heavy use of composition — where objects are built by adding instance variables that hold references to other objects — in favor of a modular design based on a paradigm called Entity Component System (ECS) that divides application objects into one of three types.\nFollowing the ECS paradigm allows you to re-use the functionality contained in a component in many different entities, even if they have very different inheritance chains. Even if two objects have no common ancestors other thanEntity, you can add the same components to both of them and give them the same behavior or functionality.\nStart with entities\nEntities are the core actors of RealityKit. Any object that you can put into a scene, whether visible or not, is an entity and must be a descendent ofEntity. Entities can be 3D models, shape primitives, lights, or even invisible items like sound emitters or trigger volumes. Add components to entities to let them store additional state relevant to a specific type of functionality. Entities themselves contain relatively few properties: Nearly all entity state is stored on an entity’s components.\nRealityKit provides a number of entity types you use to represent different kinds of objects. For example, aModelEntityrepresents a 3D model, such as one imported from a.usdzor.realityfile. These provided entities are essentially just anEntitywith certain components already added to them. Adding aModelComponentto an instance ofEntity, for example, results in an entity with identical functionality to aModelEntity.\nAdd components to entities\nComponents are modular building blocks that you add to an entity; they identify which entities a system will act on, and maintain the per-entity state that systems rely on. Components can contain logic, but limit component logic to code that validates its property values or sets its initial state. Use systems for any logic that affects the behavior of entities or that potentially changes their state on every frame. To add accessibility information to an entity, for example, add aAccessibilityComponentto it and populate its fields with the information the accessibility system needs, such as putting the description that VoiceOver reads into itslabelproperty.\nKeep in mind that an entity can only hold one copy of any particular type of component at a time. So, for example, you can’t add two accessibility components to one entity. If you add an accessibility component to an entity that already has one, the new component replaces the previous one.\nCreate systems to implement entity behavior\nASystemcontains code that RealityKit calls on every frame to implement a specific type of entity behavior or to update a particular type of entity state. Systems use components to store their entity-specific state and query for entities to act on by looking for ones with a specific component or combination of components.\nFor example, a game might have a damage system that monitors and updates the health of every entity that can be damaged or destroyed. Systems typically work together with one or more components, so that damage system might use a health component to keep track of how much damage each entity has taken and how much each one is able to take before it’s destroyed. It might also interact with other components. For example, an entity might have an armor component that provides protection to the entity, and the damage system would also need to use the state stored in that component.\nEvery frame, the damage system queries for entities that have the health component and updates values on those entities’ components based on the current state of the app. If an entity has taken too much damage, the system might trigger a specific animation or remove the entity from the scene.\nWriting entity logic in a system avoids duplication of work. Using traditional OOP design patterns, where this type of logic would reside on the entity class, can often result in the same calculations being performed multiple times, once for every entity potentially affected. No matter how many entities the calculation potentially impacts the system only has to do the calculation once.\nFor more information on creating systems, seeImplementing systems for entities in a scene\nSee Also\nRealityKit and Reality Composer Pro",
    "https://developer.apple.com/documentation/visionos/bringing-your-arkit-app-to-visionos": "visionOS\nBringing your ARKit app to visionOS\nBringing your ARKit app to visionOS\nBringing your ARKit app to visionOS\nBringing your ARKit app to visionOS\nOverview\nIf you use ARKit to create an augmented reality experience on iPhone or iPad, you need to rethink your use of that technology when bringing your app to visionOS. ARKit plays a crucial role in delivering your content to the display in iPadOS and iOS. In visionOS, you use ARKit only to acquire data about the person’s surroundings, and you do so using a different set of APIs.\nIn visionOS, you don’t need a special view to display an augmented reality interface. Build windows with your app’s content using SwiftUI or UIKit. When you display those windows, visionOS places them in the person’s surroundings for you. If you want to control the placement of any 2D or 3D content in the person’s surroundings, build your content using SwiftUI and RealityKit.\nWhen migrating your app to visionOS, reuse as much of your app’s existing content as you can. visionOS supports most of the same technologies as iOS, so you can reuse project assets, 3D models, and most custom views. Don’t reuse your app’s ARKit code or any code that relies on technologies visionOS doesn’t support.\nFor general guidance on how to port apps to visionOS, seeBringing your existing apps to visionOS.\nAdopt technologies available in both iOS and visionOS\nTo create a single app that runs in both iOS and visionOS, use technologies that are available on both platforms. While ARKit in iOS lets you create your interface using several different technologies, the preferred technologies in visionOS are SwiftUI and RealityKit. If you’re not currently using RealityKit for 3D content, consider switching to it before you start adding visionOS support. If you retain code that uses older technologies in your iOS app, you might need to re-create much of that code using RealityKit when migrating to visionOS.\nIf you useMetalto draw your app’s content, you can bring your code to visionOS to create content for 2D views or to create fully immersive experiences. You can’t use Metal to create 3D content that integrates with the person’s surroundings. This restriction prevents apps from sampling pixels of the person’s surroundings, which might contain sensitive information. For information on how to create a fully immersive experience with Metal, seeDrawing fully immersive content using Metal.\nNote\nIn visionOS 2.0 and later, you can create a mixed immersive experience with Metal as well. WatchRender Metal with passthrough in visionOSfor more information and sample code.\nConvert 3D assets to the USDZ format\nThe recommended format for 3D assets in iOS and visionOS is USDZ. This format offers a compact single file for everything, including your models, textures, behaviors, physics, anchoring, and more. If you have assets that don’t use this format, use the Reality Converter tool that comes with Xcode to convert them for your project.\nWhen building 3D scenes for visionOS, use Reality Composer Pro to create your scenes that incorporate your USDZ assets. With Reality Composer Pro, you can import your USD files and edit them in place, nondestructively. If your iOS app applies custom materials to your assets, convert those materials to shader graphs in the app.\nAlthough you can bring models and materials to your project using USDZ files, you can’t bring custom shaders you wrote using Metal. Replace any custom shader code with MaterialX shaders. Many digital content creation tools support the MaterialX standard, and let you create dynamic shaders and save them with your USDZ files. Reality Composer Pro and RealityKit support MaterialX shaders, and incorporate them with your other USDZ asset content. See theMaterialXsite for more information.\nUpdate your interface to support visionOS\nIn visionOS, you manage your app’s content, and the system handles the integration of that content with the person’s surroundings. This approach differs from iOS, where you use a special ARKit view to blend your content and the live camera content. Bringing your interface to visionOS therefore means you need to remove this special ARKit view and focus only on your content.\nIf you can display your app’s content using SwiftUI or UIKit views, build a window with those views and present it from your visionOS app. If you use other technologies to incorporate 2D or 3D content into the person’s surroundings, make the following substitutions in the visionOS version of your app.\nIf you create your AR experience using:\nUpdate to:\nRealityKit andARView\nRealityKitandRealityView\nSceneKit andARSCNView\nRealityKitandRealityView\nSpriteKit andARSKView\nRealityKitorSwiftUI\nARealityViewis a SwiftUI view that manages the content and animations you create using RealityKit and Reality Composer Pro. You can add aRealityViewto any of your app’s windows to display 2D or 3D content. You can also add the view to anImmersiveSpacescene, which you use to integrate your RealityKit content into the person’s surroundings.\nNote\nYou can load iOS storyboards into a visionOS app, but you can’t customize your interface for visionOS or include 3D content. If you want to share interface files between iOS and visionOS, adopt SwiftUI views or create your interface programmatically.\nFor more information about how to useRealityViewand respond to interactions with your content, seeAdding 3D content to your app.\nReplace your ARKit code\nARKit provides different APIs for iOS and visionOS, and the way you use ARKit services on the platforms is also different. In iOS, you must use ARKit to put your content onscreen, and you can also use it to manage interactions between your content and a person’s surroundings. In visionOS, the system puts your content onscreen, so you only use ARKit to manage interactions with the surroundings. Because of this more limited usage, some apps don’t need ARKit at all in visionOS.\nThe only time you use ARKit in visionOS is when you need one of the following services:\nPlane detection\nPlane detection\nImage tracking\nImage tracking\nScene reconstruction\nScene reconstruction\nHand tracking\nHand tracking\nWorld tracking and device-pose prediction\nWorld tracking and device-pose prediction\nUse plane detection, image tracking, and scene reconstruction to facilitate interactions between your app’s virtual content and real-world items. For example, use plane detection to detect a tabletop on which to place your content. Use world tracking to record anchors that you want to persist between launches of your app. Use hand tracking if your app requires custom hand-based input.\nImportant\nIn visionOS 2.0 and later, useSpatialTrackingSessionfor available AR data instead.\nTo start ARKit services in your app, create anARKitSessionobject and run it with the data providers for each service. Unlike ARKit in iOS, services in visionOS are independent of one another, and you can start and stop each one at any time. The following example shows how to detect horizontal and vertical planes. Data providers deliver new information using an asynchronous sequence.\nIf you use the world-tracking data provider in visionOS, ARKit automatically persists the anchors you add to your app’s content. You don’t need to persist these anchors yourself.\nFor more information about how to use ARKit, seeARKit.\nIsolate ARKit features not available in visionOS\nIf your app uses ARKit features that aren’t present in visionOS, isolate that code to the iOS version of your app. The following features are available in iOS, but don’t have an equivalent in visionOS:\nFace tracking\nFace tracking\nBody tracking\nBody tracking\nGeotracking and placing anchors using a latitude and longitude\nGeotracking and placing anchors using a latitude and longitude\nObject detection\nObject detection\nApp Clip Code detection\nApp Clip Code detection\nVideo frame post-processing\nVideo frame post-processing\nAlthough whole body tracking isn’t available in visionOS, you can track the hands of the person wearing the device. Hand gestures are an important way of interacting with content in visionOS. SwiftUI handles common types of interactions like taps and drags, but you can use custom hand tracking for more complex gestures your app supports.\nIf you use ARKit raycasting in iOS to detect interactions with objects in the person’s surroundings, you might not need that code in visionOS. SwiftUI and RealityKit handle both direct and indirect interactions with your app’s content in 3D space, eliminating the need for raycasting in many situations. In other situations, you can use the features of ARKit and RealityKit to manage interactions with your content. For example, you might use ARKit hand tracking to determine where someone is pointing in the scene, and use scene reconstruction to build a mesh you can integrate into your RealityKit content.\nSee Also\niOS migration and compatibility",
    "https://developer.apple.com/documentation/visionos#Explore-new-kinds-of-interaction": "visionOS\nOverview\nvisionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.\n\nDeveloping for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app’s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS.\nExpand your app into immersive spaces\nStart with a familiar window-based experience to introduce people to your content. From there, addSwiftUIscene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences.\nBuild your app’s 3D content withRealityKitand Reality Composer Pro, and display it with aRealityView. In an immersive experience, useARKitto integrate your content with the person’s surroundings.\n\nExplore new kinds of interaction\nPeople can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures.SwiftUIprovides built-in support for these standard gestures, so rely on them for most of your app’s input. When you want to go beyond the standard gestures, useARKitto create custom gestures.\nTap to select\nPinch to rotate\nManipulate objects\nCreate custom gestures\nDive into featured sample apps\nExplore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash.\nTopics\nApp construction\nDesign\nSwiftUI\nRealityKit and Reality Composer Pro\nARKit\nVideo playback\nXcode and Simulator\nPerformance\niOS migration and compatibility\nEnterprise APIs for visionOS\nvisionOS\nOverview\nExpand your app into immersive spaces\nExplore new kinds of interaction\nDive into featured sample apps\nTopics",
    "https://developer.apple.com/documentation/visionos/world": "visionOS\nHello World\nHello World\nHello World\nHello World\nOverview\nYou can use visionOS scene types and styles to share information in fun and compelling ways. Features like volumes and immersive spaces let you put interactive virtual objects into people’s environments, or put people into a virtual environment.\nHello World uses these tools to teach people about the Earth — the planet we call home. The app shows how the Earth’s tilt creates the seasons, how objects move as they orbit the Earth, and how Earth appears from space.\nThe app uses SwiftUI to define its interface, including both 2D and 3D elements. To create, customize, and manage 3D models and effects, it also relies on the RealityKit framework and Reality Composer Pro.\nCreate an entry point into the app\nHello World constructs the scene that it displays at launch — the first scene that appears in theWorldAppstructure — using aWindowGroup:\nLike other platforms — for example, macOS and iOS — visionOS displays a window group as a familiar-looking window. In visionOS, people can resize and move windows around the Shared Space. Even if your app offers a sophisticated 3D experience, a window is a great starting point for an app because it eases people into the experience. It’s also a good place to provide instructions or controls.\nTip\nThis particular window group uses theplainwindow style to maintain control over the glass background effect that visionOS would otherwise automatically add.\nPresent different modules using a navigation stack\nAfter you watch a brief introductory animation that shows the text Hello World typing in, theModulesview that defines the primary scene’s content presents options to explore different aspects of the world. This view contains a table of contents at the root of aNavigationStack:\nA visionOS navigation stack has the same behavior that it has in other platforms. When it first appears, the stack displays its root view. When someone chooses an embeddedNavigationLink, the stack draws a new view and displays a back button in the toolbar. When someone taps the back button, the stack restores the previous view.\n\nThe trailing closure of thenavigationDestination(for:destination:)view modifier in the code above displays a view when someone activates a link based on amoduleinput that comes from the corresponding link’s initializer:\nThe possiblemodulevalues come from a customModuleenumeration:\nDisplay an interactive globe in a new scene\nTheglobemodule opens with a few facts about the Earth in the main window next to a decorative, flat image that supports the content. To help people understand even more, the module includes a button titled View Globe that opens a 3D interactive globe in a new window.\n\nTo be able to open multiple scene types, Hello World includes theUIApplicationSceneManifestkey in itsInformation Property Listfile. The value for this key is a dictionary that includes theUIApplicationSupportsMultipleSceneskey with a value oftrue:\nDeclare a volume for the globe\nWith the key in place, the app makes use of a secondWindowGroupin itsAppdeclaration. This new window group uses theGlobeview as its content:\nThis window group creates avolume— which is a container that has three dimensions and behaves like a transparent box — because Hello World uses thevolumetricwindow style scene modifier. People can move this box around the Shared Space like they move other window types, and the content remains fixed inside. ThedefaultSize(width:height:depth:in:)modifier specifies a size for the volume in meters, including a depth dimension.\nTheGlobeview inside the volume contains 3D content, but is still just a SwiftUI view. It contains two elements in aZStack: a subview that draws a model of the Earth, and another that provides a control panel that people can use to configure the model’s appearance.\nOpen and dismiss the globe volume\nThe globe module presents a View Globe button that people can tap to display or dismiss the volume, depending on the current state. Hello World achieves this behavior by creating aTogglewith the button style, and embedding it in a customGlobeToggleview.\n\nWhen someone taps the toggle, theisShowingGlobestate changes, and theonChange(of:initial:_:)modifier calls theopenWindowordismissWindowaction to open or dismiss the volume, respectively. The view gets these actions from the environment and uses an identifier that matches the volume’s identifier.\nDisplay objects that orbit the Earth\nYou use windows in visionOS the same way you do in other platforms. But even 2D windows in visionOS provide a small amount of depth you can use to create 3D effects — like elements that appear in front of other elements. Hello World takes advantage of this depth to present small models inline with 2D content.\nThe app’s second module, Objects in Orbit, provides information about objects that go around the Earth, like the Moon and artificial satellites. To give a sense of what these objects look like, the module displays 3D models of these items directly inside the window.\n\nHello World loads these models from the asset bundle using aModel3Dstructure inside a customItemView. The view scales and positions the model to fit the available space, and applies optional orientation adjustments:\nThe app uses thisItemViewonce for each model, placing each in an overlay that only becomes visible based on the current selection. For example, the following overlay displays the satellite model with a small amount of tilt in the x-axis and z-axis:\nTheVStackthat contains the models also contains aPickerthat people use to select a model to view:\nWhen you add 3D effects to a 2D window, keep this guidance in mind:\nDon’t overdo it.These kinds of effects add interest, but can unintentionally obscure important controls or information as people view the window from different directions.\nDon’t overdo it.These kinds of effects add interest, but can unintentionally obscure important controls or information as people view the window from different directions.\nEnsure that elements don’t exceed the available depth.Excess depth causes elements to clip. Account for any position or orientation changes that might occur after initial placement.\nEnsure that elements don’t exceed the available depth.Excess depth causes elements to clip. Account for any position or orientation changes that might occur after initial placement.\nAvoid models intersecting with the backing glass.Again, account for potential movement after initial placement.\nAvoid models intersecting with the backing glass.Again, account for potential movement after initial placement.\nShow Earth’s relationship to its satellites in an immersive space\nPeople can visualize how satellites move around the Earth because the app’s orbit module displays the Earth, the Moon, and a communications satellite together as a single system. People can move the system anywhere in their environment or resize it using standard gestures. They can also move themselves around the system to get different perspectives.\n\nNote\nTo learn about designing with gestures in visionOS, readGesturesinHuman Interface Guidelines.\nTo create this visualization, the app displays theOrbitview — which contains a singleRealityViewthat models the entire system — in anImmersiveSpacescene with themixedimmersion style:\nAs with any secondary scene in a visionOS app, this scene depends on having theUIApplicationSupportsMultipleSceneskey in theInformation Property Listfile. The app also opens and closes the space using a toggle view that resembles the one used for the globe:\nThere are a few key differences from the version that appears in the sectionOpen and dismiss the globe volume:\nOrbitToggleusesopenImmersiveSpaceanddismissImmersiveSpacefrom the environment, rather than the window equivalents.\nOrbitToggleusesopenImmersiveSpaceanddismissImmersiveSpacefrom the environment, rather than the window equivalents.\nThe dismiss action in this case doesn’t require an identifier, because people can only open one space at a time, even across apps.\nThe dismiss action in this case doesn’t require an identifier, because people can only open one space at a time, even across apps.\nThe open and dismiss actions for spaces operate asynchronously, and so they appear inside aTask.\nThe open and dismiss actions for spaces operate asynchronously, and so they appear inside aTask.\nView the solar system from space using full immersion\nThe app’s final module gives people a sense of the Earth’s place in the solar system. Like other modules, this one includes information and a decorative image next to a button that leads to another visualization — in this case so people can experience Earth from space.\nWhen a person taps the button, the app takes over the entire display and shows stars in all directions. The Earth appears directly in front, the Moon to the right, and the Sun to the left. The main window also shows a small control panel that people can use to exit the fully immersive experience.\n\nTip\nPeople can always close the currently open immersive space by pressing the device’s Digital Crown, but it’s typically useful when you provide a built-in mechanism to maintain control of the experience within your app.\nThe app uses another immersive space scene for this module, but here with thefullimmersion style that turns off the passthrough video:\nThis scene depends on the sameUIApplicationSupportsMultipleSceneskey that other secondary scenes do, and is activated by aSolarSystemTogglethat’s similar to the ones that the app uses for the other scenes:\nThis control appears in the main window to provide a way to begin the fully immersive experience, and separately in the control panel as a way to exit the experience. Because the app uses this control as two distinct buttons rather than as a toggle in one location, it’s composed of aButtonwith behavior that changes depending on the app state rather than as a toggle with a button style.\nTo reuse the main window for the solar system controls, Hello World places both the navigation stack and the controls in aZStack, and then sets the opacity of each to ensure that only one appears at a time:\nSee Also"
}