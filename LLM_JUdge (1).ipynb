{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai pandas numpy scikit-learn rouge-score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Nbx18yuzO8f",
        "outputId": "213a0ad8-0229-4601-c71a-6046339a9fd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.164.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.13.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=e33b09bc44f3c3a19868f842f0c8dc9ceeb3ec007f9ada25885366e4ecc8f1be\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1io65i0r3vPM",
        "outputId": "efd12b37-a2f3-47ea-f57b-e3e7c121b952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.23.1-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.0)\n",
            "Downloading groq-0.23.1-py3-none-any.whl (127 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.4/127.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.23.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-29QlKuyQNZ",
        "outputId": "f95234bf-29d6-4721-c1f8-bca194052423"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7 matching question-PDF pairs for evaluation.\n",
            "Processing question 1/7: What mechanisms does BOT-anist use to ensure respo...\n",
            "Processing question 2/7: How does BOT-anist handle cross-platform deploymen...\n",
            "Processing question 3/7: What techniques are used to adapt the user interfa...\n",
            "Processing question 4/7: In what ways does BOT-anist utilize blend shapes f...\n",
            "Processing question 5/7: How does the JointPinSystem and JointPinComponent ...\n",
            "Processing question 6/7: Explain how BOT-anist differentiates its window ma...\n",
            "Processing question 7/7: How does BOT-anist implement dynamic lighting and ...\n",
            "\n",
            "--- Numeric Scoring Summary ---\n",
            "+-----------------------+-----------------+\n",
            "| Metric                | Average Score   |\n",
            "+=======================+=================+\n",
            "| Avg_comprehensiveness | 2.29/10         |\n",
            "+-----------------------+-----------------+\n",
            "| Avg_diversity         | 1.71/10         |\n",
            "+-----------------------+-----------------+\n",
            "| Avg_empowerment       | 2.57/10         |\n",
            "+-----------------------+-----------------+\n",
            "| Avg_directness        | 2.57/10         |\n",
            "+-----------------------+-----------------+\n",
            "| Avg_rougel            | 0.20            |\n",
            "+-----------------------+-----------------+\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "from typing import List, Dict, Tuple\n",
        "import pandas as pd\n",
        "from rouge_score import rouge_scorer\n",
        "import time\n",
        "from groq import Groq\n",
        "from tabulate import tabulate\n",
        "from google.colab import userdata\n",
        "\n",
        "# --- Groq API Configuration ---\n",
        "GROQ_MODEL = \"llama3-70b-8192\"\n",
        "try:\n",
        "    GROQ_API_KEY = \"\"\n",
        "    if not GROQ_API_KEY:\n",
        "        raise ValueError(\"GROQ_API_KEY not set in Colab Secrets or environment.\")\n",
        "    client = Groq(api_key=GROQ_API_KEY)\n",
        "except Exception as e:\n",
        "    print(f\"Error configuring Groq API: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- Prompt Templates ---\n",
        "NUMERIC_SCORING_PROMPT = \"\"\"\n",
        "You are an impartial AI grader. Your task is to evaluate Answer B relative to Answer A (the reference answer) based on the following criterion.\n",
        "\n",
        "**Criterion:** {criterion_name}: {criterion_description}\n",
        "\n",
        "**Question:**\n",
        "{question}\n",
        "\n",
        "**Answer A (Reference):**\n",
        "{answer_a}\n",
        "\n",
        "**Answer B (System Response):**\n",
        "{answer_b}\n",
        "\n",
        "**Instructions:**\n",
        "1. Analyze both answers step-by-step, comparing how well Answer B satisfies the criterion relative to Answer A.\n",
        "2. Assign a score to Answer B from 0 (poor) to 10 (excellent).\n",
        "3. Return *only* a valid JSON object with 'reasoning' and 'score' fields, with no additional text or markdown.\n",
        "4. Ensure the output is valid JSON, e.g.:\n",
        "{{\n",
        "  \"reasoning\": \"Your step-by-step analysis here\",\n",
        "  \"score\": 7\n",
        "}}\n",
        "\n",
        "**Output:**\n",
        "{{\n",
        "  \"reasoning\": \"Your step-by-step analysis here\",\n",
        "  \"score\": <number>\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "# --- Criterion Definitions ---\n",
        "CRITERIA = {\n",
        "    \"comprehensiveness\": \"How much detail does the answer provide to cover all aspects and details of the question?\",\n",
        "    \"diversity\": \"How varied is the answer in perspectives or examples?\",\n",
        "    \"empowerment\": \"Does the answer help the user understand or act confidently?\",\n",
        "    \"directness\": \"Is the answer concise and directly answering the question?\"\n",
        "}\n",
        "\n",
        "# --- Groq API Call ---\n",
        "def call_groq(prompt: str, retries: int = 3) -> str:\n",
        "    \"\"\"Calls Groq API with retry logic and rate limiting.\"\"\"\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            completion = client.chat.completions.create(\n",
        "                model=GROQ_MODEL,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.7,\n",
        "                max_tokens=1024\n",
        "            )\n",
        "            time.sleep(10)  # Rate limit: 6 RPM = 10 seconds per request\n",
        "            return completion.choices[0].message.content\n",
        "        except Exception as e:\n",
        "            print(f\"Groq API error on attempt {attempt + 1}: {e}\")\n",
        "            if attempt == retries - 1:\n",
        "                return '{\"reasoning\": \"Error: Failed to get response from Groq.\", \"score\": -1}'\n",
        "            time.sleep(1)  # Wait before retrying\n",
        "    time.sleep(10)  # Rate limit even on failure\n",
        "    return '{\"reasoning\": \"Error: Failed to get response from Groq.\", \"score\": -1}'\n",
        "\n",
        "# --- Score Parser ---\n",
        "def parse_score(response: str) -> Tuple[float, str]:\n",
        "    \"\"\"Parses JSON response to extract score and reasoning, with fallback parsing.\"\"\"\n",
        "    # Try to extract JSON block using regex\n",
        "    json_match = re.search(r'\\{[\\s\\S]*?\\}', response, re.MULTILINE)\n",
        "    if json_match:\n",
        "        json_str = json_match.group(0)\n",
        "        try:\n",
        "            data = json.loads(json_str)\n",
        "            score = float(data.get(\"score\", -1))\n",
        "            reasoning = data.get(\"reasoning\", \"No reasoning provided\")\n",
        "            if not (0 <= score <= 10):\n",
        "                return -1.0, \"Invalid score: Not in range 0–10\"\n",
        "            return score, reasoning\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Invalid JSON in response: {json_str}\")\n",
        "    else:\n",
        "        print(f\"No JSON block found in response: {response}\")\n",
        "\n",
        "    # Fallback: Try to extract score from narrative text\n",
        "    score_match = re.search(r\"(?:score of|assigned a score of)\\s*(\\d{1,2}(?:\\.\\d+)?)\", response, re.IGNORECASE)\n",
        "    if score_match:\n",
        "        try:\n",
        "            score = float(score_match.group(1))\n",
        "            if 0 <= score <= 10:\n",
        "                return score, \"Fallback: Score extracted from narrative text\"\n",
        "        except ValueError:\n",
        "            pass\n",
        "\n",
        "    # Log full response for debugging\n",
        "    with open(\"parsing_errors.log\", \"a\") as f:\n",
        "        f.write(f\"Invalid response: {response}\\n\\n\")\n",
        "    return -1.0, \"Error: Failed to parse JSON or extract score\"\n",
        "\n",
        "# --- ROUGE Score Calculator ---\n",
        "def calculate_rouge(answer_a: str, answer_b: str) -> Dict[str, float]:\n",
        "    \"\"\"Calculates ROUGE-L score between two answers.\"\"\"\n",
        "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    scores = scorer.score(answer_a, answer_b)\n",
        "    return {\"rougeL\": scores['rougeL'].fmeasure}\n",
        "\n",
        "# --- Answer Validation ---\n",
        "def validate_jsons(original_data: Dict, compare_data: Dict) -> List[Tuple[Dict, Dict]]:\n",
        "    \"\"\"Validates JSON data and matches question-PDF pairs from PDF-keyed structure.\"\"\"\n",
        "    required_fields = {\"question\", \"answer\"}\n",
        "    pairs = []\n",
        "\n",
        "    # Get common PDF names\n",
        "    common_pdfs = set(original_data.keys()) & set(compare_data.keys())\n",
        "    if not common_pdfs:\n",
        "        print(\"Error: No common PDF names found.\")\n",
        "        return []\n",
        "\n",
        "    for pdf in common_pdfs:\n",
        "        original_entries = original_data[pdf]\n",
        "        compare_entries = compare_data[pdf]\n",
        "\n",
        "        # Create question-to-entry mappings\n",
        "        original_map = {entry[\"question\"]: entry for entry in original_entries}\n",
        "        compare_map = {entry[\"question\"]: entry for entry in compare_entries}\n",
        "\n",
        "        # Validate fields\n",
        "        for entry, name in [(original_entries, \"original\"), (compare_entries, \"AURA\")]:\n",
        "            for item in entry:\n",
        "                if not all(field in item for field in required_fields):\n",
        "                    print(f\"Error: Missing fields in {name} JSON for PDF {pdf}: {item}\")\n",
        "                    return []\n",
        "\n",
        "        # Find common questions\n",
        "        common_questions = set(original_map.keys()) & set(compare_map.keys())\n",
        "        if not common_questions:\n",
        "            print(f\"Error: No matching questions found for PDF {pdf}.\")\n",
        "            continue\n",
        "\n",
        "        # Create pairs with PDF context\n",
        "        for question in common_questions:\n",
        "            original_entry = original_map[question].copy()\n",
        "            compare_entry = compare_map[question].copy()\n",
        "            original_entry[\"pdf\"] = pdf  # Add pdf field for downstream compatibility\n",
        "            compare_entry[\"pdf\"] = pdf\n",
        "            pairs.append((original_entry, compare_entry))\n",
        "\n",
        "    if not pairs:\n",
        "        print(\"Error: No matching question-PDF pairs found.\")\n",
        "        return []\n",
        "    return pairs\n",
        "# --- Evaluation Runner ---\n",
        "def evaluate_answers_with_scores(original_json_path: str, compare_json_path: str) -> Dict:\n",
        "    \"\"\"Evaluates answers from two JSON files with numerical scoring and ROUGE.\"\"\"\n",
        "    # Load JSON files\n",
        "    try:\n",
        "        with open(original_json_path, 'r') as f:\n",
        "            original_data = json.load(f)\n",
        "        with open(compare_json_path, 'r') as f:\n",
        "            compare_data = json.load(f)\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: JSON file not found: {e}\")\n",
        "        return {}\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON: {e}\")\n",
        "        return {}\n",
        "\n",
        "    # Validate and match pairs\n",
        "    pairs = validate_jsons(original_data, compare_data)\n",
        "    if not pairs:\n",
        "        return {}\n",
        "    print(f\"Found {len(pairs)} matching question-PDF pairs for evaluation.\")\n",
        "\n",
        "    rows = []\n",
        "    detailed_results = []\n",
        "    parse_errors = 0\n",
        "\n",
        "    for i, (original_entry, compare_entry) in enumerate(pairs, 1):\n",
        "        question = original_entry[\"question\"]\n",
        "        answer_a = original_entry[\"answer\"]\n",
        "        answer_b = compare_entry[\"answer\"]\n",
        "        pdf = original_entry[\"pdf\"]\n",
        "        print(f\"Processing question {i}/{len(pairs)}: {question[:50]}...\")\n",
        "\n",
        "        row = {\"question\": question, \"pdf\": pdf}\n",
        "        detailed_entry = {\n",
        "            \"question\": question,\n",
        "            \"pdf\": pdf,\n",
        "            \"answer_a\": answer_a,\n",
        "            \"answer_b\": answer_b,\n",
        "            \"scores\": {}\n",
        "        }\n",
        "\n",
        "        # LLM-as-a-Judge Scoring\n",
        "        for criterion in CRITERIA:\n",
        "            prompt = NUMERIC_SCORING_PROMPT.format(\n",
        "                question=question,\n",
        "                criterion_name=criterion.capitalize(),\n",
        "                criterion_description=CRITERIA[criterion],\n",
        "                answer_a=answer_a,\n",
        "                answer_b=answer_b\n",
        "            )\n",
        "            response = call_groq(prompt)\n",
        "            score, reasoning = parse_score(response)\n",
        "            row[f\"{criterion}_score\"] = score\n",
        "            detailed_entry[\"scores\"][criterion] = {\"score\": score, \"reasoning\": reasoning}\n",
        "            if score == -1.0:\n",
        "                parse_errors += 1\n",
        "\n",
        "        # ROUGE Score\n",
        "        rouge_scores = calculate_rouge(answer_a, answer_b)\n",
        "        row[\"rougeL\"] = rouge_scores[\"rougeL\"]\n",
        "        detailed_entry[\"rougeL\"] = rouge_scores[\"rougeL\"]\n",
        "\n",
        "        rows.append(row)\n",
        "        detailed_results.append(detailed_entry)\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(\"evaluation_numeric_scores.csv\", index=False)\n",
        "\n",
        "    # Save Detailed Results\n",
        "    with open(\"evaluation_detailed_results.json\", \"w\") as f:\n",
        "        json.dump(detailed_results, f, indent=2)\n",
        "\n",
        "    # Summary\n",
        "    summary = {f\"avg_{c}_score\": df[f\"{c}_score\"].mean() for c in CRITERIA}\n",
        "    summary[\"avg_rougeL\"] = df[\"rougeL\"].mean()\n",
        "\n",
        "    # Create Formatted Table\n",
        "    table_data = [\n",
        "        [k.replace(\"_score\", \"\").capitalize(), f\"{v:.2f}\" + (\"/10\" if \"rougeL\" not in k else \"\")]\n",
        "        for k, v in summary.items()\n",
        "    ]\n",
        "    print(\"\\n--- Numeric Scoring Summary ---\")\n",
        "    print(tabulate(table_data, headers=[\"Metric\", \"Average Score\"], tablefmt=\"grid\"))\n",
        "    if parse_errors > 0:\n",
        "        print(f\"\\nWarning: {parse_errors} responses failed to parse correctly. See 'parsing_errors.log' for details.\")\n",
        "\n",
        "    return summary\n",
        "\n",
        "# --- Run Evaluation ---\n",
        "if __name__ == \"__main__\":\n",
        "    original_json_path = \"/content/question_answers.json\"\n",
        "    compare_json_path = \"/content/generated_rag_answers (2).json\"\n",
        "    results = evaluate_answers_with_scores(original_json_path, compare_json_path)"
      ]
    }
  ]
}